---
title: "Scientific Paper: Neural Network for Forecasted Avalanche Hazard"
author:
  - name: Brendon Pretorius
  - name: Johan John
  - name: Nabil Patel
  - name: Nkateko Nakkie Mawelele

date: "September 27, 2025"
embed-resources: true
format: 
  html:
    theme: sais-theme.scss
---

# Table of Contents

- [Introduction](#introduction)
- [Background](#background)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [EDA](#eda)
  - [Target Variable Distribution](#target-variable-distribution)
  - [Missing Data Analysis](#missing-data-analysis)
  - [Temporal Analysis](#temporal-analysis)
- [Data Processing](#data-processing)
  - [Outlier Detection](#subsection-outlier-detection)
  - [Feature Engineering](#subsection-feature-engineering)
  - [Imputation for Missing Values](#imputation-for-missing-values)
  - [Target Encoding](#target-encoding)
  - [Feature Selection](#feature-selection)
- [Implementation](#implementation)
  - [Data Splitting and ROSE Balancing](#data-splitting-and-rose-balancing)
  - [Neural Network Architecture](#neural-network-architecture)
- [Model Performance Evaluation](#model-performance-evaluation)
  - [Overall Performance](#overall-performance)
  - [Confusion Matrix and Class-Level Performance](#confusion-matrix-and-class-level-performance)
  - [Confidence-Based Reliability](#confidence-based-reliability)
  - [Avalanche-Specific Safety Metrics](#avalanche-specific-safety-metrics)
- [Feature Importance Assessment](#feature-importance-assessment)
- [Conclusion and Recommendations](#conclusion-and-recommendations)
- [Author Contributions](#author-contributions)
- [References (APA)](#references-apa)


## Introduction

Avalanche forecasting integrates mountain meteorology, snowpack physics, terrain analysis, and risk communication to reduce loss of life and support safer winter travel. In Scotland, the Scottish Avalanche Information Service (SAIS) publishes daily next-day hazard assessments across six high-mountain regions during winter which draws upon field observations, snowpack tests and weather guidance ([Scottish Avalanche Information Service [SAIS], n.d.](#ref-sais-nd)). This report contributes a data-driven complement to that long-running operational effort. Our task is defined as to construct and evaluate a neural network model that predicts the forecasted avalanche hazard (FAH).

This research isn’t meant to replace expert judgment. It gives forecasters a second opinion that consistently flags risky combinations of weather, terrain, and snowpack variables. In Scotland’s maritime, wind-dominated winters with frequent storm cycles and rapid thaw freeze shifts this kind of decision support sharpens situational awareness and strengthens risk communication ([SAIS, n.d.](#ref-sais-nd); [EAWS, n.d.](#ref-eaws-nd)).

## Background

Scotland’s avalanche forecasts are produced by the Scottish Avalanche Information Service (SAIS). The SAIS conducts daily fieldwork, snow profiles, stability tests and terrain analysis that is blended with meteorological guidance to issue next-day public danger ratings, snowpack summaries and travel advice. This information spans Torridon, Northern Cairngorms, Southern Cairngorms, Creag Meagaidh, Lochaber, and Glencoe. The communication follows the standard EAWS danger scale to keep messages consistent and comparable.

Avalanches are rapid snow flows that can contain ice, rock, and vegetation. The most dangerous to travellers are slab avalanches, where a cohesive slab slides on a weaker layer once shear strength is exceeded by gravitational and external loading. The triggers for an avalanche may be natural (snowfall, wind loading, warming) or manmade (a skier, climber, or snowmobile). These avalanches most often occur on slopes around 34–45 degrees. In Scotland’s maritime, wind-dominated winters wind slabs are common ([Schweizer, Jamieson, & Schneebeli, 2003](#ref-schweizer-2003)).

Avalanche forecasting looks to improving consistency, calibration, and timeliness of hazard assessments which can essentially save lives and reduce societal costs. In Europe, despite substantial growth in backcountry participation, the long-term average annual avalanche fatality count has remained broadly steady at 100 per year across the Alps. This is attributed in part to better education, equipment, and forecasting ([Techel et al., 2016](#ref-techel-2016)). Scotland’s totals are smaller, but fatal and non-fatal involvements recur most winters and SAIS seasonal reports routinely document hundreds of observed avalanches ([SAIS, n.d.](#ref-sais-nd)). The impacts however extend beyond casualty numbers as they tend to disrupt transport, tourism, and emergency services.

## Dataset

The dataset is an operational archive produced by the Scottish Avalanche Information Service (SAIS). It spans approximately fifteen winter seasons and reflects real-time fieldwork. Forecasters collect these records in severe weather environments and under time constraints. As a result, the data shows uneven sampling across storm cycles and regions, along with missing entries during periods of limited access.

The variables in the dataset include physically informed features from the environment. They are grouped into categories below. A complete list of all variables, including their descriptions, data types, and basic statistics, is provided in the [Appendix](appendix.html).

### Terrain Susceptibility (Incline, Aspect, Alt)
Slope angle is the primary release control for slab avalanches, as most events initiate between the mid-30s and mid-40s degrees. Thus, Incline serves as a direct indicator of where failures are most likely. Aspect determines how much wind and sun a slope receives; when analyzed with recent Wind.Dir, it identifies lee (wind-loaded) and windward slopes. Alt specifies the altitude of the observation site in meters above sea level ([McClung & Schaerer, 2006](#ref-mcclung-2006); [Schweizer, Jamieson, & Schneebeli, 2003](#ref-schweizer-2003)).

### Loading and Weather Drivers (Air.Temp, Wind.Dir, Wind.Speed, Cloud, Precip.Code, Drift, Rain.at.900, Summit)
Near-surface Air.Temp highlights warm spells that weaken snow bonds. Wind.Dir and Wind.Speed together govern snow transport, building slabs quickly on lee and cross-loaded slopes. Cloud cover influences cooling and heating, driving near-surface faceting or slowing refreezing. Precip.Code indicates the type and intensity of new loading (e.g., snow or showers). Rain.at.900 flags rainfall at 900m altitude, which affects snowmelt. Field Drift observations confirm recent snow blowing. Summit.* variables describe conditions at the summit, where most loading begins ([EAWS, n.d.](#ref-eaws-nd); [McClung & Schaerer, 2006](#ref-mcclung-2006); [Schweizer et al., 2003](#ref-schweizer-2003)).

### Snowpack structure and stability proxies (Total.Snow.Depth, Foot.Pen, Ski.Pen, Max.Temp.Grad, Max.Hardness.Grad, Snow.Temp, Wetness, Crystals, Snow.Index, No.Settle, AV.Cat, Insolation)

Total.Snow.Depth reflects total depth of snow cover at the observation site, while Foot.Pen/Ski.Pen indicate near-surface strength. Max.Temp.Grad signals the temperature across the snowpack indicating instability. Max.Hardness.Grad captures hardness within the snowpack reflecting the layer differences. Snow.Temp defines the temperature within the snowpack and Wetness defines the degree of snow wetness. Crystals identify type or size of snow crystals observed which influences avalanche risk. Snow.Index provides cues on the stability of the snow. AV.Cat classifies avalanches by type/severity. Insolation measures the impact of solar radiation affecting snow melt. ([EAWS, n.d.](#ref-eaws-nd); [Schweizer et al., 2003](#ref-schweizer-2003); [McClung & Schaerer, 2006](#ref-mcclung-2006)).

By relating these predictors to FAH the model can learn recurrent, non-linear patterns. It highlights situations where higher hazard is more likely, so forecasters can focus their checks and make cleaner day-to-day calls ([EAWS, n.d.](#ref-eaws-nd); [SAIS, n.d.](#ref-sais-nd)).

## Methodology

This research builds a supervised, ordinal-aware classification pipeline to predict next-day Forecast Avalanche Hazard (FAH) from the operational SAIS records. The end-to-end workflow was implemented in R and organized into clearly defined stages—data auditing, preprocessing, feature engineering, imputation, encoding, feature selection, data splitting and class rebalancing, neural-network modeling, and evaluation. Reproducibility is enforced via set.seed(42) and some of the core libraries include tidyverse for data wrangling, caret for preprocessing/splitting, glmnet/randomForest for feature selection signals, ROSE for class balancing, and keras3/tensorflow for model training.

We used large language models (LLMs) to accelerate routine coding tasks. Each team member supplied an LLM with a structured brief (data schema, variable descriptions, target definition, leakage constraints, evaluation metrics) to draft boilerplate R code for preprocessing, imputation functions, model scaffolding (keras3), and plotting. To promote diversity of ideas and reduce single-model bias, different teammates intentionally used different LLMs. This use of LLMs sped up drafting, but final code, methodological choices, and validation remained human-curated.

## EDA

### Target Variable Distribution

::: {#fig-target-bar}
![Bar chart showing the distribution of the FAH target variable](../R work/Graphs/fig1.jpg)

Bar chart of FAH distribution
:::

::: {#fig-target-pie}
![Pie chart showing the percentage distribution of the FAH target variable](../R work/Graphs/fig2.jpg)

Pie chart of FAH distribution
:::

From @fig-target-bar and @fig-target-pie, we observe that the FAH target variable exhibits severe class imbalance, with safer classes dominating the distribution: Low (≈32.5%) and Moderate (≈30.7%) together account for ≈63% of observations, while Considerable- is moderately represented (≈23.6%). In contrast, the higher-risk classes—Considerable+ (≈8.8%) and especially High (≈4.4%)—are rare. This skew implies that a model could achieve high overall accuracy by favoring common classes while failing to detect infrequent but critical high-hazard events. To mitigate this, we employ ROSE balancing during training and prioritize macro-averaged metrics alongside high-risk class recall in evaluation.

### Missing Data Analysis

::: {#fig-missing-data}
![Graph showing missing data diagnostics across variables](../R work/Graphs/fig3.jpg)

Missing data analysis graph
:::

From @fig-missing-data, the missing data diagnostics reveal that AV_Cat and Ski_Pen exhibit the highest gaps (≈20%+), prompting their removal from the dataset during preprocessing. Following these, a cluster of snowpack microstructure and condition variables (e.g., Crystals, Wetness, No_Settle) show moderate missingness, alongside certain wind-direction and temperature fields. The target variable FAH has negligible missingness, likely due to operational teams making inferences when weather conditions do not allow for some metrics to be captured.

### Temporal Analysis

::: {#fig-temporal-coverage}
![Group of graphs showing temporal coverage and seasonal patterns](../R work/Graphs/fig4.jpg)

Temporal analysis graphs
:::

From @fig-temporal-coverage that denotes temporal coverage we can see that the archive spans many winters with stable annual volumes after the early years and a strong seasonal concentration in Dec–Mar, tapering in shoulder months and near-zero in summer (as expected). The daily series shows a burst of entries followed by lulls which tell us that data arrives in clusters during winter storm cycles and thin out outside winter. These patterns justify including month/season features and caution against assuming stationarity across years.

::: {#fig-fah-oah}
![Graph comparing FAH versus OAH pairs](../R work/Graphs/fig5.jpg)

FAH vs OAH comparison graph
:::

From @fig-fah-oah, which analyses FAH versus OAH pairs as a verification benchmark for forecast reliability, we observe an overall accuracy of approximately 70–75%. Per-class performance varies notably: accuracy is highest for Low risk (mid-90s%), followed by Moderate (70%), then decreases for Considerable- (60–70%), drops substantially for Considerable+ (35%), and improves slightly for High (~45%). This pattern suggests that forecasters—and by extension, predictive models—struggle most with distinguishing adjacent hazard boundaries. Accordingly, our model evaluation emphasizes ordinal metrics such as adjacent accuracy (±1 level), critical-miss rate (underpredicting high risk), and high-risk sensitivity to ensure practical utility in avalanche forecasting.

The EDA completed paints a clear operational picture with our target strongly skewed toward Low/Moderate days, so hence a model judged on plain accuracy could look good while failing on the rare but consequential upper hazards. In the implementation, we therefore look to rebalance the training split with ROSE and commit to macro metrics and high-risk recall at evaluation. The missingness is structured and not entirely random, primarily with the >20% gaps in **AV.Cat** and **Ski.Pen** which we drop. The clusters in snowpack detail fields and some wind directions reinforce the choice of **Area×Season** imputation (fit on train only) and the inclusion of seasonal features. The temporal profile confirms winter-centric usage with stable annual volumes, motivating Month/Season encodings and caution about year-to-year drift. Finally, FAH–OAH verification (~70–75% overall, strongest on Low, weakest in the Considerable+/High tail) mirrors the hardest real-world boundaries, hence we will look to prioritize adjacent accuracy (±1), critical-miss rate, and high-risk sensitivity to ensure the model not only performs well on average but reliably catches dangerous days.

## Data Processing

After initial data cleaning, we removed non-predictive fields (OAH, Obs) and filtered out the 1% of rows with missing FAH which left us  with 10,562 usable records.The FAH values were standardized into five clean categories with the following counts: Low 3,433, Moderate 3,239, Considerable− 2,498, Considerable+ 933, High 459 this was confirming a strong skew toward safer days. 

A missingness audit showed two variables with >20% gaps—AV.Cat (23.4%) and Ski.Pen (22.5%) these we then dropped to avoid heavy, low-confidence imputation. The next tier of missingness is moderate and structured rather than random. Core near-surface weather and geometry variables have very low gaps (≤1–3%). These patterns motivate our Area×Season imputation (fit on the training split only).

### Outlier Detection

To stabilise modelling while preserving the signal, we applied a two-tier outlier process. First, we converted any string “NA”s to true NAs and used a robust IQR filter (k = 3.5) on key continuous variables, while looking to prune only extreme spikes: Alt (2), Wind.Speed (22), Summit.Wind.Speed (81), Total.Snow.Depth (111), Air.Temp (1), Summit.Air.Temp (0), and Foot.Pen (34). Second, we enforced physical plausibility rules and essentially removed Aspect > 360° (8), constrained Cloud to [0,100]% (3), and limited Incline to [0,90]° (5). In total, 267 rows (2.53%) were dropped. This keeps legitimate extremes (e.g., high winds/depths during storms) while eliminating impossible or clearly erroneous readings that would distort scaling, inflate variance, and hinder neural-network optimisation.

The boxplots show two kinds of outliers. Some are implausible values with AV.Cat having huge positive/negative magnitudes and >20% missingness. Cloud occasionally exceeds 100% or goes negative.  The others may be described as legitimate extremes as Snow.Index has a heavy right tail, Crystals has rare large values amid many zeros, and Rain.at.900 behaves like a binary indicator (0/1), so the “1”s only look like outliers on a numeric boxplot. Latitude spans the expected Scottish band and is not a true outlier.

Guided by this, we used a two-tier outlier strategy by first applying a physical-plausibility filters to drop AV.Cat, cap Cloud to [0,100], discard Aspect > 360°, Incline <0 or >90°, and remove negative or absurd readings for wind, snow depth, and penetrations. Second, for variables where extremes are meaningful we used robust IQR trimming with a wide cutoff (k≈3–3.5) to prune only extreme spikes while preserving genuine storm extremes. We also recast Rain.at.900 as a binary feature to avoid mislabelling valid “1”s as outliers. This approach reduces measurement/error noise, stabilizes scaling and NN training.

### Feature Engineering

We developed a small set of mechanism-aware features to turn raw measurements into signals the model can learn from. A Wind_Chill proxy (Air.Temp − 0.6×Wind.Speed) captures near-surface cooling that weakens bonds under strong winds. A Temp_Gradient (Summit.Air.Temp − Air.Temp) approximates vertical stability and transport potential. Snow_Alt_Interaction (Total.Snow.Depth × Alt/1000) lets snowfall load scale with elevation and the rain–snow line. Because Aspect is circular (0°≈360°), we encoded it as Aspect_North = cos(Aspect) and Aspect_East = sin(Aspect) to avoid artificial discontinuities and to align with wind-loading geometry. Finally, we added Month, Day_of_Year, and Season to reflect the strong winter seasonality seen in the EDA and to let the network learn intra-season cycles (e.g., cold spells vs. thaw pulses). Collectively, these features reduce redundancy, respect data geometry, and embed avalanche mechanics directly into the predictors.

::: {#fig-correlation-heatmap}
![Correlation heatmap revealing relationships among variables](../R work/Graphs/fig6.jpg)

Correlation heatmap
:::

The correlation heatmap in @fig-correlation-heatmap reveals coherent but mostly moderate correlation clusters among variables. Notably, temperatures (Air.Temp, Summit.Air.Temp) exhibit strong covariation, while Cloud and Insolation show the expected inverse relationship. Wind speeds correlate across levels with some directional noise, and penetration/strength metrics (Foot.Pen, Ski.Pen) associate with Total.Snow.Depth in predictable ways. Meanwhile, Max.Temp.Grad and Max.Hardness.Grad relate to temperature and snow-structure variables through more complex mechanisms, likely tied to metamorphic processes in the snowpack. Overall, this indicates potential multicollinearity, particularly among thermodynamic and wind features, which could destabilize linear models and hinder neural network convergence. To address this, we implemented ensemble feature selection combining LASSO regularization, random forest importance scoring, and correlation-based filtering to minimize redundancy.

Leveraging these patterns, we engineered targeted features to capture key avalanche mechanics in stable, low-redundancy forms:

- Wind–cold interaction: a simple wind-chill proxy (Air.Temp − 0.6×Wind.Speed) to reflect near-surface cooling.

- Vertical stability: summit–valley temperature gradient (Summit.Air.Temp − Air.Temp) as a rough indicator of atmospheric stratification/transport potential

- Snow–altitude interaction: Total.Snow.Depth × (Alt/1000) to allow depth effects to vary with elevation (rain/snow line).

- Circular aspect encoding: cos(Aspect) and sin(Aspect) to preserve directionality without discontinuity at 360°/0°.

- Seasonality: calendar Month, Day_of_Year, and Season factors to capture intra-winter cycles.

### Imputation for Missing Values

We address missing data with a context-aware, hierarchical imputation routine that mirrors how avalanche conditions vary across space and season. First, for each Area×Season group we fill numeric gaps with the group median (robust to outliers) and categorical gaps with the group mode, so replacements reflect typical conditions for that region and time of year rather than a blunt global average. Next, any leftovers are handled with global medians/modes as a conservative fallback. We explicitly exclude non-predictive and target fields (Date, OSgrid, Location, FAH) from imputation to avoid leakage into the label. This design preserves regional/seasonal structure noted in the EDA  while producing a complete feature matrix for modelling.

### Target Encoding

To prepare the data for a neural network, we first removed non-predictive IDs (Date, OSgrid, Location) and encoded the target, FAH, as an ordinal integer in true risk order Low→High = 0–4. A verification table confirmed a one-to-one mapping (Low=0, Moderate=1, Considerable−=2, Considerable+=3, High=4) with no unmapped values and the encoded class counts [3398, 3167, 2412, 895, 423], matching the observed imbalance. 

For predictors, we cleaned categorical fields and applied a frequency threshold (≥50) so very rare levels were folded into “Other” to limit sparsity. We then one-hot encoded the remaining categories for Area, Precip.Code, and Season, while adding binary indicator columns and dropping the original fields. This pipeline preserves the ordinal meaning of the hazard label while transforming categorical inputs into a numerically stable representation that dense neural networks can learn from, with reduced risk of overfitting to tiny categories.

### Feature Selection

After cleaning and encoding, we standardized all numeric predictors and ran an ensemble feature-selection procedure on the 7,530 complete cases to concentrate signal and control dimensionality. Three complementary signals were computed per feature—LASSO coefficients (linear sparsity), Random Forest permutation importance (non-linear/interaction effects), and target correlation—each normalized to 0–100 and combined with weights (RF 40%, LASSO 30%, Correlation 30%). LASSO retained 30 features, Random Forest and correlation evaluated 47 while the weighted scores yielded a final shortlist of 25 predictors. The top-ranked variables are physically coherent with avalanche mechanics are

- Foot.Pen (near-surface strength)

- Drift (recent wind transport)

- Summit.Air.Temp and Air.Temp (thermal state)

- Total.Snow.Depth (load)

- Wind.Speed/Summit.Wind.Speed (loading potential)

- Max.Temp.Grad (faceting potential)

- *Snow.Temp*

- *Insolation*

- Wind_Chill, Snow_Alt_Interaction, and Day_of_Year (engineered terms)

- *Precip.Code*

- *Season_Winter*

- Area_Lochaber and Area_Torridon

## Implementation

### Data Splitting and ROSE Balancing

The cleansed dataset of 7,530 observations was split into training (70%), validation (15%), and test (15%) sets, stratified by hazard level to maintain class proportions. This yielded 5,273 training samples, 1,129 validation samples, and 1,128 test samples from the 7,530 complete cases. The training set reflected the dataset's imbalance, with Low (class 0) and Moderate (class 1) each >1,700 samples, versus only 145 for High (class 4)—an imbalance ratio of ~12.4:1. Validation and test sets preserved similar distributions, retaining the rarity of high-hazard classes.

To mitigate bias toward majority classes, ROSE balancing was applied solely to the training data post-split, keeping validation/test sets natural for unbiased evaluation. Using an iterative one-versus-rest approach (since ROSE is binary-native), synthetic samples were generated for minorities until they reached ~95% of majority size, avoiding perfect balance.

The training set grew from 5,273 to 8,641 samples, with ~1,700 per class, reducing the imbalance ratio to 1.05:1 (a 91.5% improvement). This enhanced minority representation for neural network training while ensuring realistic evaluation.

### Neural Network Architecture

After hyperparameter tuning across three candidate configurations, the optimal network achieved a validation accuracy of 0.6138 with a corresponding training accuracy of 0.7159. This represented the best trade-off between learning capacity and generalization. The final architecture consisted of three hidden layers with 512, 256, and 128 units, respectively, paired with progressively increasing dropout regularization (0.20, 0.30, 0.40). Gaussian noise was applied at the input layer to mitigate overfitting and improve robustness against noisy environmental predictors. The network was trained using the Adam optimizer with a learning rate of 5×10⁻⁴ and a batch size of 32, which provided stable convergence during training.

| Layer          | Units | Dropout | Notes                             |
|----------------|-------|---------|-----------------------------------|
| Input          | –     | –       | 25 selected features              |
| Hidden Layer 1 | 512   | 0.20    | Dense + BatchNorm + GaussianNoise |
| Hidden Layer 2 | 256   | 0.30    | Dense + BatchNorm + Dropout       |
| Hidden Layer 3 | 128   | 0.40    | Dense + BatchNorm + Dropout       |
| Output         | 5     | –       | Softmax (multiclass, ordinal)     |

*Table 1: Final neural network architecture*

The training and validation curves showed that the model achieved stable convergence, though validation accuracy plateaued around 0.61, reflecting the intrinsic difficulty of predicting the rare high-risk classes.

## Model Performance Evaluation

### Overall Performance

The neural network’s performance was assessed across the training, validation, and test datasets to evaluate both its learning capacity and generalization ability. During training, the model achieved a best training accuracy of 71.6%, with the corresponding best validation accuracy reaching 61.4%. The final test accuracy was 60.6% (loss = 0.993). The gap between training and validation performance indicates some overfitting, which is expected in complex models, but the close alignment of validation and test results shows that the network generalized reasonably well to unseen data and avoided substantial performance degradation. 

Beyond test accuracy, complementary metrics provided deeper insight into generalization. Macro-averaged scores (precision = 0.499, recall = 0.452, F1 = 0.459) showed that the model was only moderately effective when treating all classes equally. Weighted averages were higher (precision = 0.589, recall = 0.606, F1 = 0.594), reflecting stronger performance on the dominant classes. Cohen’s Kappa, at 0.443, placed the model in the “moderate agreement” range, showing that predictions were significantly better than random guessing but not yet highly reliable. EDA findings again provide explanation: the removal of several snowpack variables with >20% missingness reduced the depth of predictive features, while extreme but plausible outliers (e.g., very high winds, deep snowpack events) complicated the learning process, particularly for rarer hazard categories.

Given the ordinal nature of avalanche hazard levels, error magnitudes were as important as raw accuracy. Predictions deviated by an average of only 0.45 hazard levels (MAE), with relatively low dispersion (RMSE = 0.765; MSE = 0.585). Adjacent accuracy was exceptionally high (94.5% within ±1 level), while extreme misclassifications were nearly absent (99.5% within ±2 levels; 100% within ±3 levels). These findings confirm that catastrophic errors (e.g., predicting Low when the true level was High) were virtually eliminated. This aligns with EDA evidence showing that extreme meteorological conditions such as heavy snowfall or very strong winds rarely overlapped with Low hazard conditions, providing the model with clearer separation at the extremes.

Finally, correlation-based metrics confirmed that the network preserved the ordinal structure of avalanche hazard levels. Strong monotonic associations were observed (Spearman’s ρ = 0.738, Pearson r = 0.723), with Kendall’s Tau (0.670, p < 0.001) and directional accuracy (0.718) showing that the ordering of predicted hazard levels was largely consistent with the true sequence. Together, these results show that while the model achieved only moderate exact-match classification accuracy, it maintained ordinal consistency, avoided severe misclassifications, and delivered meaningful improvements over baseline predictors.

| Metric                         | Value   |
|--------------------------------|---------|
| Training Accuracy              | 0.716   |
| Validation Accuracy            | 0.614   |
| Test Accuracy                  | 0.606   |
| Test Loss                      | 0.993   |
| Macro Precision                | 0.499   |
| Macro Recall                   | 0.452   |
| Macro F1-Score                 | 0.459   |
| Weighted Precision             | 0.589   |
| Weighted Recall                | 0.606   |
| Weighted F1-Score              | 0.594   |
| Cohen’s Kappa                  | 0.443   |
| Mean Absolute Error (MAE)      | 0.454   |
| Root Mean Squared Error (RMSE) | 0.765   |
| Mean Squared Error (MSE)       | 0.585   |
| Adjacent Accuracy (±1 level)   | 0.945   |
| Within-2 Accuracy (±2 levels)  | 0.995   |
| Within-3 Accuracy (±3 levels)  | 1.000   |
| Spearman’s ρ                   | 0.738   |
| Pearson r                      | 0.723   |
| Kendall’s Tau                  | 0.670   |
| Directional Accuracy           | 0.718   |

*Table 2: Overall Performance Metrics of the Neural Network*

### Confusion Matrix and Class-Level Performance

The confusion matrix offered a clear breakdown of how the model performed across individual hazard levels. For the Low hazard class, the network achieved strong results (sensitivity = 0.836, precision = 0.723, F1 = 0.775), reflecting consistent identification of stable snowpack conditions. This strength is consistent with EDA findings, which showed that Low days were associated with distinct signals — shallow snow depth, weak drift, and stable temperature gradients — making them easier for the model to separate. Similarly, the Moderate class reached moderate reliability (sensitivity = 0.540, precision = 0.578, F1 = 0.558), supported by its high prevalence in the dataset and relatively consistent meteorological profiles. Together, these two classes accounted for most correct predictions, reflecting both their dominance in the dataset and their clearer predictor signatures.

The Considerable– class performed less consistently (precision = 0.526, recall = 0.553, F1 = 0.539). Misclassifications were concentrated between adjacent levels: 79 cases confused with Moderate and 20 with Considerable+. This behavior aligns with the transitional nature of predictors highlighted in the EDA, where snow depth, crystal type, and summit air temperature overlapped substantially between Moderate and Considerable– categories.

Performance deteriorated sharply for the Considerable+ and High hazard levels, which achieved very low recall (0.241 and 0.091, respectively) and correspondingly low F1-scores (0.280 and 0.143). High cases were most often misclassified as Considerable– (14 cases) or Considerable+ (15 cases), but crucially, none were mistaken for Low. This indicates that while the model struggled to distinguish among upper hazard levels, it preserved ordinal structure and avoided catastrophic misclassifications. The EDA findings explain this limitation: these rare categories comprised less than 5% of the dataset and were characterized by overlapping meteorological signals such as heavy snowfall, wind drift, and deep weak layers. With limited training samples, the network could not fully disentangle these patterns.

The per-class F1-scores reflected this gradient in performance: Low (0.775) and Moderate (0.558) were acceptable, Considerable– was moderate (0.539), while Considerable+ (0.280) and High (0.143) were poor. These values highlight the central trade-off of the model: strong utility in predicting stable and moderately unstable conditions, but weak reliability in capturing rare but operationally critical high-risk categories.

| Predicted \ Actual | Low | Moderate | Considerable – | Considerable + | High |
|--------------------|-----|----------|----------------|----------------|------|
| Low                | 315 | 105      | 11             | 5              | 0    |
| Moderate           | 55  | 205      | 79             | 15             | 1    |
| Considerable –     | 7   | 65       | 141            | 41             | 14   |
| Considerable +     | 0   | 5        | 20             | 20             | 15   |
| High               | 0   | 0        | 4              | 2              | 3    |

*Table 3: Confusion Matrix of Predicted vs Actual Avalanche Hazard Levels*

| Class           | Precision | Recall | F1-Score | Support |
|-----------------|-----------|--------|----------|---------|
| Low             | 0.723     | 0.836  | 0.775    | 377     |
| Moderate        | 0.578     | 0.540  | 0.558    | 380     |
| Considerable –  | 0.526     | 0.553  | 0.539    | 255     |
| Considerable +  | 0.333     | 0.241  | 0.280    | 83      |
| High            | 0.333     | 0.091  | 0.143    | 33      |

*Table 4: Precision, Recall, and F1-Score by Avalanche Hazard Class*



### Confidence-Based Reliability

Most predictions were made with low confidence (≤0.6, 62.9%), while 23.0% were medium (0.6–0.8) and only 14.1% were high (>0.8). Prediction accuracy scaled with confidence: 88.7% at high confidence, 65.3% at medium, and 52.7% at low confidence. At the class level, Low hazard predictions averaged the highest confidence (0.708) with strong accuracy (83.6%), while Moderate (0.551) and Considerable– (0.511) showed low confidence and middling reliability.Predictions for Considerable+ (0.527) and High (0.584) carried similarly low confidence but were far less accurate (24.1% and 9.1%, respectively), highlighting systematic difficulty with rare high-risk categories. These results confirm that prediction probabilities provided a meaningful gradient of reliability, with higher confidence strongly associated with higher accuracy.

| Confidence Level   | Accuracy |
|--------------------|----------|
| High (> 0.8)       | 88.7%    |
| Medium (0.6–0.8)   | 65.3%    |
| Low (≤ 0.6)        | 52.7%    |

*Table 5: Accuracy of Predictions by Confidence Level*


| Confidence Level   | Accuracy |
|--------------------|----------|
| High (> 0.8)       | 88.7%    |
| Medium (0.6–0.8)   | 65.3%    |
| Low (≤ 0.6)        | 52.7%    |

*Table 6: Accuracy of Predictions by Confidence Level*



### Avalanche-Specific Safety Metrics

The model achieved a Critical Miss Rate of 18.1%, showing that nearly one in five severe avalanche cases (Considerable+ or High) were underestimated as Low or Moderate. Safety Effectiveness reached 74.6%, indicating that in most cases the model predicted at or above the true hazard level. A negative Conservative Bias (–0.114) revealed a mild tendency to underestimate risk overall. High-risk detection showed mixed outcomes: Sensitivity was 70.1%, meaning most but not all severe cases were detected, while Specificity was 89.8%, reflecting strong reliability in avoiding false alarms.

| Metric                   | Value   | Interpretation                                      |
|--------------------------|---------|-----------------------------------------------------|
| Critical Miss Rate       | 18.1%   | Severe events underestimated as Low/Moderate        |
| Safety Effectiveness     | 74.7%   | Predictions at or above true hazard level           |
| Conservative Bias        | –0.114  | Mild overall tendency to underestimate risk         |
| High-Risk Sensitivity    | 70.1%   | Ability to detect Considerable– and above           |
| High-Risk Specificity    | 89.8%   | Ability to avoid false alarms on lower-risk levels  |

*Table 7: Avalanche-Specific Safety Metrics*


## Feature Importance Assessment

The feature importance analysis revealed that predictors strongly aligned with established avalanche science and EDA findings. Foot Penetration and Drift were the top-ranked features, reflecting their direct links to snowpack weakness and wind-driven slab formation. Key atmospheric drivers — Summit Air Temperature and Total Snow Depth — also ranked highly, confirming their influence on hazard variability observed in the EDA. Crystal Type added further insight into snow microstructure transitions, especially between Moderate and Considerable– categories.

Wind- and temperature-related factors such as Air Temperature, Wind Chill, and Wind Speed consistently scored highly across methods, echoing EDA findings on strong thermal gradients and wind redistribution as critical instability drivers. Seasonal and temporal features — particularly Winter, Day of Year, and Snow–Altitude Interaction — captured the clustering of high hazard levels during winter peaks and at higher elevations. Geographical indicators such as Lochaber and Torridon added spatial context, though with secondary importance.

In summary, the most important features reflected a combination of snowpack properties, meteorological drivers, and seasonal patterns, confirming that the model’s predictive behaviour was rooted in meaningful physical processes identified during the exploratory analysis.

::: {#fig-feature-importance}
![Group of graphs and diagrams showing feature importance](../R work/Graphs/fig7.jpg)

Feature importance graphs
:::

## Conclusion and Recommendations

This study evaluated the application of a neural network model to forecast avalanche hazard levels in Scotland, integrating exploratory data analysis, feature engineering, and rigorous model evaluation.The neural network achieved a training accuracy of 71.6%, a validation accuracy of 61.4%, and a test accuracy of 60.6%, showing good generalization without major overfitting. While exact classification accuracy was moderate, 94.5% of predictions were within ±1 hazard level, and errors averaged only 0.45 levels, indicating that the model preserved ordinal structure and avoided large-scale misclassifications. Performance was strongest for Low and Moderate hazards but much weaker for rare severe levels (Considerable+ and High), reflected in a Critical Miss Rate of 18.1% and a Safety Effectiveness of 74.6%

Feature importance confirmed that avalanche risk was primarily driven by snowpack properties (Foot Penetration, Snow Depth, Crystal Type), wind-related processes (Drift, Wind Chill, Wind Speed), and temperature gradients (Summit Air Temperature, Air Temperature), alongside seasonal and regional influences.

To improve performance, future work should focus on better representation of high-hazard cases, possibly through expanded datasets or transfer learning. Ensemble methods and uncertainty-based outputs could also increase operational reliability. While not yet suitable as a standalone system, the model shows strong potential as a decision-support tool when combined with expert judgment.


## Author Contributions
- Brendon Pretorius: Formal Analysis, Project Administration, Writing – Review & Editing
- Johan John: Formal Analysis, Writing – Original Draft Preparation
- Nabil Patel: Formal Analysis, Methodology
- Nkateko Mawelele: Formal Analysis, Writing – Original Draft Preparation  

## References (APA)

<a id="ref-bishop-2006"></a>Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

<a id="ref-eaws-nd"></a>European Avalanche Warning Services. (n.d.). *Danger scale and standards*. https://www.avalanches.org

<a id="ref-goodfellow-2016"></a>Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press.

<a id="ref-juvik-2023"></a>Juvik, E. S. (2023). Toward data-driven decision support for avalanche-exposed roads in Norway. In *Proceedings of the International Snow Science Workshop* (ISSW 2023). Norwegian Public Roads Administration.

<a id="ref-mcclung-2006"></a>McClung, D., & Schaerer, P. (2006). *The avalanche handbook* (3rd ed.). The Mountaineers Books.

<a id="ref-sais-nd"></a>Scottish Avalanche Information Service. (n.d.). *About SAIS and daily avalanche reports*. https://www.sais.gov.uk

<a id="ref-schweizer-2003"></a>Schweizer, J., Jamieson, B., & Schneebeli, M. (2003). Snow avalanche formation. *Reviews of Geophysics, 41*(4), 1–25. https://doi.org/10.1029/2002RG000123

<a id="ref-techel-2014"></a>Techel, F., & Zweifel, B. (2014). Recreational avalanche accidents in Switzerland: Trends and patterns with an emphasis on burial, rescue methods and avalanche danger. *Cold Regions Science and Technology, 97*, 77–85. https://doi.org/10.1016/j.coldregions.2013.09.006

<a id="ref-techel-2016"></a>Techel, F., Jarry, F., Kronthaler, G., Mitterer, C., Nairz, P., Pavšek, M., Valt, M., & Zweifel, B. (2016). Avalanche fatalities in the European Alps: Long-term trends and perspectives. *Cold Regions Science and Technology, 121*, 22–31. https://doi.org/10.1016/j.coldregions.2015.11.017