[
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "",
    "text": "Avalanche forecasting integrates mountain meteorology, snowpack physics, terrain analysis, and risk communication to reduce loss of life and support safer winter travel. In Scotland, the Scottish Avalanche Information Service (SAIS) publishes daily next-day hazard assessments across six high-mountain regions during winter which draws upon field observations, snowpack tests and weather guidance (Scottish Avalanche Information Service [SAIS], n.d.). This report contributes a data-driven complement to that long-running operational effort. Our task is defined as to construct and evaluate a neural network model that predicts the forecasted avalanche hazard (FAH)."
  },
  {
    "objectID": "paper.html#introduction",
    "href": "paper.html#introduction",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "",
    "text": "Avalanche forecasting integrates mountain meteorology, snowpack physics, terrain analysis, and risk communication to reduce loss of life and support safer winter travel. In Scotland, the Scottish Avalanche Information Service (SAIS) publishes daily next-day hazard assessments across six high-mountain regions during winter which draws upon field observations, snowpack tests and weather guidance (Scottish Avalanche Information Service [SAIS], n.d.). This report contributes a data-driven complement to that long-running operational effort. Our task is defined as to construct and evaluate a neural network model that predicts the forecasted avalanche hazard (FAH)."
  },
  {
    "objectID": "paper.html#background",
    "href": "paper.html#background",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Background",
    "text": "Background\nScotland’s avalanche forecasts are produced by the Scottish Avalanche Information Service (SAIS). The SAIS conducts daily fieldwork, snow profiles, stability tests and terrain analysis that is blended with meteorological guidance to issue next-day public danger ratings, snowpack summaries and travel advice. This information spans Torridon, Northern Cairngorms, Southern Cairngorms, Creag Meagaidh, Lochaber, and Glencoe. The communication follows the standard EAWS danger scale to keep messages consistent and comparable.\nAvalanches are rapid snow flows that can contain ice, rock, and vegetation. The most dangerous to travellers are slab avalanches, where a cohesive slab slides on a weaker layer once shear strength is exceeded by gravitational and external loading. The triggers for an avalanche may be natural (snowfall, wind loading, warming) or manmade (a skier, climber, or snowmobile). These avalanches most often occur on slopes around 34–45 degrees. In Scotland’s maritime, wind-dominated winters wind slabs are common (Schweizer, Jamieson, & Schneebeli, 2003).\nAvalanche forecasting looks to improving consistency, calibration, and timeliness of hazard assessments which can essentially save lives and reduce societal costs. In Europe, despite substantial growth in backcountry participation, the long-term average annual avalanche fatality count has remained broadly steady at 100 per year across the Alps. This is attributed in part to better education, equipment, and forecasting (Techel et al., 2016). Scotland’s totals are smaller, but fatal and non-fatal involvements recur most winters and SAIS seasonal reports routinely document hundreds of observed avalanches (SAIS, n.d.). The impacts however extend beyond casualty numbers as they tend to disrupt transport, tourism, and emergency services."
  },
  {
    "objectID": "paper.html#dataset",
    "href": "paper.html#dataset",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Dataset",
    "text": "Dataset\nThe dataset is an operational archive produced by the Scottish Avalanche Information Service (SAIS). It spans approximately fifteen winter seasons and reflects real-time fieldwork. Forecasters collect these records in severe weather environments and under time constraints. As a result, the data shows uneven sampling across storm cycles and regions, along with missing entries during periods of limited access. The various features of the dataset can be seen in Appendix\nThe feature set can be categorised into five distinct groups that collectively capture the multifaceted nature of avalanche risk assessment:\nSpatial and Temporal Features - Location data (coordinates, altitude) and dates that help track where and when avalanches occur across different regions and seasons.\nTopographical Features - Terrain characteristics like slope angle and direction that affect avalanche risk (steeper slopes increase stress on snow; direction impacts wind, sun, and temperature).\nMeteorological Features - Weather conditions including temperature, wind, clouds, precipitation, and sunlight that control how snow accumulates and changes.\nSnowpack Physical Properties - Direct measurements of the snow itself: depth, penetration resistance, temperature, crystal types, moisture content, and wind-drifted snow patterns.\nSnowpack Stability Indicators - Specialized measurements and calculations (temperature/hardness changes, settling rates, composite indices) that assess the internal structure and stress within the snowpack to predict avalanche danger.\nTogether, these variables provide a comprehensive framework for understanding and forecasting avalanche hazards in Scottish mountains. [@anthropic2024claude].\nBy relating these predictors to FAH the model can learn recurrent, non-linear patterns. It highlights situations where higher hazard is more likely, so forecasters can focus their checks and make cleaner day-to-day calls (EAWS, n.d.; SAIS, n.d.)."
  },
  {
    "objectID": "paper.html#methodology",
    "href": "paper.html#methodology",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Methodology",
    "text": "Methodology\nThis solution developed a supervised neural network classifier to predict next-day FAH using 16 years of SAIS operational data. The methodology followed a systematic machine learning pipeline implemented in R with reproducibility ensured through fixed random seeds.\nThe dataset underwent comprehensive exploratory data analysis to assess missing data patterns, outlier distributions, and class imbalances. Non-predictive variables (observed avalanche hazard, observer identifiers, and location codes) were removed to prevent data leakage. Observations with missing target values were excluded, and FAH categories were standardized into five ordinal risk levels: Low, Moderate, Considerable-, Considerable+, and High. Statistical outlier detection using the interquartile range method combined with domain-specific constraints removed physically implausible values while preserving legitimate extreme weather events.\nNew meteorologically meaningful variables were created, including wind chill indices, temperature gradients, snow-altitude interactions, and trigonometric aspect transformations. Temporal features (month, day of year, season) captured seasonal avalanche patterns. A multi-method feature selection approach combined LASSO regularisation, Random Forest importance scoring, and correlation analysis to identify the 25 most predictive variables using weighted composite scores.\nThe dataset was split into training (70%), validation (15%), and test (15%) sets using stratified sampling to maintain class proportions. To address severe class imbalance (original ratio 7.2:1), the Random Over-Sampling Examples (ROSE) technique was applied exclusively to training data using a one-versus-rest strategy, reducing imbalance to approximately 1.2:1 while preserving natural distributions in evaluation sets.\nA three-layer deep neural network was implemented using Keras/TensorFlow with progressive layer sizes (256→128→64 neurons), batch normalization for training stability, and dropout regularization (rates: 0.3, 0.4, 0.5) to prevent overfitting. The model employed categorical crossentropy loss with Adam optimization and incorporated Gaussian noise injection for additional regularisation. Hyperparameter tuning evaluated three configurations, selecting the optimal based on validation accuracy.\nThe final model was trained for up to 150 epochs with early stopping and learning rate reduction callbacks. Evaluation employed both standard multiclass metrics (accuracy, precision, recall, F1-score) and ordinal-specific measures including adjacent accuracy (±1 level), mean absolute error, and safety-critical metrics such as conservative bias and critical miss rates. This comprehensive evaluation framework assessed both predictive performance and operational safety implications for avalanche forecasting.\nWe used large language models (LLMs) to accelerate routine coding tasks. Each team member supplied an LLM with a structured brief (data schema, variable descriptions, target definition, leakage constraints, evaluation metrics) to draft boilerplate R code for preprocessing, imputation functions, model scaffolding (keras3), and plotting. To promote diversity of ideas and reduce single-model bias, different teammates intentionally used different LLMs. This use of LLMs sped up drafting, but final code, methodological choices, and validation remained human-curated. Our detailed statement on the use of generative artificial intelligence (AI) is included in LLM Usage and Reflection. In the next section, we initiated exploratory data analysis (EDA) before developing our model."
  },
  {
    "objectID": "paper.html#exploratory-data-analysis",
    "href": "paper.html#exploratory-data-analysis",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThis section of the report examines the key characteristics of the Scottish avalanche forecast dataset to inform modeling decisions and identify potential challenges. In particular, it investigates the distribution and balance of the target variable (FAH), assesses missing data patterns across all variables to guide preprocessing strategies, and explores temporal coverage to understand seasonal patterns and data collection consistency.\n\nTarget Variable Distribution\nThis section examines the distribution of our target variable (FAH) to understand class balance and identify potential modeling challenges that arise from the natural rarity of high-risk avalanche conditions.\n\n\n\n\n\n\nFigure 1: FAH Distribution\n\n\n\nFrom Figure 1, we observe that the FAH target variable exhibits severe class imbalance, with safer classes dominating the distribution: Low and Moderate together account for approximately 63% of observations, while Considerable- is moderately represented at 23.6%. In contrast, the higher-risk classes i.e., Considerable+ and especially High are rare. This skew implies that a model could achieve high overall accuracy by favoring common classes i.e., naive classification, while failing to detect infrequent but critical high-hazard events. To mitigate this, we later employ ROSE balancing.\n\n\nMissing Data Analysis\nThis section assesses missing data patterns across all variables to identify problematic predictors and inform data quality decisions for preprocessing.\n\n\n\n\n\n\nFigure 2: Missing data analysis graph\n\n\n\nFrom Figure 2, the missing data diagnostics reveal that AV_Cat and Ski_Pen exhibit the highest gaps (≈20%+), prompting their removal from the dataset during preprocessing. Following these, a cluster of snowpack microstructure and condition variables (e.g., Crystals, Wetness, No Settle) show moderate missingness, alongside certain wind-direction and temperature fields. Therefore, we implemented data imputation techniques to account for the missing data in these features. The target variable FAH has negligible missingness; however, we removed observations with missing data in the target variable.\n\n\nData Quality Issues & Outliers\nOur outlier analysis during EDA revealed significant data quality issues across multiple features. Altitude measurements contained two extreme outliers (244,859m and 77,044m) that were physically impossible given Scotland’s topography, where the highest peak reaches only 1,345m. Aspect measurements included values exceeding 163,770 degrees, far beyond the valid compass range of 0-360 degrees. Wind speed data exhibited a broad range from negative values to 290 km/h, with summit wind speeds reaching up to 360 km/h—values that likely represent measurement errors rather than genuine hurricane-force conditions in Scottish mountains. Snow depth measurements displayed extreme variability, ranging from negative values to an implausible maximum of 3,000cm (30 metres), significantly exceeding typical Scottish mountain snow accumulation patterns. Cloud cover data contained impossible values, including negative percentages and readings up to 199%, exceeding the theoretical maximum of 100%. Both foot and ski penetration measurements included negative values, with foot penetration ranging up to 300cm and ski penetration reaching 55cm. Incline data spanned from negative values to 1,020 degrees, with numerous observations exceeding 90 degrees. Extreme values were addressed through a combination of IQR-based removal for most variables and domain-specific thresholds for others, with negative values corrected to zero and upper limits applied based on physical plausibility within the Scottish mountain context.\n\n\nTemporal Analysis\nThis section examines the temporal distribution of observations to understand seasonal patterns, data collection consistency, and potential time-based features for modeling.\n\n\n\n\n\n\nFigure 3: Temporal analysis graphs\n\n\n\nFrom Figure 3, it can be observed that the dataset spans from 2009 to 2025, providing 16 years of avalanche forecasting data with remarkable consistency. The monthly distribution reveals the expected strong seasonal concentration, with the vast majority of observations occurring during winter months (December through March). Summer months (May through November) show minimal or zero activity, which aligns perfectly with avalanche season expectations in Scottish mountains where snow conditions are primarily a winter phenomenon. These temporal patterns supported the inclusion of seasonal and monthly features in our neural network model, as demonstrated in the feature engineering section Feature Engineering.\n\n\nComparison of Actual and Predicted Observations\nThis section evaluates the relationship between forecast and observed avalanche hazard levels to establish baseline performance expectations and identify inherent prediction challenges.\n\n\n\n\n\n\nFigure 4: Graph comparing FAH versus OAH pairs\n\n\n\nFrom Figure 4, which analyses FAH versus OAH pairs as a verification benchmark for forecast reliability, we observe an overall accuracy of approximately 70–75%. Per-class performance varies notably: accuracy is highest for Low risk (mid-90s%), followed by Moderate (70%), then decreases for Considerable- (60–70%), drops substantially for Considerable+ (35%), and improves slightly for High (~45%). This pattern suggests that forecasters—and by extension, predictive models—struggle most with distinguishing adjacent hazard boundaries. Accordingly, our model evaluation emphasises ordinal metrics such as adjacent accuracy (±1 level), critical-miss rate (underpredicting high risk), and high-risk sensitivity to ensure practical utility in avalanche forecasting.\n\n\nCorrelation analysis\nThis section evaluates the correlations between the features as well as the correlations between the features and the output.\n\n\n\n\n\n\nFigure 5: Correlation heatmap\n\n\n\nThe correlation heatmap in Figure 5 reveals coherent but mostly moderate correlation clusters among variables. Notably, temperature features (Air temperature, and summit air temperature) exhibit strong covariation, while cloud and insolation show the expected inverse relationship. Wind speeds correlate across levels with some directional noise, and penetration/strength metrics (Foot penetration and ski penetration depth) associate with total snow depth in predictable ways. Meanwhile, maximum temperature gradient and maximum hardness gradient relate to temperature and snow-structure variables through more complex mechanisms, likely tied to metamorphic processes in the snowpack. Overall, this indicates potential multicollinearity, particularly among thermodynamic and wind features, which could destabilise linear models and hinder neural network convergence. To address this, we implemented ensemble feature selection combining LASSO regularisation, random forest importance scoring, and correlation-based filtering to minimise redundancy.\n\n\nEDA Conclusion\nThe target variable (FAH) exhibits severe class imbalance, with safer conditions (Low and Moderate) dominating approximately 63% of observations whilst higher-risk categories (Considerable+ and High) remain rare. To address this imbalance, ROSE sampling was applied to the training data using a one-versus-rest strategy, generating synthetic samples to achieve more balanced class distributions whilst preserving natural distributions in validation and test sets. Missing data patterns reveal structured gaps rather than random missingness. Variables AV.Cat and Ski.Pen show the highest missingness rates (&gt;20%), prompting their removal during preprocessing. Additional moderate gaps appear in snowpack microstructure variables and certain meteorological fields, leading to the implementation of imputation strategies for missing data. The temporal analysis confirms expected seasonal patterns, with observations concentrated heavily in winter months (December-March) across a consistent 16-year span from 2009-2025. This seasonal clustering directly motivated the creation of temporal features including month, day of year, and season variables, whilst the stable annual collection volumes validate the dataset’s temporal consistency for model training. Forecast verification analysis comparing FAH versus OAH reveals overall accuracy of approximately 70-75%, with performance varying substantially across risk levels. Accuracy peaks for Low risk conditions (mid-90s%) but deteriorates progressively through higher-risk categories, reaching lowest levels for Considerable+ (35%). This pattern highlights the inherent difficulty in distinguishing adjacent hazard boundaries, directly informing the adoption of ordinal-specific evaluation metrics including adjacent accuracy (±1 level), critical miss rates, and high-risk sensitivity measures rather than relying solely on overall classification accuracy. The following section discusses our data refinement strategy."
  },
  {
    "objectID": "paper.html#data-refinement",
    "href": "paper.html#data-refinement",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Data Refinement",
    "text": "Data Refinement\nThe raw Scottish avalanche forecast dataset required systematic preprocessing to ensure reliable neural network training whilst preserving the meteorological and topographical signals essential for hazard prediction. Our data refinement approach balanced the competing demands of maintaining data integrity and achieving computational tractability, recognising that avalanche conditions exhibit strong spatial and temporal dependencies that must be preserved throughout the cleaning process. The following subsections detail our systematic approach to feature removal, outlier detection, feature engineering, missing value imputation, target encoding, feature selection, and class balancing, culminating in a refined dataset of 7,530 complete observations suitable for robust neural network training and evaluation.\n\nFeature Removal\nOur preprocessing commenced with the removal of non-predictive administrative fields, specifically OAH (observed avalanche hazard) and Obs (observer identifier), since our objective focused on predicting forecasted rather than observed conditions. In addition, after the imputation section, we removed non-predictive features i.e., Date, OSgrid and location.\nWe eliminated 109 observations lacking FAH values, leaving 10 562 viable records. Missing data analysis exposed significant gaps in AV.Cat (23.4%) and Ski.Pen (22.5%), which we excluded to avoid unreliable imputation. The remaining variables exhibited structured rather than random missingness patterns, with core meteorological and topographical measurements showing minimal gaps (≤3%), supporting our subsequent contextual imputation approach.\n\n\nOutlier Removal\nOur outlier removal strategy involved a dual-tier outlier detection protocol combining statistical principles with domain expertise. Initially, we converted string representations of missing values to proper NA indicators and applied robust Interquartile Range filtering (with a scalar parameter of k=3.5) to meteorological variables prone to measurement errors i.e.:\n\nAlt (2)\nWind.Speed (22)\nSummit.Wind.Speed (81)\nTotal.Snow.Depth (111)\nAir.Temp (1)\n\nFoot.Pen (34)\n\nSubsequently, we enforced physical plausibility constraints, removing Aspect measurements exceeding 360 degrees (8 cases), constraining Cloud coverage to 0-100% (3 cases), and limiting Incline to physically realistic slopes between 0-90 degrees (5 cases). This conservative approach eliminated 267 observations (2.53% of the dataset) whilst preserving legitimate extreme weather events that provide crucial signal for hazard prediction. The methodology successfully removed erroneous readings that would distort feature scaling and hinder neural network optimisation, without sacrificing valuable information about genuine storm conditions.\n\n\nFeature Engineering\nWe developed five engineered features that encode avalanche formation mechanisms directly into the predictor space. Wind chill, calculated as air temperature minus 0.6 times wind speed, approximates near-surface cooling effects that influence snowpack stability under windy conditions. Temperature gradient, representing the difference between summit air temperature and air temperature, captures atmospheric stability and potential for snow transport processes. Snow altitude interaction multiplies total snow depth by altitude scaled to kilometres, reflecting elevation-dependent snow accumulation patterns and rain-snow transition zones. Recognising the circular nature of compass bearings, we transformed aspect into orthogonal components i.e., aspect north using cosine transformation and aspect east using sine transformation, eliminating artificial discontinuities at 360° and 0°. Temporal features i.e., Month, day, and season were extracted to capture the pronounced seasonal patterns evident in the EDA, enabling the network to learn both inter- and intra-seasonal risk variations.\n\n\nImputation for Missing Values\nMissing value imputation employed a hierarchical approach reflecting the spatial and temporal structure of avalanche conditions. We employed area and seasonal medians to impute missing values in numerical features, while categorical features were imputed using modal values. This ensures replacement values reflect typical conditions for specific regions and seasons rather than dataset-wide averages that could misrepresent local climate patterns.\n\n\nEncoding\nThe next step in our model involved encoding categorical features and the target variable. Target encoding transformed the categorical FAH variable into an ordinal integer sequence (Low=0, Moderate=1, Considerable-=2, Considerable+=3, High=4), preserving the natural risk hierarchy essential for ordinal classification evaluation. For predictors, we cleaned categorical fields and applied a frequency threshold (≥50) so very rare levels were folded into “Other” to limit sparsity.\n\n\nFeature Selection\nThe model implements a comprehensive feature selection process. The process begins by separating features from the target variable and standardising numerical features using centre and scale normalisation to ensure all variables are on comparable scales. The model employs a multi-method approach that combines three different feature selection techniques to create a robust scoring framework. First, it uses LASSO regularisation with cross-validation to identify features with non-zero coefficients, scoring them based on the absolute magnitude of their coefficients. Second, it applies Random Forest to evaluate feature importance, though it limits the analysis to the top 50 most correlated features if the dataset is too large. Finally, it calculates correlation-based scores by measuring the absolute correlation between each feature and the target variable.\nThe innovative aspect of this approach lies in its composite scoring system, which weights each method’s contributions (LASSO 30%, Random Forest 40%, and correlation 30%) and provides a bonus for features that perform well across multiple methods. Features selected by more methods receive higher total scores through a multiplicative bonus factor. The process concludes by ranking all features according to their composite scores and selecting the top 25 features for subsequent modelling. This methodology ensures that the selected features are not only individually predictive but also consistently identified across different selection techniques, potentially leading to more robust model performance.\n\n\nStratified Splitting and Class Balance Correction\nThe refined dataset of 7 530 observations underwent stratified partitioning to maintain proportional class representation across training (70%, 5,273 samples), validation (15%, 1,129 samples), and test (15%, 1,128 samples) sets. This preserved the natural class imbalance for realistic evaluation whilst ensuring adequate representation of each hazard level.\nClass imbalance in the training set showed a severe 12.4:1 ratio between majority and minority classes, necessitating corrective sampling. We applied ROSE exclusively to training data post-split to prevent data leakage, using an iterative one-versus-rest strategy to accommodate ROSE’s binary classification requirement. Synthetic samples were generated for minority classes until reaching approximately 95% of majority class size, avoiding perfect balance that might introduce artificial patterns.\nThe training set grew from 5,273 to 8,641 samples, with ~1,700 per class, reducing the imbalance ratio to 1.05:1 (a 91.5% improvement). This enhanced minority representation for neural network training while ensuring realistic evaluation.\n\n\n\n\n\n\nFigure 6: Test data class distribution\n\n\n\n\nRose Balanced Training Data:\n\n\n\n\n\n\nFigure 7: ROSE Balanced test data class distribution\n\n\n\n\n\nNeural Network Architecture\nFollowing various hyperparameter tuning with three candidate configurations, the optimal network achieved a validation accuracy of 0.6138 with a corresponding training accuracy of 0.7159. This represented the best trade-off between learning capacity and generalisation. The final architecture consisted of three hidden layers with 512, 256, and 128 units, respectively, paired with progressively increasing dropout regularisation (0.20, 0.30, 0.40). Gaussian noise was applied at the input layer to mitigate overfitting and improve robustness against noisy environmental predictors. The network was trained using the Adam optimiser with a learning rate of 5×10⁻⁴ and a batch size of 32, which provided stable convergence during training.\n\n\n\n\nLayer\nUnits\nDropout\nNotes\n\n\n\n\nInput\n–\n–\n25 selected features\n\n\nHidden Layer 1\n512\n0.20\nDense + BatchNorm + GaussianNoise\n\n\nHidden Layer 2\n256\n0.30\nDense + BatchNorm + Dropout\n\n\nHidden Layer 3\n128\n0.40\nDense + BatchNorm + Dropout\n\n\nOutput\n5\n–\nSoftmax (multiclass, ordinal)\n\n\n\nTable 1: Final neural network architecture\n\nThe training and validation curves showed that the model achieved stable convergence, though validation accuracy plateaued around 0.61, reflecting the intrinsic difficulty of predicting the rare high-risk classes.\nThe model accuracy can be seen by:\n\n\n\n\n\n\nFigure 8: Model Accuracy\n\n\n\n\nThe model loss can be seen by:\n\n\n\n\n\n\nFigure 9: Model Loss"
  },
  {
    "objectID": "paper.html#model-performance-evaluation",
    "href": "paper.html#model-performance-evaluation",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Model Performance Evaluation",
    "text": "Model Performance Evaluation\n\nOverall Performance\nThe neural network’s performance was assessed across the training, validation, and test datasets to evaluate both its learning capacity and generalisation ability. During training, the model achieved a best training accuracy of 71.6%, with the corresponding best validation accuracy reaching 61.4%. The final test accuracy was 60.6% (loss = 0.993). The gap between training and validation performance indicates some overfitting, which is expected in complex models, but the close alignment of validation and test results shows that the network generalised reasonably well to unseen data and avoided substantial performance degradation. Beyond test accuracy, complementary metrics provided deeper insight into generalization.\nThe various performance metrics are summarised below:\n\n\n\n\nMetric\nValue\n\n\n\n\nTraining Accuracy\n0.716\n\n\nValidation Accuracy\n0.614\n\n\nTest Accuracy\n0.606\n\n\nTest Loss\n0.993\n\n\nMacro Precision\n0.499\n\n\nMacro Recall\n0.452\n\n\nMacro F1-Score\n0.459\n\n\nWeighted Precision\n0.589\n\n\nWeighted Recall\n0.606\n\n\nWeighted F1-Score\n0.594\n\n\nCohen’s Kappa\n0.443\n\n\nMean Absolute Error (MAE)\n0.454\n\n\nRoot Mean Squared Error (RMSE)\n0.765\n\n\nMean Squared Error (MSE)\n0.585\n\n\nAdjacent Accuracy (±1 level)\n0.945\n\n\nWithin-2 Accuracy (±2 levels)\n0.995\n\n\nWithin-3 Accuracy (±3 levels)\n1.000\n\n\nSpearman’s ρ\n0.738\n\n\nPearson r\n0.723\n\n\nKendall’s Tau\n0.670\n\n\nDirectional Accuracy\n0.718\n\n\n\nTable 2: Overall Performance Metrics of the Neural Network\n\nMacro-averaged scores (precision = 0.499, recall = 0.452, F1 = 0.459) showed that the model was only moderately effective when treating all classes equally. Weighted averages were higher (precision = 0.589, recall = 0.606, F1 = 0.594), reflecting stronger performance on the dominant classes. Cohen’s Kappa, at 0.443, placed the model in the “moderate agreement” range, showing that predictions were significantly better than random guessing but not yet highly reliable. EDA findings again provide explanation: the removal of several snowpack variables with &gt;20% missingness reduced the depth of predictive features, while extreme but plausible outliers (e.g., very high winds, deep snowpack events) complicated the learning process, particularly for rarer hazard categories.\nGiven the ordinal nature of avalanche hazard levels, error magnitudes were as important as raw accuracy. Predictions deviated by an average of only 0.45 hazard levels (MAE), with relatively low dispersion (RMSE = 0.765; MSE = 0.585). Adjacent accuracy was exceptionally high (94.5% within ±1 level), while extreme misclassifications were nearly absent (99.5% within ±2 levels; 100% within ±3 levels). These findings confirm that catastrophic errors (e.g., predicting Low when the true level was High) were virtually eliminated. This aligns with EDA evidence showing that extreme meteorological conditions such as heavy snowfall or very strong winds rarely overlapped with Low hazard conditions, providing the model with clearer separation at the extremes.\nFinally, correlation-based metrics confirmed that the network preserved the ordinal structure of avalanche hazard levels. Strong monotonic associations were observed (Spearman’s ρ = 0.738, Pearson r = 0.723), with Kendall’s Tau (0.670, p &lt; 0.001) and directional accuracy (0.718) showing that the ordering of predicted hazard levels was largely consistent with the true sequence. Together, these results show that while the model achieved only moderate exact-match classification accuracy, it maintained ordinal consistency, avoided severe misclassifications, and delivered meaningful improvements over baseline predictors.\n\n\nConfusion Matrix and Class-Level Performance\nThe confusion matrix offered a clear breakdown of how the model performed across individual hazard levels:\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted  Actual\nLow\nModerate\nConsiderable –\nConsiderable +\nHigh\n\n\n\n\nLow\n315\n105\n11\n5\n0\n\n\nModerate\n55\n205\n79\n15\n1\n\n\nConsiderable –\n7\n65\n141\n41\n14\n\n\nConsiderable +\n0\n5\n20\n20\n15\n\n\nHigh\n0\n0\n4\n2\n3\n\n\n\nTable 3: Confusion Matrix of Predicted vs Actual Avalanche Hazard Levels\n\n\nFrom the confusion matrix, following hazard-level model performance metrics were obtained:\n\n\n\n\nClass\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\nLow\n0.723\n0.836\n0.775\n377\n\n\nModerate\n0.578\n0.540\n0.558\n380\n\n\nConsiderable –\n0.526\n0.553\n0.539\n255\n\n\nConsiderable +\n0.333\n0.241\n0.280\n83\n\n\nHigh\n0.333\n0.091\n0.143\n33\n\n\n\nTable 4: Precision, Recall, and F1-Score by Avalanche Hazard Class\n\nFor the Low hazard class, the network achieved strong results (precision = 0.723, recall = 0.836, F1 = 0.775), reflecting consistent identification of stable snowpack conditions. This strength is consistent with EDA findings, which showed that Low days were associated with distinct signals — shallow snow depth, weak drift, and stable temperature gradients — making them easier for the model to separate. Similarly, the Moderate class reached moderate reliability (precision = 0.578, recall = 0.540, F1 = 0.558). Together, these two classes accounted for most correct predictions, reflecting both their dominance in the dataset and their clearer predictor signatures.\nThe Considerable– class performed less consistently (precision = 0.526, recall = 0.553, F1 = 0.539). Misclassifications were concentrated between adjacent levels: 79 cases confused with Moderate and 20 with Considerable+.\nPerformance deteriorated sharply for the Considerable+ and High hazard levels, which achieved very low recall (0.241 and 0.091, respectively) and correspondingly low F1-scores (0.280 and 0.143). High cases were most often misclassified as Considerable– (14 cases) or Considerable+ (15 cases), but crucially, none were mistaken for Low. This indicates that while the model struggled to distinguish among upper hazard levels, it preserved ordinal structure and avoided catastrophic misclassifications. The EDA findings explain this limitation: these rare categories comprised less than 5%. With limited training samples, the network could not fully disentangle these patterns.\nThe per-class F1-scores reflected this gradient in performance: Low (0.775) and Moderate (0.558) were acceptable, Considerable– was moderate (0.539), while Considerable+ (0.280) and High (0.143) were poor. These values highlight the central trade-off of the model: strong utility in predicting stable and moderately unstable conditions, but weak reliability in capturing rare but operationally critical high-risk categories.\n\n\nConfidence-Based Reliability\nTo evaluate the reliability of the model’s predictions, we examined the distribution of confidence scores and their associated accuracy across different levels.\n\n\n\n\nConfidence Level\nCount\nPercentage\n\n\n\n\nHigh (&gt; 0.8)\n159\n14.1%\n\n\nMedium (0.6–0.8)\n259\n23.0%\n\n\nLow (≤ 0.6)\n710\n62.9%\n\n\n\nTable 5: Distribution of Predictions by Confidence Level\n\n\n\n\n\n\nConfidence Level\nAccuracy\n\n\n\n\nHigh (&gt; 0.8)\n88.7%\n\n\nMedium (0.6–0.8)\n65.3%\n\n\nLow (≤ 0.6)\n52.7%\n\n\n\nTable 6: Accuracy of Predictions by Confidence Level\n\n\nThe prediction confidence distribution can be seen by:\n\n\n\n\n\n\nFigure 10: Prediction Confidence Distribution\n\n\n\nMost predictions were made with low confidence (≤0.6, 62.9%), while 23.0% were medium (0.6–0.8) and only 14.1% were high (&gt;0.8). Prediction accuracy scaled with confidence: 88.7% at high confidence, 65.3% at medium, and 52.7% at low confidence. At the class level, Low hazard predictions averaged the highest confidence (0.708) with strong accuracy (83.6%), while Moderate (0.551) and Considerable– (0.511) showed low confidence and middling reliability.Predictions for Considerable+ (0.527) and High (0.584) carried similarly low confidence but were far less accurate (24.1% and 9.1%, respectively), highlighting systematic difficulty with rare high-risk categories. These results confirm that prediction probabilities provided a meaningful gradient of reliability, with higher confidence strongly associated with higher accuracy.\n\n\nAvalanche-Specific Safety Metrics\nThe model achieved a Critical Miss Rate of 18.1%, showing that nearly one in five severe avalanche cases (Considerable+ or High) were underestimated as Low or Moderate. Safety Effectiveness reached 74.6%, indicating that in most cases the model predicted at or above the true hazard level. A negative Conservative Bias (–0.114) revealed a mild tendency to underestimate risk overall. High-risk detection showed mixed outcomes: Sensitivity was 70.1%, meaning most but not all severe cases were detected, while Specificity was 89.8%, reflecting strong reliability in avoiding false alarms.\n\n\n\n\n\n\n\n\n\nMetric\nValue\nInterpretation\n\n\n\n\nCritical Miss Rate\n18.1%\nSevere events underestimated as Low/Moderate\n\n\nSafety Effectiveness\n74.7%\nPredictions at or above true hazard level\n\n\nConservative Bias\n–0.114\nMild overall tendency to underestimate risk\n\n\nHigh-Risk Sensitivity\n70.1%\nAbility to detect Considerable– and above\n\n\nHigh-Risk Specificity\n89.8%\nAbility to avoid false alarms on lower-risk levels\n\n\n\nTable 7: Avalanche-Specific Safety Metrics"
  },
  {
    "objectID": "paper.html#feature-importance-assessment",
    "href": "paper.html#feature-importance-assessment",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Feature Importance Assessment",
    "text": "Feature Importance Assessment\nThe feature importance analysis revealed that predictors strongly aligned with established avalanche science and EDA findings. Foot Penetration and Drift were the top-ranked features, reflecting their direct links to snowpack weakness and wind-driven slab formation. Key atmospheric drivers — Summit Air Temperature and Total Snow Depth — also ranked highly, confirming their influence on hazard variability observed in the EDA. Crystal Type added further insight into snow microstructure transitions, especially between Moderate and Considerable– categories.\nWind- and temperature-related factors such as Air Temperature, Wind Chill, and Wind Speed consistently scored highly across methods, echoing EDA findings on strong thermal gradients and wind redistribution as critical instability drivers. Seasonal and temporal features — particularly Winter, Day of Year, and Snow–Altitude Interaction — captured the clustering of high hazard levels during winter peaks and at higher elevations. Geographical indicators such as Lochaber and Torridon added spatial context, though with secondary importance.\nIn summary, the most important features reflected a combination of snowpack properties, meteorological drivers, and seasonal patterns, confirming that the model’s predictive behaviour was rooted in meaningful physical processes identified during the exploratory analysis.\n\n\n\n\nRank\nFeature\nTotal Score\n\n\n\n\n1\nFoot Penetration\n134.8\n\n\n2\nDrift\n114.6\n\n\n3\nSummit Air Temp\n106.4\n\n\n4\nTotal Snow Depth\n92.0\n\n\n5\nCrystals\n80.6\n\n\n6\nAir Temp\n78.6\n\n\n7\nWind Chill\n65.5\n\n\n8\nPrecip Code: None\n65.2\n\n\n9\nPrecip Code: Snow\n64.1\n\n\n10\nSnow Index\n63.6\n\n\n\nTable 8: Top 10 most important predictors of avalanche hazard"
  },
  {
    "objectID": "paper.html#conclusion-and-recommendations",
    "href": "paper.html#conclusion-and-recommendations",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Conclusion and Recommendations",
    "text": "Conclusion and Recommendations\nThis study evaluated the application of a neural network model to forecast avalanche hazard levels in Scotland, integrating exploratory data analysis, feature engineering, and rigorous model evaluation.The neural network achieved a training accuracy of 71.6%, a validation accuracy of 61.4%, and a test accuracy of 60.6%, showing good generalization without major overfitting. While exact classification accuracy was moderate, 94.5% of predictions were within ±1 hazard level, and errors averaged only 0.45 levels, indicating that the model preserved ordinal structure and avoided large-scale misclassifications. Performance was strongest for Low and Moderate hazards but much weaker for rare severe levels (Considerable+ and High), reflected in a Critical Miss Rate of 18.1% and a Safety Effectiveness of 74.6%\nFeature importance confirmed that avalanche risk was primarily driven by snowpack properties (Foot Penetration, Snow Depth, Crystal Type), wind-related processes (Drift, Wind Chill, Wind Speed), and temperature gradients (Summit Air Temperature, Air Temperature), alongside seasonal and regional influences.\nTo improve performance, future work should focus on better representation of high-hazard cases, possibly through expanded datasets or transfer learning. Ensemble methods and uncertainty-based outputs could also increase operational reliability. While not yet suitable as a standalone system, the model shows strong potential as a decision-support tool when combined with expert judgment."
  },
  {
    "objectID": "paper.html#author-contributions",
    "href": "paper.html#author-contributions",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "Author Contributions",
    "text": "Author Contributions\n\nBrendon Pretorius: Formal Analysis, Project Administration, Writing – Review & Editing\nJohan John: Formal Analysis, Writing – Original Draft Preparation\nNabil Patel: Formal Analysis, Methodology, Writing – Original Draft Preparation\nNkateko Mawelele: Formal Analysis, Writing – Original Draft Preparation"
  },
  {
    "objectID": "paper.html#references",
    "href": "paper.html#references",
    "title": "Scientific Paper: Neural Network for Forecasted Avalanche Hazard",
    "section": "References",
    "text": "References\nAnthropic. (2024). Claude Sonnet 4 (December 28 version) [Large language model]. https://claude.ai/chat\nEuropean Avalanche Warning Services. (n.d.). Danger scale and standards. https://www.avalanches.org\nScottish Avalanche Information Service. (n.d.). About SAIS and daily avalanche reports. https://www.sais.gov.uk\nSchweizer, J., Jamieson, B., & Schneebeli, M. (2003). Snow avalanche formation. Reviews of Geophysics, 41(4), 1–25. https://doi.org/10.1029/2002RG000123\nTechel, F., & Zweifel, B. (2014). Recreational avalanche accidents in Switzerland: Trends and patterns with an emphasis on burial, rescue methods and avalanche danger. Cold Regions Science and Technology, 97, 77–85. https://doi.org/10.1016/j.coldregions.2013.09.006"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA5073Z Assignment 1: Avalanche Hazard Prediction",
    "section": "",
    "text": "Welcome to our Avalanche Hazard Prediction project.\n- Scientific Paper\n- LLM Reflection\n- Appendix\n Source: wallpapers.com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Data\nThe following table provides descriptions, datatypes, and basic distributional analysis for the dataset variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nData Type\nExample\nDefinition\nNA’s/Blanks\nMissing Percentage (%)\nMin\n1st Qu\nMedian\nMean\n3rd Qu\nMax\n\n\n\n\nDate\ndate\n17/12/2009 12:30\nThe date and time when the observation or forecast was recorded.\n0\n0\n-\n-\n-\n-\n-\n-\n\n\nArea\nchr\nCreag Meagaidh\nThe specific mountain region or forecast area in Scotland (e.g., Creag Meagaidh, Torridon).\n0\n0\n-\n-\n-\n-\n-\n-\n\n\nlongitude\nnum\n-0.392\nThe longitude coordinate of the observation location in decimal degrees.\n0\n0\n-1.2021\n-0.5424\n-0.4315\n-0.3984\n-0.2705\n0.5701\n\n\nlatitude\nnum\n2.22\nThe latitude coordinate of the observation location in decimal degrees.\n0\n0\n1.361\n1.982\n2.198\n2.298\n2.711\n3.08\n\n\nAlt\nint\n800\nAltitude of the observation site in meters above sea level.\n6\n0.05622716\n-1\n800\n910\n934\n1030\n244859\n\n\nAspect\nint\n160\nThe compass direction (in degrees) that the slope faces, relevant to avalanche risk.\n355\n3.3267735\n-1\n50\n90\n153.2\n225\n163770\n\n\nIncline\nint\n28\nThe slope angle (in degrees) where the observation was taken, critical for avalanche potential.\n36\n0.33736295\n-1\n22\n28\n26.76\n32\n1020\n\n\nObs\nchr\nWS\nObservation type or condition code (e.g., WS for Wet Snow, TR for Tracks), indicating snow or terrain state.\n21\n0.197795988\n-\n-\n-\n-\n-\n-\n\n\nFAH\nchr\nModerate\nForecast Avalanche Hazard, the predicted avalanche danger level for the next 24 hours.\n109\n1.026655364\n-\n-\n-\n-\n-\n-\n\n\nOAH\nchr\nModerate\nObserved Avalanche Hazard, the assessed avalanche danger based on current field observations.\n453\n4.266742018\n-\n-\n-\n-\n-\n-\n\n\nAir.Temp\nnum\n-3.2\nAir temperature (in °C) at the observation site, influencing snow stability.\n34\n0.31862056\n-10.8\n-2.1\n-0.2\n0.06021\n2.1\n17\n\n\nWind.Dir\nnum\n45\nWind direction (in degrees) affecting snow distribution and drift formation.\n158\n1.48064849\n-2.6\n150\n220\n202.1\n270\n905\n\n\nWind.Speed\nnum\n10\nWind speed (in km/h or mph), impacting snow transport and avalanche risk.\n53\n0.49667323\n-2\n8\n15\n16.34\n22\n290\n\n\nCloud\nint\n90\nCloud cover percentage, affecting temperature and precipitation patterns.\n29\n0.2717646\n-1\n70\n100\n80.21\n100\n199\n\n\nPrecip.Code\nchr\n2 - Trace\nPrecipitation code indicating type and intensity (e.g., snow, rain, trace).\n583\n5.491193369\n-\n-\n-\n-\n-\n-\n\n\nDrift\nint\n1\nIndicator of snow drifting (1 = present, 0 = absent), affecting stability.\n0\n0\n-\n-\n-\n-\n-\n-\n\n\nTotal.Snow.Depth\nint\n45\nTotal depth of snow cover (in cm) at the observation site.\n163\n1.52750445\n-1\n45\n70\n95.75\n118\n3000\n\n\nFoot.Pen\nnum\n0\nFoot penetration depth (in cm), indicating snowpack consistency.\n42\n0.3935901\n-1\n5\n10\n13.96\n20\n300\n\n\nSki.Pen\nint\nNA\nSki penetration depth (in cm), showing snowpack support for skiing.\n2404\n22.52834786\n-1\n0\n0\n0.5928\n0\n55\n\n\nRain.at.900\nint\n0\nRainfall at 900m altitude (in mm or binary indicator), affecting snow melt.\n0\n0\n0\n0\n0\n0.1912\n0\n1\n\n\nSummit.Air.Temp\nnum\nNA\nAir temperature (in °C) at the summit, influencing upper snowpack conditions.\n754\n7.06587949\n-13.4\n-3.6\n-1.3\n-1.24\n0.8\n15\n\n\nSummit.Wind.Dir\nint\nNA\nWind direction (in degrees) at the summit, impacting snow distribution.\n1318\n12.35123231\n-2\n160\n220\n200.1\n250\n2213\n\n\nSummit.Wind.Speed\nint\nNA\nWind speed (in km/h or mph) at the summit, affecting snow transport.\n909\n8.51841439\n-8\n14\n25\n27.89\n38\n360\n\n\nMax.Temp.Grad\nnum\n20\nMaximum temperature gradient (in °C) across the snowpack, indicating instability.\n710\n6.653547\n0\n0\n0.2\n1.211\n1\n130\n\n\nMax.Hardness.Grad\nint\n4\nMaximum hardness gradient within the snowpack, reflecting layer differences.\n629\n5.89448037\n0\n1\n2\n2.037\n3\n5\n\n\nNo.Settle\nint\nNA\nNumber of days since the last significant snowfall, affecting settlement.\n289\n2.70827476\n-1\n54\n128\n139.1\n206\n676\n\n\nSnow.Index\nint\nNA\nAn index or score related to snowpack stability or quality (specific method unclear).\n745\n6.98153875\n-1\n0\n0\n1.786\n0\n368\n\n\nInsolation\nint\nNA\nSolar radiation or insolation impact (in hours or intensity), affecting snow melt.\n507\n4.75119483\n-55\n3\n6\n7.594\n10\n208\n\n\nCrystals\nint\nNA\nType or size of snow crystals observed, influencing avalanche risk.\n988\n9.25873864\n-1\n0\n0\n1.45\n0\n15\n\n\nWetness\nint\nNA\nDegree of snow wetness (scale or percentage), impacting stability.\n576\n5.39780714\n-1\n1\n1\n1.414\n2\n10\n\n\nAV.Cat\nint\nNA\nAvalanche Category, likely a classification of avalanche type or severity.\n2494\n23.37175522\n-9999\n0\n0\n-321.7\n0\n8800\n\n\nSnow.Temp\nnum\nNA\nTemperature (in °C) within the snowpack, critical for stability assessment.\n410\n3.84218911\n-13.1\n-2.2\n-0.4\n-0.7149\n0\n124\n\n\n\n\nR Code\n\n# Scottish Avalanche Hazard Prediction using Enhanced Neural Networks\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(keras3)\nlibrary(tensorflow)\nlibrary(glmnet)\nlibrary(VIM)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(ROSE)\nlibrary(randomForest)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(lubridate)\nlibrary(irr)\nlibrary(MLmetrics)\n\n# Check if corrplot is available, if not, skip correlation plot\ncorrplot_available &lt;- requireNamespace(\"corrplot\", quietly = TRUE)\n\n# Set random seed for reproducibility\nset.seed(42)\n\n# DATA LOADING AND INITIAL EXPLORATION\n# Assuming the marker has the dataset loaded in the source directory\n# Load the dataset\ndata &lt;- read.csv(\"scotland_avalanche_forecasts_2009_2025.csv\", stringsAsFactors = FALSE)\n\nThe section below covers exploratory data analysis (EDA) and primarily focuses on providing an overview of the data set.\n\n# DATASET OVERVIEW & STRUCTURE\n\n\ncat(\"\\n=== DATASET OVERVIEW & STRUCTURE ===\\n\")\n\n\n=== DATASET OVERVIEW & STRUCTURE ===\n\n# Dataset dimensions\ncat(sprintf(\"Dataset Dimensions: %d rows × %d columns\\n\", nrow(data), ncol(data)))\n\nDataset Dimensions: 10671 rows × 34 columns\n\n# Data structure analysis\ndata_structure &lt;- data.frame(\n  Variable = colnames(data),\n  Type = sapply(data, class),\n  Non_Missing = sapply(data, function(x) sum(!is.na(x) & x != \"\" & x != \"NA\")),\n  Missing = sapply(data, function(x) sum(is.na(x) | x == \"\" | x == \"NA\")),\n  Missing_Pct = round(sapply(data, function(x) sum(is.na(x) | x == \"\" | x == \"NA\") / length(x) * 100), 2),\n  Unique_Values = sapply(data, function(x) {\n    clean_x &lt;- x[!is.na(x) & x != \"\" & x != \"NA\"]\n    length(unique(clean_x))\n  }),\n  stringsAsFactors = FALSE\n)\n\n# Identify variable types\nnumeric_vars &lt;- data_structure$Variable[data_structure$Type %in% c(\"numeric\", \"integer\")]\ncategorical_vars &lt;- data_structure$Variable[data_structure$Type %in% c(\"character\", \"factor\")]\n\ndata_structure$Variable_Type &lt;- ifelse(data_structure$Variable %in% numeric_vars, \"Numeric\", \"Categorical\")\n\ncat(\"Data Structure Summary:\\n\")\n\nData Structure Summary:\n\nkable(data_structure, caption = \"Dataset Structure Overview\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDataset Structure Overview\n\n\n\nVariable\nType\nNon_Missing\nMissing\nMissing_Pct\nUnique_Values\nVariable_Type\n\n\n\n\nDate\nDate\ncharacter\n10671\n0\n0.00\n9715\nCategorical\n\n\nArea\nArea\ncharacter\n10671\n0\n0.00\n6\nCategorical\n\n\nOSgrid\nOSgrid\ncharacter\n10671\n0\n0.00\n3525\nCategorical\n\n\nlongitude\nlongitude\nnumeric\n10671\n0\n0.00\n3525\nNumeric\n\n\nlatitude\nlatitude\nnumeric\n10671\n0\n0.00\n3525\nNumeric\n\n\nAlt\nAlt\ninteger\n10665\n6\n0.06\n690\nNumeric\n\n\nAspect\nAspect\ninteger\n10316\n355\n3.33\n342\nNumeric\n\n\nIncline\nIncline\ninteger\n10635\n36\n0.34\n55\nNumeric\n\n\nLocation\nLocation\ncharacter\n10662\n9\n0.08\n3401\nCategorical\n\n\nObs\nObs\ncharacter\n10650\n21\n0.20\n708\nCategorical\n\n\nFAH\nFAH\ncharacter\n10562\n109\n1.02\n5\nCategorical\n\n\nOAH\nOAH\ncharacter\n10218\n453\n4.25\n5\nCategorical\n\n\nAir.Temp\nAir.Temp\nnumeric\n10637\n34\n0.32\n223\nNumeric\n\n\nWind.Dir\nWind.Dir\nnumeric\n10513\n158\n1.48\n320\nNumeric\n\n\nWind.Speed\nWind.Speed\nnumeric\n10618\n53\n0.50\n72\nNumeric\n\n\nCloud\nCloud\ninteger\n10642\n29\n0.27\n35\nNumeric\n\n\nPrecip.Code\nPrecip.Code\ncharacter\n10088\n583\n5.46\n6\nCategorical\n\n\nDrift\nDrift\ninteger\n10671\n0\n0.00\n2\nNumeric\n\n\nTotal.Snow.Depth\nTotal.Snow.Depth\ninteger\n10508\n163\n1.53\n268\nNumeric\n\n\nFoot.Pen\nFoot.Pen\nnumeric\n10629\n42\n0.39\n74\nNumeric\n\n\nSki.Pen\nSki.Pen\ninteger\n8267\n2404\n22.53\n28\nNumeric\n\n\nRain.at.900\nRain.at.900\ninteger\n10671\n0\n0.00\n2\nNumeric\n\n\nSummit.Air.Temp\nSummit.Air.Temp\nnumeric\n9917\n754\n7.07\n221\nNumeric\n\n\nSummit.Wind.Dir\nSummit.Wind.Dir\ninteger\n9353\n1318\n12.35\n352\nNumeric\n\n\nSummit.Wind.Speed\nSummit.Wind.Speed\ninteger\n9762\n909\n8.52\n143\nNumeric\n\n\nMax.Temp.Grad\nMax.Temp.Grad\nnumeric\n9961\n710\n6.65\n109\nNumeric\n\n\nMax.Hardness.Grad\nMax.Hardness.Grad\ninteger\n10042\n629\n5.89\n6\nNumeric\n\n\nNo.Settle\nNo.Settle\ninteger\n10382\n289\n2.71\n325\nNumeric\n\n\nSnow.Index\nSnow.Index\ninteger\n9926\n745\n6.98\n56\nNumeric\n\n\nInsolation\nInsolation\ninteger\n10164\n507\n4.75\n17\nNumeric\n\n\nCrystals\nCrystals\ninteger\n9683\n988\n9.26\n9\nNumeric\n\n\nWetness\nWetness\ninteger\n10095\n576\n5.40\n8\nNumeric\n\n\nAV.Cat\nAV.Cat\ninteger\n8177\n2494\n23.37\n16\nNumeric\n\n\nSnow.Temp\nSnow.Temp\nnumeric\n10261\n410\n3.84\n236\nNumeric\n\n\n\n\n\n\ncat(sprintf(\"\\nVariable Type Distribution:\\n\"))\n\n\nVariable Type Distribution:\n\ncat(sprintf(\"- Numeric variables: %d\\n\", length(numeric_vars)))\n\n- Numeric variables: 26\n\ncat(sprintf(\"- Categorical variables: %d\\n\", length(categorical_vars)))\n\n- Categorical variables: 8\n\n# Summary statistics for numeric variables\nif(length(numeric_vars) &gt; 0) {\n  cat(\"\\n=== NUMERIC VARIABLES SUMMARY STATISTICS ===\\n\")\n  \n  # Convert to numeric and handle non-numeric values\n  numeric_data &lt;- data[, numeric_vars, drop = FALSE]\n  for(col in names(numeric_data)) {\n    numeric_data[[col]] &lt;- as.numeric(ifelse(numeric_data[[col]] == \"NA\", NA, numeric_data[[col]]))\n  }\n  \n  numeric_summary &lt;- numeric_data %&gt;%\n    summarise_all(list(\n      Count = ~sum(!is.na(.)),\n      Mean = ~round(mean(., na.rm = TRUE), 3),\n      Median = ~round(median(., na.rm = TRUE), 3),\n      SD = ~round(sd(., na.rm = TRUE), 3),\n      Min = ~round(min(., na.rm = TRUE), 3),\n      Max = ~round(max(., na.rm = TRUE), 3),\n      Q25 = ~round(quantile(., 0.25, na.rm = TRUE), 3),\n      Q75 = ~round(quantile(., 0.75, na.rm = TRUE), 3)\n    )) %&gt;%\n    gather(key = \"Metric\", value = \"Value\") %&gt;%\n    separate(Metric, into = c(\"Variable\", \"Statistic\"), sep = \"_\") %&gt;%\n    spread(key = \"Statistic\", value = \"Value\")\n  \n  kable(numeric_summary, caption = \"Summary Statistics for Numeric Variables\") %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n}\n\n\n=== NUMERIC VARIABLES SUMMARY STATISTICS ===\n\n\n\nSummary Statistics for Numeric Variables\n\n\nVariable\nCount\nMax\nMean\nMedian\nMin\nQ25\nQ75\nSD\n\n\n\n\nAir.Temp\n10637\n17.000\n0.060\n-0.200\n-10.800\n-2.100\n2.100\n3.380\n\n\nAlt\n10665\n244859.000\n933.974\n910.000\n-1.000\n800.000\n1030.000\n2480.596\n\n\nAspect\n10316\n163770.000\n153.156\n90.000\n-1.000\n50.000\n225.000\n1614.867\n\n\nAV.Cat\n8177\n8800.000\n-321.677\n0.000\n-9999.000\n0.000\n0.000\n1800.360\n\n\nCloud\n10642\n199.000\n80.212\n100.000\n-1.000\n70.000\n100.000\n31.607\n\n\nCrystals\n9683\n15.000\n1.450\n0.000\n-1.000\n0.000\n0.000\n3.192\n\n\nDrift\n10671\n1.000\n0.415\n0.000\n0.000\n0.000\n1.000\n0.493\n\n\nFoot.Pen\n10629\n300.000\n13.960\n10.000\n-1.000\n5.000\n20.000\n12.965\n\n\nIncline\n10635\n1020.000\n26.765\n28.000\n-1.000\n22.000\n32.000\n16.583\n\n\nInsolation\n10164\n208.000\n7.594\n6.000\n-55.000\n3.000\n10.000\n5.811\n\n\nlatitude\n10671\n57.901\n56.970\n56.948\n56.151\n56.817\n57.111\n0.275\n\n\nlongitude\n10671\n-2.975\n-4.399\n-4.580\n-6.174\n-4.963\n-3.652\n0.711\n\n\nMax.Hardness.Grad\n10042\n5.000\n2.037\n2.000\n0.000\n1.000\n3.000\n0.966\n\n\nMax.Temp.Grad\n9961\n130.000\n1.211\n0.200\n0.000\n0.000\n1.000\n4.083\n\n\nNo.Settle\n10382\n676.000\n139.111\n128.000\n-1.000\n54.000\n206.000\n99.647\n\n\nRain.at.900\n10671\n1.000\n0.191\n0.000\n0.000\n0.000\n0.000\n0.393\n\n\nSki.Pen\n8267\n55.000\n0.593\n0.000\n-1.000\n0.000\n0.000\n2.946\n\n\nSnow.Index\n9926\n368.000\n1.786\n0.000\n-1.000\n0.000\n0.000\n12.612\n\n\nSnow.Temp\n10261\n124.000\n-0.715\n-0.400\n-13.100\n-2.200\n0.000\n6.136\n\n\nSummit.Air.Temp\n9917\n15.000\n-1.240\n-1.300\n-13.400\n-3.600\n0.800\n3.372\n\n\nSummit.Wind.Dir\n9353\n2213.000\n200.087\n220.000\n-2.000\n160.000\n250.000\n82.878\n\n\nSummit.Wind.Speed\n9762\n360.000\n27.891\n25.000\n-8.000\n14.000\n38.000\n25.803\n\n\nTotal.Snow.Depth\n10508\n3000.000\n95.753\n70.000\n-1.000\n45.000\n118.000\n96.388\n\n\nWetness\n10095\n10.000\n1.414\n1.000\n-1.000\n1.000\n2.000\n1.242\n\n\nWind.Dir\n10513\n905.000\n202.147\n220.000\n-2.600\n150.000\n270.000\n86.308\n\n\nWind.Speed\n10618\n290.000\n16.335\n15.000\n-2.000\n8.000\n22.000\n13.431\n\n\n\n\n\n\n\nThis next section of the Model continues EDA and focuses on the distribution of the target variable i.e., FAH.\nThis code chunk performs a comprehensive analysis of the target variable FAH (Forecast Avalanche Hazard), which is essential for understanding the distribution and quality of the response variable in our neural network model. The analysis begins by calculating the frequency distribution and percentages for each hazard level. A formatted table displays these statistics to provide clear insight into the class distribution. The chunk also quantifies missing data patterns by calculating both the count and percentage of missing FAH values versus valid observations, which is crucial for assessing data quality and potential preprocessing needs. To evaluate class imbalance, the code computes the ratio between the most and least frequent classes, identifying potential challenges for model training that may require techniques like class weighting or resampling. Finally, the analysis generates four visualizations: a bar plot showing the raw distribution of FAH levels, a pie chart displaying percentage distributions, and a bar chart comparing valid versus missing observations, providing both numerical and visual perspectives on the target variable’s characteristics that will inform subsequent modeling decisions.\n\n# TARGET VARIABLE ANALYSIS (FAH)\n\n# Define the SAIS-inspired palette (blue, grey, black tones)\nsais_palette &lt;- c(\n  \"#002d54\",  # Darker blue (deep mountain shadows)\n  \"#0065bd\",  # Blue (primary Scottish Gov blue, for key lines/fills)\n  \"#55a8f2\",  # Light blue (from sequential palette for snowy tones)\n  \"#000000\",  # Black (text, outlines)\n  \"#333333\",  # Dark grey (increased contrast from black)\n  \"#666666\",  # Grey (mid-tone fills)\n  \"#949494\",  # Medium grey (from focus palette, subtle accents)\n  \"#cccccc\",  # Light grey (highlights)\n  \"#f0f0f0\",  # Lighter grey (backgrounds)\n  \"#f8f8f8\"   # Very light grey/off-white (very light backgrounds)\n)\n\ncat(\"\\n=== TARGET VARIABLE ANALYSIS (FAH) ===\\n\")\n\n\n=== TARGET VARIABLE ANALYSIS (FAH) ===\n\n# Define ordinal levels\nfah_levels &lt;- c(\"Low\", \"Moderate\", \"Considerable -\", \"Considerable +\", \"High\")\n\n# FAH distribution analysis with ordinal factor\nfah_clean &lt;- data$FAH[!is.na(data$FAH) & data$FAH != \"\" & data$FAH != \"NA\"]\nfah_clean &lt;- factor(fah_clean, levels = fah_levels, ordered = TRUE)\nfah_distribution &lt;- table(fah_clean)\nfah_pct &lt;- round(prop.table(fah_distribution) * 100, 2)\n\nfah_summary &lt;- data.frame(\n  FAH_Level = names(fah_distribution),\n  Count = as.numeric(fah_distribution),\n  Percentage = as.numeric(fah_pct),\n  stringsAsFactors = FALSE\n)\n\ncat(\"FAH Distribution:\\n\")\n\nFAH Distribution:\n\nkable(fah_summary, caption = \"Forecast Avalanche Hazard (FAH) Distribution\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nForecast Avalanche Hazard (FAH) Distribution\n\n\nFAH_Level\nCount\nPercentage\n\n\n\n\nLow\n3433\n32.50\n\n\nModerate\n3239\n30.67\n\n\nConsiderable -\n2498\n23.65\n\n\nConsiderable +\n933\n8.83\n\n\nHigh\n459\n4.35\n\n\n\n\n\n\n# Missing FAH analysis\nfah_missing &lt;- sum(is.na(data$FAH) | data$FAH == \"\" | data$FAH == \"NA\")\nfah_missing_pct &lt;- round(fah_missing / nrow(data) * 100, 2)\n\ncat(sprintf(\"\\nMissing FAH Values:\\n\"))\n\n\nMissing FAH Values:\n\ncat(sprintf(\"- Missing count: %d (%.2f%%)\\n\", fah_missing, fah_missing_pct))\n\n- Missing count: 109 (1.02%)\n\ncat(sprintf(\"- Valid count: %d (%.2f%%)\\n\", nrow(data) - fah_missing, 100 - fah_missing_pct))\n\n- Valid count: 10562 (98.98%)\n\n# Class imbalance assessment\nif(length(fah_distribution) &gt; 1) {\n  max_class &lt;- max(fah_distribution)\n  min_class &lt;- min(fah_distribution)\n  imbalance_ratio &lt;- max_class / min_class\n  \n  cat(sprintf(\"\\nClass Imbalance Analysis:\\n\"))\n  cat(sprintf(\"- Most frequent class: %s (%d observations)\\n\", names(which.max(fah_distribution)), max_class))\n  cat(sprintf(\"- Least frequent class: %s (%d observations)\\n\", names(which.min(fah_distribution)), min_class))\n  cat(sprintf(\"- Imbalance ratio: %.2f:1\\n\", imbalance_ratio))\n}\n\n\nClass Imbalance Analysis:\n- Most frequent class: Low (3433 observations)\n- Least frequent class: High (459 observations)\n- Imbalance ratio: 7.48:1\n\n# First plot - Bar chart (ordinal order, colors light to dark for increasing risk)\npar(mar = c(6, 4, 3, 2))\nbarplot(fah_distribution, \n        main = \"FAH Distribution\", \n        ylab = \"Count\", \n        col = sais_palette[c(10,9,7,5,1)],  # Light to dark: Low (very light grey) to High (dark blue)\n        las = 2,\n        cex.names = 0.8)\n\n\n\n\n\n\n\n# Second plot - Pie chart (gets full window space)\npar(mar = c(2, 2, 3, 2))\npie(fah_distribution, \n    main = \"FAH Distribution (%)\", \n    col = sais_palette[c(10,9,7,5,1)],  # Same light-to-dark ordinal coloring\n    labels = paste0(names(fah_distribution), \"\\n(\", fah_pct, \"%)\"))\n\n\n\n\n\n\n\n# Third plot - Missing data\nmissing_data &lt;- c(nrow(data) - fah_missing, fah_missing)\nnames(missing_data) &lt;- c(\"Valid\", \"Missing\")\npar(mar = c(6, 4, 3, 2))\nbarplot(missing_data, \n        main = \"FAH Data Completeness\", \n        ylab = \"Count\",\n        col = sais_palette[c(3,6)])  # Light blue for valid, mid grey for missing\n\n\n\n\n\n\n\n\nThis code chunk conducts a systematic analysis of missing data patterns across all variables in the dataset. The analysis leverages the previously computed data structure summary to examine missing data percentages by variable, presenting results in a well-formatted table sorted by missing data severity. The chunk implements a threshold-based categorization system that identifies variables with high missing rates (&gt;20%) and moderate missing rates (5-20%), providing clear guidance on which variables may require special handling such as imputation, removal, or careful consideration during feature selection. Two complementary visualizations enhance the analysis: a color-coded bar plot highlighting the top 15 variables with the highest missing percentages using a traffic light system (red for high, orange for moderate, blue for low missingness), and an aggregation plot from the VIM package that creates a missing data heatmap for the top 20 most problematic variables.\n\n# \n# MISSING DATA ANALYSIS\n\ncat(\"\\n=== MISSING DATA ANALYSIS ===\\n\")\n\n\n=== MISSING DATA ANALYSIS ===\n\n# Variable-wise missing analysis (already computed in data_structure)\nmissing_summary &lt;- data_structure %&gt;%\n  select(Variable, Missing, Missing_Pct, Variable_Type) %&gt;%\n  arrange(desc(Missing_Pct))\n\ncat(\"Missing Data by Variable:\\n\")\n\nMissing Data by Variable:\n\nkable(missing_summary, caption = \"Missing Data Analysis by Variable\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nMissing Data Analysis by Variable\n\n\n\nVariable\nMissing\nMissing_Pct\nVariable_Type\n\n\n\n\nAV.Cat\nAV.Cat\n2494\n23.37\nNumeric\n\n\nSki.Pen\nSki.Pen\n2404\n22.53\nNumeric\n\n\nSummit.Wind.Dir\nSummit.Wind.Dir\n1318\n12.35\nNumeric\n\n\nCrystals\nCrystals\n988\n9.26\nNumeric\n\n\nSummit.Wind.Speed\nSummit.Wind.Speed\n909\n8.52\nNumeric\n\n\nSummit.Air.Temp\nSummit.Air.Temp\n754\n7.07\nNumeric\n\n\nSnow.Index\nSnow.Index\n745\n6.98\nNumeric\n\n\nMax.Temp.Grad\nMax.Temp.Grad\n710\n6.65\nNumeric\n\n\nMax.Hardness.Grad\nMax.Hardness.Grad\n629\n5.89\nNumeric\n\n\nPrecip.Code\nPrecip.Code\n583\n5.46\nCategorical\n\n\nWetness\nWetness\n576\n5.40\nNumeric\n\n\nInsolation\nInsolation\n507\n4.75\nNumeric\n\n\nOAH\nOAH\n453\n4.25\nCategorical\n\n\nSnow.Temp\nSnow.Temp\n410\n3.84\nNumeric\n\n\nAspect\nAspect\n355\n3.33\nNumeric\n\n\nNo.Settle\nNo.Settle\n289\n2.71\nNumeric\n\n\nTotal.Snow.Depth\nTotal.Snow.Depth\n163\n1.53\nNumeric\n\n\nWind.Dir\nWind.Dir\n158\n1.48\nNumeric\n\n\nFAH\nFAH\n109\n1.02\nCategorical\n\n\nWind.Speed\nWind.Speed\n53\n0.50\nNumeric\n\n\nFoot.Pen\nFoot.Pen\n42\n0.39\nNumeric\n\n\nIncline\nIncline\n36\n0.34\nNumeric\n\n\nAir.Temp\nAir.Temp\n34\n0.32\nNumeric\n\n\nCloud\nCloud\n29\n0.27\nNumeric\n\n\nObs\nObs\n21\n0.20\nCategorical\n\n\nLocation\nLocation\n9\n0.08\nCategorical\n\n\nAlt\nAlt\n6\n0.06\nNumeric\n\n\nDate\nDate\n0\n0.00\nCategorical\n\n\nArea\nArea\n0\n0.00\nCategorical\n\n\nOSgrid\nOSgrid\n0\n0.00\nCategorical\n\n\nlongitude\nlongitude\n0\n0.00\nNumeric\n\n\nlatitude\nlatitude\n0\n0.00\nNumeric\n\n\nDrift\nDrift\n0\n0.00\nNumeric\n\n\nRain.at.900\nRain.at.900\n0\n0.00\nNumeric\n\n\n\n\n\n\n# Threshold analysis\nhigh_missing_vars &lt;- missing_summary$Variable[missing_summary$Missing_Pct &gt; 20]\nmoderate_missing_vars &lt;- missing_summary$Variable[missing_summary$Missing_Pct &gt; 5 & missing_summary$Missing_Pct &lt;= 20]\n\ncat(sprintf(\"\\n=== MISSING DATA THRESHOLD ANALYSIS ===\\n\"))\n\n\n=== MISSING DATA THRESHOLD ANALYSIS ===\n\ncat(sprintf(\"Variables with &gt;20%% missing: %d\\n\", length(high_missing_vars)))\n\nVariables with &gt;20% missing: 2\n\nif(length(high_missing_vars) &gt; 0) {\n  cat(\"High missing variables:\", paste(high_missing_vars, collapse = \", \"), \"\\n\")\n}\n\nHigh missing variables: AV.Cat, Ski.Pen \n\ncat(sprintf(\"Variables with 5-20%% missing: %d\\n\", length(moderate_missing_vars)))\n\nVariables with 5-20% missing: 9\n\nif(length(moderate_missing_vars) &gt; 0) {\n  cat(\"Moderate missing variables:\", paste(moderate_missing_vars, collapse = \", \"), \"\\n\")\n}\n\nModerate missing variables: Summit.Wind.Dir, Crystals, Summit.Wind.Speed, Summit.Air.Temp, Snow.Index, Max.Temp.Grad, Max.Hardness.Grad, Precip.Code, Wetness \n\n# Missing data visualization\npar(mfrow = c(1, 2), mar = c(10, 4, 3, 2))\n\n# Top 15 variables by missing percentage\ntop_missing &lt;- head(missing_summary, 15)\nbarplot(top_missing$Missing_Pct, \n        names.arg = substr(top_missing$Variable, 1, 8), \n        main = \"Top 15 Variables by Missing %\", \n        ylab = \"Missing Percentage\",\n        col = ifelse(top_missing$Missing_Pct &gt; 20, \"red\",  # red for high missing\n                    ifelse(top_missing$Missing_Pct &gt; 5, sais_palette[1],  # Dark grey for moderate\n                           sais_palette[5])),  # Light grey for low\n        las = 2,\n        cex.names = 0.9)\n\n# Missing data heatmap (top 20 variables)\nif(require(VIM, quietly = TRUE)) {\n  top_20_vars &lt;- head(missing_summary$Variable, 20)\n  aggr(data[, top_20_vars], \n       col = c(sais_palette[8], sais_palette[1]),  # Light grey for present, dark blue for missing\n       numbers = TRUE, \n       sortVars = TRUE)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n          Variable       Count\n            AV.Cat 0.233717552\n           Ski.Pen 0.225283479\n   Summit.Wind.Dir 0.123512323\n          Crystals 0.092587386\n Summit.Wind.Speed 0.085184144\n   Summit.Air.Temp 0.070658795\n        Snow.Index 0.069815387\n     Max.Temp.Grad 0.066535470\n Max.Hardness.Grad 0.058944804\n           Wetness 0.053978071\n        Insolation 0.047511948\n         Snow.Temp 0.038421891\n               OAH 0.034579702\n            Aspect 0.033267735\n         No.Settle 0.027082748\n  Total.Snow.Depth 0.015275045\n          Wind.Dir 0.014806485\n        Wind.Speed 0.004966732\n       Precip.Code 0.000000000\n               FAH 0.000000000\n\npar(mfrow = c(1, 1))\n\nThis code chunk conducts geographical and temporal analyses to understand the spatial and time-based distribution of the avalanche forecast data. This section examines observation counts across different forecasting areas, creates summary tables showing area-specific statistics, and calculates geographic coverage using latitude and longitude coordinates for each region.\nBar plots and pie charts visualize the distribution of observations across areas to identify potential geographic biases or data concentration patterns. The temporal analysis converts date fields and examines the dataset’s time span, calculating yearly and monthly distributions to assess seasonal patterns typical in avalanche forecasting.\nTime series visualizations including daily observation counts, annual trends, and monthly patterns help identify data collection consistency and seasonal variations. The analysis concludes with data consistency metrics including mean annual observations, standard deviation, and coefficient of variation to evaluate temporal data quality.\n\n# GEOGRAPHICAL ANALYSIS\n\ncat(\"\\n GEOGRAPHICAL ANALYSIS \\n\")\n\n\n GEOGRAPHICAL ANALYSIS \n\nif(\"Area\" %in% colnames(data)) {\n  # Area distribution\n  area_distribution &lt;- table(data$Area)\n  area_summary &lt;- data.frame(\n    Area = names(area_distribution),\n    Count = as.numeric(area_distribution),\n    Percentage = round(as.numeric(prop.table(area_distribution) * 100), 2),\n    stringsAsFactors = FALSE\n  ) %&gt;%\n    arrange(desc(Count))\n  \n  cat(\"Observations by Forecasting Area:\\n\")\n  kable(area_summary, caption = \"Distribution of Observations by Area\") %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n  \n  # Geographic spread analysis\n  if(all(c(\"latitude\", \"longitude\") %in% colnames(data))) {\n    geo_summary &lt;- data %&gt;%\n      filter(!is.na(latitude) & !is.na(longitude) & !is.na(Area)) %&gt;%\n      group_by(Area) %&gt;%\n      summarise(\n        Observations = n(),\n        Min_Lat = round(min(latitude, na.rm = TRUE), 3),\n        Max_Lat = round(max(latitude, na.rm = TRUE), 3),\n        Mean_Lat = round(mean(latitude, na.rm = TRUE), 3),\n        Min_Lon = round(min(longitude, na.rm = TRUE), 3),\n        Max_Lon = round(max(longitude, na.rm = TRUE), 3),\n        Mean_Lon = round(mean(longitude, na.rm = TRUE), 3),\n        .groups = 'drop'\n      ) %&gt;%\n      arrange(desc(Observations))\n    \n    cat(\"\\nGeographic Spread by Area:\\n\")\n    kable(geo_summary, caption = \"Geographic Coverage by Forecasting Area\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n  }\n  \n  # Area visualization\n  par(mfrow = c(1, 2), mar = c(8, 4, 3, 2))\n  \n  # Top areas bar plot\n  top_areas &lt;- head(area_summary, 10)\n  barplot(top_areas$Count, \n          names.arg = substr(top_areas$Area, 1, 10), \n          main = \"Top 10 Areas by Observation Count\", \n          ylab = \"Count\",\n          col = rainbow(nrow(top_areas)),\n          las = 2,\n          cex.names = 0.7)\n  \n  # Area distribution pie chart (top 8 + others)\n  if(nrow(area_summary) &gt; 8) {\n    top_8_areas &lt;- head(area_summary, 8)\n    other_count &lt;- sum(tail(area_summary$Count, -8))\n    pie_data &lt;- c(top_8_areas$Count, other_count)\n    pie_labels &lt;- c(top_8_areas$Area, \"Others\")\n  } else {\n    pie_data &lt;- area_summary$Count\n    pie_labels &lt;- area_summary$Area\n  }\n  \n  pie(pie_data, \n      labels = paste0(substr(pie_labels, 1, 10), \"\\n(\", \n                     round(pie_data/sum(pie_data)*100, 1), \"%)\"),\n      main = \"Area Distribution\",\n      col = rainbow(length(pie_data)))\n  \n  par(mfrow = c(1, 1))\n}\n\nObservations by Forecasting Area:\n\nGeographic Spread by Area:\n\n\n\n\n\n\n\n\n# TEMPORAL ANALYSIS\n\ncat(\"\\n=== TEMPORAL ANALYSIS ===\\n\")\n\n\n=== TEMPORAL ANALYSIS ===\n\nif(\"Date\" %in% colnames(data)) {\n  # Convert date and handle missing values\n  data_with_dates &lt;- data %&gt;%\n    filter(!is.na(Date) & Date != \"\" & Date != \"NA\") %&gt;%\n    mutate(Date = as.Date(Date))\n  \n  if(nrow(data_with_dates) &gt; 0) {\n    # Time span analysis\n    date_range &lt;- range(data_with_dates$Date, na.rm = TRUE)\n    total_days &lt;- as.numeric(date_range[2] - date_range[1])\n    total_years &lt;- round(total_days / 365.25, 1)\n    \n    cat(sprintf(\"Data Collection Time Span:\\n\"))\n    cat(sprintf(\"- Start date: %s\\n\", date_range[1]))\n    cat(sprintf(\"- End date: %s\\n\", date_range[2]))\n    cat(sprintf(\"- Total period: %.1f years (%d days)\\n\", total_years, total_days))\n    \n    # Yearly distribution\n    data_with_dates$Year &lt;- year(data_with_dates$Date)\n    yearly_counts &lt;- table(data_with_dates$Year)\n    \n    yearly_summary &lt;- data.frame(\n      Year = as.numeric(names(yearly_counts)),\n      Count = as.numeric(yearly_counts),\n      Percentage = round(as.numeric(prop.table(yearly_counts) * 100), 2),\n      stringsAsFactors = FALSE\n    )\n    \n    cat(\"\\nObservations by Year:\\n\")\n    kable(yearly_summary, caption = \"Annual Distribution of Observations\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n    \n    # Monthly distribution\n    data_with_dates$Month &lt;- month(data_with_dates$Date)\n    monthly_counts &lt;- table(data_with_dates$Month)\n    month_names &lt;- month.name[as.numeric(names(monthly_counts))]\n    \n    monthly_summary &lt;- data.frame(\n      Month = month_names,\n      Count = as.numeric(monthly_counts),\n      Percentage = round(as.numeric(prop.table(monthly_counts) * 100), 2),\n      stringsAsFactors = FALSE\n    )\n    \n    cat(\"\\nObservations by Month:\\n\")\n    kable(monthly_summary, caption = \"Monthly Distribution of Observations\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n    \n    # Temporal visualizations\n    par(mfrow = c(2, 2), mar = c(8, 4, 3, 2))\n    \n    # Yearly distribution\n    barplot(yearly_counts, \n            main = \"Observations by Year\", \n            ylab = \"Count\",\n            col = sais_palette[2],  # Mid blue for yearly bars\n            las = 2)\n    \n    # Monthly distribution\n    barplot(monthly_counts, \n            names.arg = substr(month_names, 1, 3),\n            main = \"Observations by Month\", \n            ylab = \"Count\",\n            col = sais_palette[3],  # Light blue for monthly bars\n            las = 2)\n    \n    # Time series plot\n    daily_counts &lt;- data_with_dates %&gt;%\n      count(Date) %&gt;%\n      arrange(Date)\n    \n    plot(daily_counts$Date, daily_counts$n, \n         type = \"l\", \n         main = \"Daily Observation Count Over Time\",\n         xlab = \"Date\", \n         ylab = \"Daily Count\",\n         col = sais_palette[1])  # Dark blue for time series line\n    \n   # Data consistency check\n  consistency_stats &lt;- yearly_summary %&gt;%\n  summarise(\n    Mean_Annual = round(mean(Count), 1),\n    SD_Annual = round(sd(Count), 1),\n    CV_Annual = round(sd(Count)/mean(Count), 3)\n     )\n\n  plot(yearly_summary$Year, yearly_summary$Count,\n     type = \"b\",\n     main = \"Annual Data Consistency\",\n     xlab = \"Year\",\n     ylab = \"Annual Count\",\n     col = \"red\",\n     pch = 16)\n  abline(h = consistency_stats$Mean_Annual, col = \"blue\", lty = 2)\n\n    \n    cat(sprintf(\"\\nData Consistency Metrics:\\n\"))\n    cat(sprintf(\"- Mean annual observations: %.1f\\n\", consistency_stats$Mean_Annual))\n    cat(sprintf(\"- Standard deviation: %.1f\\n\", consistency_stats$SD_Annual))\n    cat(sprintf(\"- Coefficient of variation: %.3f\\n\", consistency_stats$CV_Annual))\n    \n    par(mfrow = c(1, 1))\n  }\n}\n\nData Collection Time Span:\n- Start date: 2009-12-17\n- End date: 2025-03-17\n- Total period: 15.2 years (5569 days)\n\nObservations by Year:\n\nObservations by Month:\n\n\n\n\n\n\n\n\n\n\nData Consistency Metrics:\n- Mean annual observations: 627.7\n- Standard deviation: 165.9\n- Coefficient of variation: 0.264\n\n\nThis code chunk assesses data quality through duplicate detection and forecast accuracy analysis. It identifies duplicate rows in the dataset and calculates their percentage. The chunk then performs a comprehensive comparison between forecast avalanche hazard (FAH) and observed avalanche hazard (OAH). A confusion matrix displays the relationship between predicted and actual hazard levels, while accuracy metrics are calculated for each risk level to identify patterns in forecasting performance. Visualizations include bar charts showing accuracy by hazard level and overall forecast performance.\n\n# DATA QUALITY ASSESSMENT\n\ncat(\"\\n=== DATA QUALITY ASSESSMENT ===\\n\")\n\n\n=== DATA QUALITY ASSESSMENT ===\n\n# Duplicate detection\nduplicate_rows &lt;- sum(duplicated(data))\ncat(sprintf(\"Duplicate Observations: %d (%.2f%%)\\n\", duplicate_rows, duplicate_rows/nrow(data)*100))\n\nDuplicate Observations: 4 (0.04%)\n\n# OAH vs FAH comparison (forecast vs observed avalanche hazard)\nif(all(c(\"OAH\", \"FAH\") %in% colnames(data))) {\n  # Clean both variables\n  comparison_data &lt;- data %&gt;%\n    filter(!is.na(FAH) & FAH != \"\" & FAH != \"NA\" &\n           !is.na(OAH) & OAH != \"\" & OAH != \"NA\") %&gt;%\n    mutate(\n      FAH_clean = str_trim(str_to_title(FAH)),\n      OAH_clean = str_trim(str_to_title(OAH))\n    )\n  \n  if(nrow(comparison_data) &gt; 0) {\n    # Accuracy calculation\n    forecast_accuracy &lt;- mean(comparison_data$FAH_clean == comparison_data$OAH_clean, na.rm = TRUE)\n    \n    cat(sprintf(\"\\nForecast vs Observed Comparison:\\n\"))\n    cat(sprintf(\"- Paired observations: %d\\n\", nrow(comparison_data)))\n    cat(sprintf(\"- Forecast accuracy: %.2f%%\\n\", forecast_accuracy * 100))\n    \n    # Confusion matrix for forecast vs observed\n    confusion_fah_oah &lt;- table(Forecast = comparison_data$FAH_clean, \n                               Observed = comparison_data$OAH_clean)\n    \n    cat(\"\\nForecast vs Observed Confusion Matrix:\\n\")\n    print(confusion_fah_oah)\n    \n    # Accuracy by risk level\n    accuracy_by_level &lt;- comparison_data %&gt;%\n      group_by(FAH_clean) %&gt;%\n      summarise(\n        Count = n(),\n        Correct = sum(FAH_clean == OAH_clean),\n        Accuracy = round(mean(FAH_clean == OAH_clean) * 100, 2),\n        .groups = 'drop'\n      ) %&gt;%\n      arrange(desc(Count))\n    \n    cat(\"\\nForecast Accuracy by Risk Level:\\n\")\n    kable(accuracy_by_level, caption = \"Forecast Accuracy by FAH Level\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n    \n    # Visualization\n    par(mfrow = c(1, 2), mar = c(8, 4, 3, 2))\n    \n    # Accuracy by level\n    barplot(accuracy_by_level$Accuracy,\n            names.arg = substr(accuracy_by_level$FAH_clean, 1, 8),\n            main = \"Forecast Accuracy by Level\",\n            ylab = \"Accuracy (%)\",\n            col = sais_palette[c(10,9,7,5,1)],  # Use first 5 colors for variety\n            las = 2,\n            cex.names = 0.7)\n    \n    # Overall accuracy visualization\n    accuracy_data &lt;- c(forecast_accuracy * 100, (1 - forecast_accuracy) * 100)\n    names(accuracy_data) &lt;- c(\"Correct\", \"Incorrect\")\n    barplot(accuracy_data,\n            main = \"Overall Forecast Accuracy\",\n            ylab = \"Percentage\",\n            col = sais_palette[c(3,5)])  # Light blue for Correct, dark grey for Incorrect\n    \n    par(mfrow = c(1, 1))\n  }\n}\n\n\nForecast vs Observed Comparison:\n- Paired observations: 10181\n- Forecast accuracy: 72.91%\n\nForecast vs Observed Confusion Matrix:\n                Observed\nForecast         Considerable - Considerable + High  Low Moderate\n  Considerable -           1553            173   20   48      627\n  Considerable +            438            309   30   10      121\n  High                      132             83  207    3       23\n  Low                         2              1    0 3173       92\n  Moderate                  160             15    1  779     2181\n\nForecast Accuracy by Risk Level:\n\n\n\n\n\n\n\n\n\nThis code chunk analyzes relationships between numeric variables through correlation analysis to identify potential multicollinearity issues. The analysis computes pairwise correlations using complete observations, identifies highly correlated variable pairs with absolute correlation coefficients above 0.7, and flags potential multicollinearity problems where correlations exceed 0.9. Summary tables display the most correlated variable pairs ranked by correlation strength. A correlation matrix heatmap visualizes the relationships between variables (limited to the top 20 most important variables for readability).\n\n# FEATURE RELATIONSHIP ANALYSIS\n\ncat(\"\\n=== FEATURE RELATIONSHIP ANALYSIS ===\\n\")\n\n\n=== FEATURE RELATIONSHIP ANALYSIS ===\n\n# Correlation analysis for numeric variables\nif(length(numeric_vars) &gt; 1) {\n  # Prepare numeric data for correlation\n  numeric_data_clean &lt;- data[, numeric_vars, drop = FALSE]\n  for(col in names(numeric_data_clean)) {\n    numeric_data_clean[[col]] &lt;- as.numeric(ifelse(numeric_data_clean[[col]] == \"NA\", NA, numeric_data_clean[[col]]))\n  }\n  \n  # Calculate correlation matrix\n  cor_matrix &lt;- cor(numeric_data_clean, use = \"pairwise.complete.obs\")\n  \n  # Find highly correlated pairs (&gt;0.7 or &lt;-0.7)\n  high_cor_pairs &lt;- which(abs(cor_matrix) &gt; 0.7 & cor_matrix != 1, arr.ind = TRUE)\n  \n  if(nrow(high_cor_pairs) &gt; 0) {\n    high_cor_df &lt;- data.frame(\n      Variable1 = rownames(cor_matrix)[high_cor_pairs[,1]],\n      Variable2 = colnames(cor_matrix)[high_cor_pairs[,2]],\n      Correlation = round(cor_matrix[high_cor_pairs], 3),\n      stringsAsFactors = FALSE\n    )\n    \n    # Remove duplicate pairs\n    high_cor_df &lt;- high_cor_df[high_cor_df$Variable1 &lt; high_cor_df$Variable2, ]\n    high_cor_df &lt;- high_cor_df[order(abs(high_cor_df$Correlation), decreasing = TRUE), ]\n    \n    cat(\"Highly Correlated Variable Pairs (|r| &gt; 0.7):\\n\")\n    kable(high_cor_df, caption = \"Highly Correlated Variable Pairs\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n    \n    # Multicollinearity assessment\n    very_high_cor &lt;- high_cor_df[abs(high_cor_df$Correlation) &gt; 0.9, ]\n    if(nrow(very_high_cor) &gt; 0) {\n      cat(\"\\nPotential Multicollinearity Issues (|r| &gt; 0.9):\\n\")\n      kable(very_high_cor, caption = \"Variables with Potential Multicollinearity\") %&gt;%\n        kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n    }\n  } else {\n    cat(\"No highly correlated variable pairs found (|r| &gt; 0.7)\\n\")\n  }\n  \n  # Correlation visualization\n  if(corrplot_available) {\n    # Select top variables for visualization (max 20 for readability)\n    if(ncol(cor_matrix) &gt; 20) {\n      # Select variables with highest variance or most correlations\n      var_importance &lt;- rowSums(abs(cor_matrix), na.rm = TRUE)\n      top_vars &lt;- names(sort(var_importance, decreasing = TRUE)[1:20])\n      cor_matrix_subset &lt;- cor_matrix[top_vars, top_vars]\n    } else {\n      cor_matrix_subset &lt;- cor_matrix\n    }\n    \n    corrplot(cor_matrix_subset, \n             method = \"color\",\n             type = \"upper\",\n             order = \"hclust\",\n             tl.cex = 0.7,\n             tl.col = \"black\",\n             title = \"Correlation Matrix Heatmap\",\n             mar = c(0, 0, 2, 0))\n  }\n}\n\nHighly Correlated Variable Pairs (|r| &gt; 0.7):\n\n\n\n\n\n\n\n\n\nThis code chunk performs outlier detection across all numeric variables using the Interquartile Range (IQR) method to identify extreme values. The chunk calculates quartiles, IQR bounds, and outlier thresholds (Q1-1.5×IQR and Q3+1.5×IQR) for each variable, then counts observations falling outside these bounds. A comprehensive summary table displays outlier statistics including counts and percentages, sorted by outlier prevalence to prioritize variables requiring attention. The chunk identifies variables with high outlier rates (&gt;5%) that may need preprocessing such as transformation, capping, or removal before model training. Box plots visualize the top six variables with the highest outlier percentages, highlighting extreme values in red to provide intuitive understanding of data distribution issues that could impact neural network performance and convergence.\n\n# OUTLIER ANALYSIS\n\ncat(\"\\n=== OUTLIER ANALYSIS ===\\n\")\n\n\n=== OUTLIER ANALYSIS ===\n\n# Outlier detection for numeric variables using IQR method\noutlier_summary &lt;- data.frame(\n  Variable = character(),\n  Total_Obs = numeric(),\n  Valid_Obs = numeric(),\n  Q1 = numeric(),\n  Q3 = numeric(),\n  IQR = numeric(),\n  Lower_Bound = numeric(),\n  Upper_Bound = numeric(),\n  Outliers = numeric(),\n  Outlier_Pct = numeric(),\n  stringsAsFactors = FALSE\n)\n\nnumeric_data_for_outliers &lt;- data[, numeric_vars, drop = FALSE]\nfor(col in names(numeric_data_for_outliers)) {\n  numeric_data_for_outliers[[col]] &lt;- as.numeric(ifelse(numeric_data_for_outliers[[col]] == \"NA\", NA, numeric_data_for_outliers[[col]]))\n}\n\nfor(var in numeric_vars) {\n  if(var %in% colnames(numeric_data_for_outliers)) {\n    values &lt;- numeric_data_for_outliers[[var]]\n    valid_values &lt;- values[!is.na(values)]\n    \n    if(length(valid_values) &gt; 0) {\n      Q1 &lt;- quantile(valid_values, 0.25)\n      Q3 &lt;- quantile(valid_values, 0.75)\n      IQR_val &lt;- Q3 - Q1\n      lower_bound &lt;- Q1 - 1.5 * IQR_val\n      upper_bound &lt;- Q3 + 1.5 * IQR_val\n      \n      outliers &lt;- sum(valid_values &lt; lower_bound | valid_values &gt; upper_bound)\n      outlier_pct &lt;- round(outliers / length(valid_values) * 100, 2)\n      \n      outlier_summary &lt;- rbind(outlier_summary, data.frame(\n        Variable = var,\n        Total_Obs = length(values),\n        Valid_Obs = length(valid_values),\n        Q1 = round(Q1, 3),\n        Q3 = round(Q3, 3),\n        IQR = round(IQR_val, 3),\n        Lower_Bound = round(lower_bound, 3),\n        Upper_Bound = round(upper_bound, 3),\n        Outliers = outliers,\n        Outlier_Pct = outlier_pct,\n        stringsAsFactors = FALSE\n      ))\n    }\n  }\n}\n\n# Sort by outlier percentage\noutlier_summary &lt;- outlier_summary[order(outlier_summary$Outlier_Pct, decreasing = TRUE), ]\n\ncat(\"Outlier Analysis Summary:\\n\")\n\nOutlier Analysis Summary:\n\nkable(outlier_summary, caption = \"Outlier Detection Results (IQR Method)\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nOutlier Detection Results (IQR Method)\n\n\n\nVariable\nTotal_Obs\nValid_Obs\nQ1\nQ3\nIQR\nLower_Bound\nUpper_Bound\nOutliers\nOutlier_Pct\n\n\n\n\n25%22\nCrystals\n10671\n9683\n0.000\n0.000\n0.000\n0.000\n0.000\n2165\n22.36\n\n\n25%20\nSnow.Index\n10671\n9926\n0.000\n0.000\n0.000\n0.000\n0.000\n2156\n21.72\n\n\n25%13\nRain.at.900\n10671\n10671\n0.000\n0.000\n0.000\n0.000\n0.000\n2040\n19.12\n\n\n25%24\nAV.Cat\n10671\n8177\n0.000\n0.000\n0.000\n0.000\n0.000\n1541\n18.85\n\n\n25%8\nCloud\n10671\n10642\n70.000\n100.000\n30.000\n25.000\n145.000\n1233\n11.59\n\n\n25%1\nlatitude\n10671\n10671\n56.817\n57.111\n0.295\n56.374\n57.554\n1054\n9.88\n\n\n25%12\nSki.Pen\n10671\n8267\n0.000\n0.000\n0.000\n0.000\n0.000\n702\n8.49\n\n\n25%10\nTotal.Snow.Depth\n10671\n10508\n45.000\n118.000\n73.000\n-64.500\n227.500\n834\n7.94\n\n\n25%17\nMax.Temp.Grad\n10671\n9961\n0.000\n1.000\n1.000\n-1.500\n2.500\n722\n7.25\n\n\n25%25\nSnow.Temp\n10671\n10261\n-2.200\n0.000\n2.200\n-5.500\n3.300\n605\n5.90\n\n\n25%15\nSummit.Wind.Dir\n10671\n9353\n160.000\n250.000\n90.000\n25.000\n385.000\n517\n5.53\n\n\n25%4\nIncline\n10671\n10635\n22.000\n32.000\n10.000\n7.000\n47.000\n571\n5.37\n\n\n25%11\nFoot.Pen\n10671\n10629\n5.000\n20.000\n15.000\n-17.500\n42.500\n287\n2.70\n\n\n25%23\nWetness\n10671\n10095\n1.000\n2.000\n1.000\n-0.500\n3.500\n226\n2.24\n\n\n25%7\nWind.Speed\n10671\n10618\n8.000\n22.000\n14.000\n-13.000\n43.000\n214\n2.02\n\n\n25%2\nAlt\n10671\n10665\n800.000\n1030.000\n230.000\n455.000\n1375.000\n167\n1.57\n\n\n25%5\nAir.Temp\n10671\n10637\n-2.100\n2.100\n4.200\n-8.400\n8.400\n154\n1.45\n\n\n25%16\nSummit.Wind.Speed\n10671\n9762\n14.000\n38.000\n24.000\n-22.000\n74.000\n117\n1.20\n\n\n25%14\nSummit.Air.Temp\n10671\n9917\n-3.600\n0.800\n4.400\n-10.200\n7.400\n115\n1.16\n\n\n25%19\nNo.Settle\n10671\n10382\n54.000\n206.000\n152.000\n-174.000\n434.000\n71\n0.68\n\n\n25%3\nAspect\n10671\n10316\n50.000\n225.000\n175.000\n-212.500\n487.500\n7\n0.07\n\n\n25%21\nInsolation\n10671\n10164\n3.000\n10.000\n7.000\n-7.500\n20.500\n4\n0.04\n\n\n25%6\nWind.Dir\n10671\n10513\n150.000\n270.000\n120.000\n-30.000\n450.000\n1\n0.01\n\n\n25%\nlongitude\n10671\n10671\n-4.963\n-3.652\n1.311\n-6.929\n-1.686\n0\n0.00\n\n\n25%9\nDrift\n10671\n10671\n0.000\n1.000\n1.000\n-1.500\n2.500\n0\n0.00\n\n\n25%18\nMax.Hardness.Grad\n10671\n10042\n1.000\n3.000\n2.000\n-2.000\n6.000\n0\n0.00\n\n\n\n\n\n\n# Identify variables with high outlier rates\nhigh_outlier_vars &lt;- outlier_summary$Variable[outlier_summary$Outlier_Pct &gt; 5]\ncat(sprintf(\"\\nVariables with &gt;5%% outliers: %d\\n\", length(high_outlier_vars)))\n\n\nVariables with &gt;5% outliers: 12\n\nif(length(high_outlier_vars) &gt; 0) {\n  cat(\"High outlier variables:\", paste(high_outlier_vars, collapse = \", \"), \"\\n\")\n}\n\nHigh outlier variables: Crystals, Snow.Index, Rain.at.900, AV.Cat, Cloud, latitude, Ski.Pen, Total.Snow.Depth, Max.Temp.Grad, Snow.Temp, Summit.Wind.Dir, Incline \n\n# Outlier visualization - box plots for top outlier variables\nif(nrow(outlier_summary) &gt; 0) {\n  top_outlier_vars &lt;- head(outlier_summary$Variable, 6)\n  \n  par(mfrow = c(2, 3), mar = c(8, 4, 3, 2))\n  \n  for(var in top_outlier_vars) {\n    if(var %in% colnames(numeric_data_for_outliers)) {\n      values &lt;- numeric_data_for_outliers[[var]]\n      boxplot(values, \n              main = paste(\"Outliers in\", var),\n              ylab = var,\n              col = \"lightblue\",\n              outcol = \"red\",\n              outcex = 0.5)\n    }\n  }\n  \n  par(mfrow = c(1, 1))\n}\n\n\n\n\n\n\n\n\nThe coding chunk below provides a basic summary of the EDA findings.\n\n# EDA SUMMARY\ncat(\"\\n=== KEY FINDINGS SUMMARY ===\\n\")\n\n\n=== KEY FINDINGS SUMMARY ===\n\ncat(sprintf(\"- Total observations: %d\\n\", nrow(data)))\n\n- Total observations: 10671\n\ncat(sprintf(\"- Variables: %d (%d numeric, %d categorical)\\n\", \n            ncol(data), length(numeric_vars), length(categorical_vars)))\n\n- Variables: 34 (26 numeric, 8 categorical)\n\ncat(sprintf(\"- Missing FAH: %.1f%%\\n\", fah_missing_pct))\n\n- Missing FAH: 1.0%\n\ncat(sprintf(\"- Duplicate rows: %d\\n\", duplicate_rows))\n\n- Duplicate rows: 4\n\nif(exists(\"forecast_accuracy\")) {\n  cat(sprintf(\"- Forecast accuracy: %.1f%%\\n\", forecast_accuracy * 100))\n}\n\n- Forecast accuracy: 72.9%\n\nif(exists(\"high_cor_pairs\") && nrow(high_cor_pairs) &gt; 0) {\n  cat(sprintf(\"- High correlations: %d pairs\\n\", nrow(high_cor_df)))\n}\n\n- High correlations: 1 pairs\n\nif(nrow(outlier_summary) &gt; 0) {\n  cat(sprintf(\"- Variables with &gt;5%% outliers: %d\\n\", length(high_outlier_vars)))\n}\n\n- Variables with &gt;5% outliers: 12\n\n# Clean up large objects to save memory\nrm(list = c(\"numeric_data\", \"numeric_data_clean\", \"numeric_data_for_outliers\", \n            \"comparison_data\", \"data_with_dates\"))\nif(exists(\"cor_matrix\")) rm(cor_matrix)\nif(exists(\"cor_matrix_subset\")) rm(cor_matrix_subset)\n\ngc()  # Garbage collection\n\n          used  (Mb) gc trigger  (Mb) max used  (Mb)\nNcells 2862010 152.9    5153475 275.3  5153475 275.3\nVcells 5299192  40.5   10146329  77.5 10144272  77.4\n\n\nThis section of the Model now starts with data preprocessing. For good measure, the data set is reloaded.\nThis chunk handles the initial data preprocessing steps for the dataset. This chunk begin by loading the raw data and removing the OAH (observed avalanche hazard) and Obs (observer) columns since our objective is to predict the forecasted avalanche hazard (FAH) rather than the observed outcomes. The chunk then filters out observations with missing or invalid FAH values, as these represent our target variable. To ensure consistency in the categorical target variable, we standardize the FAH values by trimming whitespace, normalizing case formatting, and mapping various text representations to five standard avalanche hazard levels: Low, Moderate, Considerable -, Considerable +, and High. Finally, we assess missing data patterns across all remaining variables and remove columns with more than 20% missing values to maintain data quality for our neural network model. This preprocessing ensures we have a clean dataset with a properly formatted target variable and reduces potential issues from excessive missing data in predictor variables.\n\n# DATA PREPROCESSING\ndata &lt;- read.csv(\"scotland_avalanche_forecasts_2009_2025.csv\", stringsAsFactors = FALSE)\n\ncat(\"\\n=== DATA PREPROCESSING ===\\n\")\n\n\n=== DATA PREPROCESSING ===\n\n# Drop OAH (actual observations) and Obs (observer) columns\ncolumns_to_drop &lt;- c(\"OAH\", \"Obs\")\ndata &lt;- data %&gt;% select(-all_of(columns_to_drop))\n\n# Remove observations with missing FAH\ndata_clean &lt;- data %&gt;% \n  filter(!is.na(FAH) & FAH != \"\" & FAH != \"NA\")\n\ncat(\"Observations after removing missing FAH:\", nrow(data_clean), \"\\n\")\n\nObservations after removing missing FAH: 10562 \n\n# Properly examine and clean FAH values\ncat(\"\\nUnique FAH values found:\\n\")\n\n\nUnique FAH values found:\n\nunique_fah &lt;- unique(data_clean$FAH)\nprint(unique_fah)\n\n[1] \"Moderate\"       \"Considerable -\" \"Considerable +\" \"High\"          \n[5] \"Low\"           \n\n# Clean FAH values - handle case sensitivity and variations\ndata_clean$FAH &lt;- str_trim(data_clean$FAH)  # Remove whitespace\ndata_clean$FAH &lt;- str_to_title(data_clean$FAH)  # Standardize case\n\n# Map variations to standard levels\ndata_clean$FAH &lt;- case_when(\n  str_detect(tolower(data_clean$FAH), \"^low\") ~ \"Low\",\n  str_detect(tolower(data_clean$FAH), \"^moderate\") ~ \"Moderate\", \n  str_detect(tolower(data_clean$FAH), \"considerable.*-|considerable.*minus\") ~ \"Considerable -\",\n  str_detect(tolower(data_clean$FAH), \"considerable.*\\\\+|considerable.*plus\") ~ \"Considerable +\",\n  str_detect(tolower(data_clean$FAH), \"^high\") ~ \"High\",\n  TRUE ~ data_clean$FAH\n)\n\n# Check cleaned FAH distribution\ncat(\"\\nCleaned FAH distribution:\\n\")\n\n\nCleaned FAH distribution:\n\nprint(table(data_clean$FAH, useNA = \"always\"))\n\n\nConsiderable - Considerable +           High            Low       Moderate \n          2498            933            459           3433           3239 \n          &lt;NA&gt; \n             0 \n\n# Remove any remaining unmapped values\nvalid_fah_levels &lt;- c(\"Low\", \"Moderate\", \"Considerable -\", \"Considerable +\", \"High\")\ndata_clean &lt;- data_clean %&gt;% \n  filter(FAH %in% valid_fah_levels)\n\ncat(\"Final observations with valid FAH:\", nrow(data_clean), \"\\n\")\n\nFinal observations with valid FAH: 10562 \n\ncat(\"Final FAH distribution:\\n\")\n\nFinal FAH distribution:\n\nprint(table(data_clean$FAH))\n\n\nConsiderable - Considerable +           High            Low       Moderate \n          2498            933            459           3433           3239 \n\n# Check missing value percentages for all columns\nmissing_percentages &lt;- data_clean %&gt;%\n  summarise_all(~sum(is.na(.) | . == \"\" | . == \"NA\") / length(.) * 100) %&gt;%\n  gather(key = \"Variable\", value = \"Missing_Percentage\") %&gt;%\n  arrange(desc(Missing_Percentage))\n\nprint(missing_percentages)\n\n            Variable Missing_Percentage\n1             AV.Cat        23.36678659\n2            Ski.Pen        22.48627154\n3    Summit.Wind.Dir        12.35561447\n4           Crystals         9.20280250\n5  Summit.Wind.Speed         8.53058133\n6    Summit.Air.Temp         7.06305624\n7         Snow.Index         6.97784511\n8      Max.Temp.Grad         6.07839424\n9  Max.Hardness.Grad         5.32096194\n10           Wetness         5.25468661\n11       Precip.Code         5.15053967\n12        Insolation         4.74341981\n13         Snow.Temp         3.66407877\n14            Aspect         2.94451808\n15         No.Settle         2.70782049\n16  Total.Snow.Depth         1.39178186\n17          Wind.Dir         1.34444234\n18        Wind.Speed         0.37871615\n19          Foot.Pen         0.22722969\n20          Air.Temp         0.19882598\n21           Incline         0.18935808\n22             Cloud         0.15148646\n23          Location         0.08521113\n24               Alt         0.04733952\n25              Date         0.00000000\n26              Area         0.00000000\n27            OSgrid         0.00000000\n28         longitude         0.00000000\n29          latitude         0.00000000\n30               FAH         0.00000000\n31             Drift         0.00000000\n32       Rain.at.900         0.00000000\n\n# Drop columns with &gt;20% missing values\nhigh_missing_threshold &lt;- 20\nhigh_missing_cols &lt;- missing_percentages %&gt;%\n  filter(Missing_Percentage &gt; high_missing_threshold) %&gt;%\n  pull(Variable)\n\nif(length(high_missing_cols) &gt; 0) {\n  data_clean &lt;- data_clean %&gt;% select(-all_of(high_missing_cols))\n  cat(\"Dropped high-missing columns:\", paste(high_missing_cols, collapse = \", \"), \"\\n\")\n}\n\nDropped high-missing columns: AV.Cat, Ski.Pen \n\n\nThis chunk performs comprehensive outlier detection and removal to ensure data quality for the neural network model. It begins by converting string representations of missing values to proper NA values for all numeric variables. The approach combines statistical outlier detection using the Interquartile Range (IQR) method with domain-specific knowledge about physically plausible ranges for avalanche forecasting variables. For variables like altitude, wind speeds, snow depth, temperature, and penetration measurements, we apply IQR-based outlier removal with a threshold of 2.5 standard deviations to eliminate extreme statistical outliers while preserving legitimate extreme weather events. Additionally, we implement domain-specific constraints to remove physically impossible values: aspect measurements exceeding 360 degrees (compass directions), cloud cover outside the 0-100% range, and slope inclines greater than 90 degrees or negative values. This dual approach ensures that our dataset contains only realistic measurements that are appropriate for training a predictive model of avalanche conditions in Scottish mountains.\n\n# OUTLIER DETECTION AND REMOVAL\n\n#Altitude (Alt) - removed via IQR method\n\n#Problem: Two observations have impossible values (244,859m and 77,044m)\n#Context: Scotland's highest peak (Ben Nevis) is 1,345m\n#Action: Remove these observations\n\n#Aspect - Not removed via iqr - removed specifically\n\n#Problem: Values exceeding 163,770 degrees (impossible for compass directions)\n#Context: Aspect should be 0-360 degrees\n#Action: Remove observations with Aspect &gt; 360\n\n#Wind Speed - removed via iqr method\n\n#Current range: -2 to 290 km/h\n#Outlier cap:  remove negative values and cap at 250\n#Rationale: Winds &gt;200 km/h are hurricane-force and extremely rare in Scottish mountains\n\n#Summit Wind Speed - removed via iqr method\n\n#Current range: -8 to 360 km/h\n#Outlier threshold: remove negative values and cap at 300\n#Rationale: Summit winds can be stronger but &gt;300 km/h values are likely errors\n\n#Total Snow Depth - removed via iqr method\n\n#Current range: -1 to 3,000cm (30 meters!)\n#Outlier cap: &gt; remove negative values and cap at 1200cm\n#Rationale: Snow depths &gt;10m are extremely rare and likely measurement errors\n\n#Cloud Cover - not removed via iqr method - special removal\n\n#Current range: -1 to 199%\n#Issue: Values &gt;100% and negative values\n#Outlier cap: 0% to 100% (remove impossible values)\n\n#Foot Penetration & Ski Penetration - removed via iqr\n\n#Foot Pen range: -1 to 300cm\n#Ski Pen range: -1 to 55cm\n#Action: Cap negative values at 0\n\n#Incline - not removed via iqr method - special removal\n\n#Range: -1 to 1,020 degrees\n#Issue: Slopes &gt;90° are overhangs (possible but rare)\n#Recommended cap: remove negative values and values over 180\n\ncat(\"\\n=== OUTLIER Removal ===\\n\")\n\n\n=== OUTLIER Removal ===\n\n# Convert character \"NA\" to actual NA for numeric columns\nnumeric_cols &lt;- c(\"Alt\", \"Aspect\", \"Wind.Speed\", \"Summit.Wind.Speed\", \n                  \"Total.Snow.Depth\", \"Cloud\", \"Foot.Pen\", \"Incline\",\n                  \"Air.Temp\", \"Summit.Air.Temp\", \"Max.Temp.Grad\", \n                  \"Max.Hardness.Grad\", \"No.Settle\", \"Snow.Index\", \n                  \"Insolation\", \"Crystals\", \"Wetness\", \"Snow.Temp\")\n\nfor(col in numeric_cols) {\n  if(col %in% colnames(data_clean)) {\n    data_clean[[col]] &lt;- as.numeric(ifelse(data_clean[[col]] == \"NA\", NA, data_clean[[col]]))\n  }\n}\n\n# robust outlier detection using IQR method\nremove_outliers_iqr &lt;- function(data, col_name, k = 3) {\n  if(col_name %in% colnames(data)) {\n    values &lt;- data[[col_name]]\n    Q1 &lt;- quantile(values, 0.25, na.rm = TRUE)\n    Q3 &lt;- quantile(values, 0.75, na.rm = TRUE)\n    IQR &lt;- Q3 - Q1\n    lower_bound &lt;- Q1 - k * IQR\n    upper_bound &lt;- Q3 + k * IQR\n    \n    outliers &lt;- sum(!is.na(values) & (values &lt; lower_bound | values &gt; upper_bound), na.rm = TRUE)\n    data &lt;- data %&gt;% filter(is.na(.data[[col_name]]) | \n                           (.data[[col_name]] &gt;= lower_bound & .data[[col_name]] &lt;= upper_bound))\n    cat(\"Removed\", outliers, \"outliers from\", col_name, \"\\n\")\n    return(data)\n  }\n  return(data)\n}\n\noriginal_count &lt;- nrow(data_clean)\n\n# Apply IQR-based outlier removal to key variables\noutlier_cols &lt;- c(\"Alt\", \"Wind.Speed\", \"Summit.Wind.Speed\", \"Total.Snow.Depth\", \n                  \"Air.Temp\", \"Summit.Air.Temp\", \"Foot.Pen\")\n\nfor(col in outlier_cols) {\n  data_clean &lt;- remove_outliers_iqr(data_clean, col, k = 3.5)\n}\n\nRemoved 2 outliers from Alt \nRemoved 22 outliers from Wind.Speed \nRemoved 81 outliers from Summit.Wind.Speed \nRemoved 111 outliers from Total.Snow.Depth \nRemoved 1 outliers from Air.Temp \nRemoved 0 outliers from Summit.Air.Temp \nRemoved 34 outliers from Foot.Pen \n\n# Domain-specific outlier removal\n# Aspect\n# Problem: Values exceeding 163,770 degrees (impossible for compass directions)\n# Context: Aspect should be 0-360 degrees\n# Action: Remove observations with Aspect &gt; 360\nif(\"Aspect\" %in% colnames(data_clean)) {\n  aspect_outliers &lt;- sum(!is.na(data_clean$Aspect) & data_clean$Aspect &gt; 360, na.rm = TRUE)\n  data_clean &lt;- data_clean %&gt;% filter(is.na(Aspect) | Aspect &lt;= 360)\n  cat(\"Removed aspect outliers:\", aspect_outliers, \"\\n\")\n}\n\nRemoved aspect outliers: 8 \n\n#Cloud Cover\n#Current range: -1 to 199%\n#Issue: Values &gt;100% and negative values\n#Outlier cap: 0% to 100% (remove impossible values)\nif(\"Cloud\" %in% colnames(data_clean)) {\n  cloud_outliers &lt;- sum(!is.na(data_clean$Cloud) & (data_clean$Cloud &lt; 0 | data_clean$Cloud &gt; 100), na.rm = TRUE)\n  data_clean &lt;- data_clean %&gt;% filter(is.na(Cloud) | (Cloud &gt;= 0 & Cloud &lt;= 100))\n  cat(\"Removed cloud cover outliers:\", cloud_outliers, \"\\n\")\n}\n\nRemoved cloud cover outliers: 3 \n\n#Incline\n#Range: -1 to 1,020 degrees\n#Issue: Slopes &gt;90° are overhangs (possible but rare)\n#cap: remove negative values and values over 180\nif(\"Incline\" %in% colnames(data_clean)) {\n  incline_outliers &lt;- sum(!is.na(data_clean$Incline) & (data_clean$Incline &lt; 0 | data_clean$Incline &gt; 90), na.rm = TRUE)\n  data_clean &lt;- data_clean %&gt;% filter(is.na(Incline) | (Incline &gt;= 0 & Incline &lt;= 90))\n  cat(\"Removed incline outliers:\", incline_outliers, \"\\n\")\n}\n\nRemoved incline outliers: 5 \n\ncat(\"Total observations removed due to outliers:\", original_count - nrow(data_clean), \"\\n\")\n\nTotal observations removed due to outliers: 267 \n\noutlier_removal_percentage &lt;- ((original_count - nrow(data_clean)) / original_count) * 100\ncat(\"Percentage of dataset removed due to outliers:\", round(outlier_removal_percentage, 2), \"%\\n\")\n\nPercentage of dataset removed due to outliers: 2.53 %\n\n\nThis chunk performs feature engineering to create new variables that may improve our neural network’s predictive performance by capturing important relationships and patterns in the avalanche forecast data. We generate several interaction features that combine existing variables in meteorologically meaningful ways: a wind chill index that accounts for the cooling effect of wind on air temperature, a temperature gradient between summit and base elevations that indicates atmospheric stability, and a snow-altitude interaction term that captures how snow accumulation varies with elevation.\nThe aspect (slope direction) is transformed from a single angular measurement into two orthogonal components (north-south and east-west) using trigonometric functions, which better represents the circular nature of compass directions for machine learning algorithms.\nFinally, we extract temporal features from the date variable, including month, day of year, and season, which can capture seasonal patterns in avalanche risk that are crucial for accurate forecasting. These engineered features provide the model with additional context about weather interactions, topographic influences, and seasonal dynamics that are fundamental to avalanche formation.\n\n# FEATURE ENGINEERING\n\ncat(\"\\n=== FEATURE ENGINEERING ===\\n\")\n\n\n=== FEATURE ENGINEERING ===\n\n# Create interaction features\nif(all(c(\"Air.Temp\", \"Wind.Speed\") %in% colnames(data_clean))) {\n  data_clean$Wind_Chill &lt;- data_clean$Air.Temp - (data_clean$Wind.Speed * 0.6)\n}\n\nif(all(c(\"Summit.Air.Temp\", \"Air.Temp\") %in% colnames(data_clean))) {\n  data_clean$Temp_Gradient &lt;- data_clean$Summit.Air.Temp - data_clean$Air.Temp\n}\n\nif(all(c(\"Total.Snow.Depth\", \"Alt\") %in% colnames(data_clean))) {\n  data_clean$Snow_Alt_Interaction &lt;- data_clean$Total.Snow.Depth * (data_clean$Alt / 1000)\n}\n\nif(\"Aspect\" %in% colnames(data_clean)) {\n  # Convert aspect to cardinal directions\n  data_clean$Aspect_North &lt;- cos(data_clean$Aspect * pi / 180)\n  data_clean$Aspect_East &lt;- sin(data_clean$Aspect * pi / 180)\n}\n\n# Create temporal features from date\nif(\"Date\" %in% colnames(data_clean)) {\n  data_clean$Date &lt;- as.Date(data_clean$Date)\n  data_clean$Month &lt;- as.numeric(month(data_clean$Date))\n  data_clean$Day_of_Year &lt;- as.numeric(yday(data_clean$Date))\n  data_clean$Season &lt;- case_when(\n    month(data_clean$Date) %in% c(12, 1, 2) ~ \"Winter\",\n    month(data_clean$Date) %in% c(3, 4, 5) ~ \"Spring\",\n    month(data_clean$Date) %in% c(6, 7, 8) ~ \"Summer\",\n    TRUE ~ \"Autumn\"\n  )\n}\n\ncat(\"Added engineered features\\n\")\n\nAdded engineered features\n\n\nThis chunk implements a multi-level imputation strategy to handle missing values in our dataset while preserving the underlying patterns that are important for avalanche forecasting. The approach uses contextual imputation by first grouping observations by Area and Season, recognizing that avalanche conditions vary significantly across different Scottish regions and times of year. Furthermore, this approach is supported by the fact that our missing value analysis in EDA noted that missing values were concentrated for certain seasons.\nWithin each group, missing numeric values are imputed using the group median (which is more robust to outliers than the mean), while categorical variables are imputed using the group mode (most frequent value). This ensures that imputed values reflect the typical conditions for that specific region and season rather than global averages that might not be representative.\nFor any remaining missing values after group-based imputation, we apply global imputation using overall dataset medians and modes as fallback values.\nWe exclude non-predictive variables like Date, OSgrid, Location, and our target variable FAH from the imputation process to prevent data leakage and maintain the integrity of our prediction target. This hierarchical imputation strategy helps preserve the regional and seasonal meteorological patterns that are crucial for accurate avalanche hazard prediction.\n\n# IMPUTATION\n\ncat(\"\\n=== IMPUTATION ===\\n\")\n\n\n=== IMPUTATION ===\n\n# More sophisticated imputation using multiple strategies\nimpute_advanced &lt;- function(data) {\n  # Remove non-predictive columns for imputation\n  cols_to_exclude &lt;- c(\"Date\", \"OSgrid\", \"Location\", \"FAH\")\n  imputation_data &lt;- data %&gt;% select(-all_of(cols_to_exclude[cols_to_exclude %in% colnames(data)]))\n  \n  # Group-based imputation (by Area and Season)\n  grouping_vars &lt;- intersect(c(\"Area\", \"Season\"), colnames(data))\n  \n  if(length(grouping_vars) &gt; 0) {\n    # Create grouping combinations\n    groups &lt;- data %&gt;% \n      select(all_of(grouping_vars)) %&gt;%\n      unite(\"group\", all_of(grouping_vars), sep = \"_\", remove = FALSE) %&gt;%\n      pull(group)\n    \n    unique_groups &lt;- unique(groups)\n    \n    for(group in unique_groups) {\n      group_mask &lt;- groups == group\n      \n      # Numeric imputation with group median\n      numeric_vars &lt;- names(imputation_data)[sapply(imputation_data, is.numeric)]\n      for(var in numeric_vars) {\n        if(var %in% colnames(data)) {\n          group_values &lt;- data[group_mask, var]\n          group_median &lt;- median(group_values, na.rm = TRUE)\n          if(!is.na(group_median)) {\n            data[group_mask & is.na(data[[var]]), var] &lt;&lt;- group_median\n          }\n        }\n      }\n      \n      # Categorical imputation with group mode\n      categorical_vars &lt;- names(imputation_data)[!sapply(imputation_data, is.numeric)]\n      for(var in categorical_vars) {\n        if(var %in% colnames(data) && var != \"FAH\") {\n          group_values &lt;- data[group_mask, var]\n          group_values &lt;- group_values[!is.na(group_values) & group_values != \"\" & group_values != \"NA\"]\n          if(length(group_values) &gt; 0) {\n            group_mode &lt;- names(sort(table(group_values), decreasing = TRUE))[1]\n            data[group_mask & (is.na(data[[var]]) | data[[var]] == \"\" | data[[var]] == \"NA\"), var] &lt;&lt;- group_mode\n          }\n        }\n      }\n    }\n  }\n  \n  # Global imputation for remaining missing values\n  for(var in names(imputation_data)) {\n    if(var %in% colnames(data) && var != \"FAH\") {\n      if(is.numeric(data[[var]])) {\n        global_median &lt;- median(data[[var]], na.rm = TRUE)\n        if(!is.na(global_median)) {\n          data[is.na(data[[var]]), var] &lt;&lt;- global_median\n        }\n      } else {\n        non_missing &lt;- data[[var]][!is.na(data[[var]]) & data[[var]] != \"\" & data[[var]] != \"NA\"]\n        if(length(non_missing) &gt; 0) {\n          global_mode &lt;- names(sort(table(non_missing), decreasing = TRUE))[1]\n          data[is.na(data[[var]]) | data[[var]] == \"\" | data[[var]] == \"NA\", var] &lt;&lt;- global_mode\n        }\n      }\n    }\n  }\n}\n\nimpute_advanced(data_clean)\n\nThis chunk handles target variable encoding and comprehensive feature encoding to prepare the data for neural network training. We begin by removing non-predictive identifier columns (Date, OSgrid, Location) and then carefully encode our target variable (FAH) using proper ordinal encoding that respects the natural risk hierarchy of avalanche hazard levels from Low (0) to High (4). The encoding verification ensures all avalanche hazard categories are correctly mapped and no values are lost in translation. For categorical predictor variables, we implement a systematic approach that first cleans variable names to avoid parsing issues, then applies frequency-based filtering to retain only categories that appear at least 50 times in the dataset (grouping rare categories as “Other” to prevent overfitting). We then create one-hot encoded dummy variables for each categorical level, converting them to binary indicators that neural networks can effectively process. This encoding strategy maintains the interpretability of our target variable while transforming all categorical predictors into a format suitable for neural network computation, ensuring our model can learn from both the ordinal nature of avalanche risk and the categorical patterns in weather and terrain variables.\n\n# TARGET ENCODING \n\ncat(\"\\n=== TARGET ENCODING ===\\n\")\n\n\n=== TARGET ENCODING ===\n\n# Drop non-predictive columns\ndate_area_cols &lt;- c(\"Date\", \"OSgrid\", \"Location\")\ndata_processed &lt;- data_clean %&gt;% select(-all_of(date_area_cols[date_area_cols %in% colnames(data_clean)]))\n\n# Define exact FAH levels in proper risk order (0-based for neural networks)\nfah_levels &lt;- c(\"Low\", \"Moderate\", \"Considerable -\", \"Considerable +\", \"High\")\n\n# Verify all FAH values are in expected levels\ncat(\"Expected FAH levels:\", paste(fah_levels, collapse = \", \"), \"\\n\")\n\nExpected FAH levels: Low, Moderate, Considerable -, Considerable +, High \n\ncat(\"Actual FAH levels in data:\", paste(sort(unique(data_processed$FAH)), collapse = \", \"), \"\\n\")\n\nActual FAH levels in data: Considerable -, Considerable +, High, Low, Moderate \n\n# Check for any unmapped values BEFORE encoding\nunmapped_values &lt;- setdiff(unique(data_processed$FAH), fah_levels)\nif(length(unmapped_values) &gt; 0) {\n  cat(\"WARNING: Unmapped FAH values found:\", paste(unmapped_values, collapse = \", \"), \"\\n\")\n  # Remove unmapped values\n  data_processed &lt;- data_processed %&gt;% filter(FAH %in% fah_levels)\n  cat(\"Removed unmapped values. New size:\", nrow(data_processed), \"\\n\")\n}\n\n# Create proper ordinal encoding (0-4 for 5 classes)\ndata_processed$FAH_encoded &lt;- as.numeric(factor(data_processed$FAH, \n                                               levels = fah_levels,\n                                               ordered = TRUE)) - 1  # Convert to 0-based\n\n# Verify encoding\ncat(\"\\nTarget encoding verification:\\n\")\n\n\nTarget encoding verification:\n\nencoding_check &lt;- data_processed %&gt;% \n  select(FAH, FAH_encoded) %&gt;% \n  distinct() %&gt;% \n  arrange(FAH_encoded)\nprint(encoding_check)\n\n             FAH FAH_encoded\n1            Low           0\n2       Moderate           1\n3 Considerable -           2\n4 Considerable +           3\n5           High           4\n\ncat(\"\\nFinal target distribution (encoded):\\n\")\n\n\nFinal target distribution (encoded):\n\nprint(table(data_processed$FAH_encoded))\n\n\n   0    1    2    3    4 \n3398 3167 2412  895  423 \n\n# Ensure no missing encoded values\nif(any(is.na(data_processed$FAH_encoded))) {\n  cat(\"ERROR: Some FAH values could not be encoded!\\n\")\n  stop(\"Fix FAH encoding before proceeding\")\n}\n\n\ncat(\"\\n COMPREHENSIVE FEATURE ENCODING \\n\")\n\n\n COMPREHENSIVE FEATURE ENCODING \n\n# Handle categorical variables more systematically\ncategorical_vars &lt;- sapply(data_processed, function(x) is.character(x) || is.factor(x))\ncategorical_vars &lt;- names(categorical_vars[categorical_vars])\ncategorical_vars &lt;- setdiff(categorical_vars, c(\"FAH\", \"FAH_encoded\"))\n\ncat(\"Categorical variables to encode:\", paste(categorical_vars, collapse = \", \"), \"\\n\")\n\nCategorical variables to encode: Area, Precip.Code, Season \n\n# One-hot encode categorical variables with frequency threshold\nfor(var in categorical_vars) {\n  if(var %in% colnames(data_processed)) {\n    # Clean variable values to avoid formula issues\n    data_processed[[var]] &lt;- make.names(data_processed[[var]])\n    \n    # Keep only categories that appear at least 50 times\n    freq_table &lt;- table(data_processed[[var]])\n    keep_categories &lt;- names(freq_table[freq_table &gt;= 50])\n    \n    # Group rare categories into \"Other\"\n    data_processed[[var]] &lt;- ifelse(data_processed[[var]] %in% keep_categories, \n                                   data_processed[[var]], \"Other\")\n    \n    # Create dummy variables manually to avoid formula parsing issues\n    unique_vals &lt;- unique(data_processed[[var]])\n    \n    for(val in unique_vals) {\n      new_col_name &lt;- paste0(make.names(var), \"_\", make.names(val))\n      data_processed[[new_col_name]] &lt;- as.numeric(data_processed[[var]] == val)\n    }\n    \n    # Remove original categorical variable\n    data_processed &lt;- data_processed %&gt;% select(-all_of(var))\n  }\n}\n\nThis chunk implements a comprehensive multi-method feature selection strategy to identify the most predictive variables for our neural network model. We begin by separating features from our target variable and standardizing all numeric features using centering and scaling to ensure variables with different units contribute equally to the selection process.\nThe approach combines three complementary feature selection methods: LASSO regularization to identify features with non-zero coefficients that contribute to linear prediction, Random Forest importance scoring to capture non-linear relationships and feature interactions, and correlation analysis to measure direct linear associations with the target variable. Each method contributes normalized scores (scaled 0-100) that are combined using weighted averaging (LASSO 30%, Random Forest 40%, Correlation 30%) to create composite feature importance scores.\nFeatures that perform well across multiple methods receive additional weighting bonuses to favor robust predictors. The final selection chooses the top 25 features based on these composite scores, providing our neural network with a focused set of the most informative variables while reducing dimensionality and potential overfitting. This multi-method approach ensures we capture both linear and non-linear relationships while maintaining model interpretability and computational efficiency.\n\n# FEATURE SELECTION\n\ncat(\"\\n=== FEATURE SELECTION ===\\n\")\n\n\n=== FEATURE SELECTION ===\n\n# Separate features and target\ntarget_col &lt;- \"FAH_encoded\"\nexclude_cols &lt;- c(\"FAH\", \"FAH_encoded\")\nfeature_cols &lt;- setdiff(colnames(data_processed), exclude_cols)\n\nX &lt;- data_processed[, feature_cols]\ny &lt;- data_processed[[target_col]]\n\n# Standardize numerical features\nnumeric_features &lt;- sapply(X, is.numeric)\nif(sum(numeric_features) &gt; 0) {\n  preprocess_params &lt;- preProcess(X[, numeric_features], method = c(\"center\", \"scale\"))\n  X[, numeric_features] &lt;- predict(preprocess_params, X[, numeric_features])\n}\n\n# Enhanced multi-method feature selection with proper ranking\nX_matrix &lt;- as.matrix(X)\ny_vector &lt;- as.numeric(y)\n\n# Remove any remaining missing values\ncomplete_cases &lt;- complete.cases(X_matrix, y_vector)\nX_matrix &lt;- X_matrix[complete_cases, ]\ny_vector &lt;- y_vector[complete_cases]\n\ncat(\"Complete cases for feature selection:\", sum(complete_cases), \"\\n\")\n\nComplete cases for feature selection: 7530 \n\n# Create feature scoring framework\nall_features &lt;- colnames(X_matrix)\nfeature_scores &lt;- data.frame(\n  Feature = all_features,\n  LASSO_Score = 0,\n  RF_Score = 0,\n  Correlation_Score = 0,\n  Total_Score = 0,\n  Method_Count = 0,\n  stringsAsFactors = FALSE\n)\n\n# 1. LASSO feature selection with scoring\nset.seed(42)\ncv_lasso &lt;- cv.glmnet(X_matrix, y_vector, alpha = 1, nfolds = 10)\nlasso_coef &lt;- coef(cv_lasso, s = \"lambda.1se\")\nlasso_coef_matrix &lt;- as.matrix(lasso_coef)\nlasso_nonzero &lt;- which(lasso_coef_matrix[,1] != 0 & rownames(lasso_coef_matrix) != \"(Intercept)\")\n\nif(length(lasso_nonzero) &gt; 0) {\n  lasso_features &lt;- rownames(lasso_coef_matrix)[lasso_nonzero]\n  lasso_coeffs &lt;- abs(lasso_coef_matrix[lasso_nonzero, 1])\n  \n  # Normalize LASSO coefficients to 0-100 scale\n  if(max(lasso_coeffs) &gt; 0) {\n    lasso_scores_norm &lt;- (lasso_coeffs / max(lasso_coeffs)) * 100\n    \n    # Update feature scores\n    for(i in 1:length(lasso_features)) {\n      feat_idx &lt;- which(feature_scores$Feature == lasso_features[i])\n      if(length(feat_idx) &gt; 0) {\n        feature_scores$LASSO_Score[feat_idx] &lt;- lasso_scores_norm[i]\n        feature_scores$Method_Count[feat_idx] &lt;- feature_scores$Method_Count[feat_idx] + 1\n      }\n    }\n  }\n  cat(\"LASSO selected\", length(lasso_features), \"features\\n\")\n} else {\n  cat(\"LASSO selected no features\\n\")\n}\n\nLASSO selected 30 features\n\n# 2. Random Forest feature importance with scoring\ntryCatch({\n  set.seed(42)\n  rf_data &lt;- X_matrix\n  rf_target &lt;- y_vector\n  \n  # Limit features if too many for Random Forest\n  if(ncol(rf_data) &gt; 50) {\n    cor_with_target &lt;- abs(cor(rf_data, rf_target, use = \"complete.obs\"))\n    top_cor_features &lt;- names(sort(cor_with_target[,1], decreasing = TRUE))[1:50]\n    rf_data &lt;- rf_data[, top_cor_features, drop = FALSE]\n  }\n  \n  rf_model &lt;- randomForest(x = rf_data, y = as.factor(rf_target), \n                          importance = TRUE, ntree = 100, na.action = na.omit)\n  rf_importance &lt;- importance(rf_model, type = 1)\n  \n  # Normalize RF importance to 0-100 scale\n  rf_scores_norm &lt;- (rf_importance[,1] / max(rf_importance[,1])) * 100\n  \n  # Update feature scores for all features in RF model\n  for(i in 1:nrow(rf_importance)) {\n    feat_name &lt;- rownames(rf_importance)[i]\n    feat_idx &lt;- which(feature_scores$Feature == feat_name)\n    if(length(feat_idx) &gt; 0) {\n      feature_scores$RF_Score[feat_idx] &lt;- rf_scores_norm[i]\n      feature_scores$Method_Count[feat_idx] &lt;- feature_scores$Method_Count[feat_idx] + 1\n    }\n  }\n  cat(\"Random Forest evaluated\", nrow(rf_importance), \"features\\n\")\n  \n}, error = function(e) {\n  cat(\"Warning: Random Forest feature selection failed:\", e$message, \"\\n\")\n  rf_importance &lt;- NULL\n})\n\nRandom Forest evaluated 47 features\n\n# 3. Correlation-based feature selection with scoring\ncor_with_target &lt;- abs(cor(X_matrix, y_vector, use = \"complete.obs\"))\ncor_scores_norm &lt;- (cor_with_target[,1] / max(cor_with_target[,1])) * 100\n\n# Update correlation scores for all features\nfor(i in 1:length(cor_scores_norm)) {\n  feat_name &lt;- names(cor_scores_norm)[i]\n  feat_idx &lt;- which(feature_scores$Feature == feat_name)\n  if(length(feat_idx) &gt; 0) {\n    feature_scores$Correlation_Score[feat_idx] &lt;- cor_scores_norm[i]\n    feature_scores$Method_Count[feat_idx] &lt;- feature_scores$Method_Count[feat_idx] + 1\n  }\n}\ncat(\"Correlation analysis evaluated\", length(cor_scores_norm), \"features\\n\")\n\nCorrelation analysis evaluated 47 features\n\n# 4. Calculate composite scores with method weighting\nfeature_scores$Total_Score &lt;- (\n  feature_scores$LASSO_Score * 0.3 +           \n  feature_scores$RF_Score * 0.4 +              \n  feature_scores$Correlation_Score * 0.3       \n) * (1 + (feature_scores$Method_Count - 1) * 0.2)  \n\n# Sort by total score and select top features\nfeature_scores &lt;- feature_scores[order(feature_scores$Total_Score, decreasing = TRUE), ]\n\n# Select top 25 features based on composite scoring\nselected_features &lt;- head(feature_scores$Feature, 25)\n\ncat(\"Selected\", length(selected_features), \"features using combined approach\\n\")\n\nSelected 25 features using combined approach\n\ncat(\"Features:\", paste(head(selected_features, 25), collapse = \", \"), \"...\\n\")\n\nFeatures: Foot.Pen, Drift, Summit.Air.Temp, Total.Snow.Depth, Crystals, Air.Temp, Wind_Chill, Precip.Code_X0...None, Precip.Code_X8...Snow, Snow.Index, Precip.Code_X6...Snow.Showers, No.Settle, Max.Temp.Grad, Season_Winter, Snow_Alt_Interaction, Snow.Temp, latitude, Alt, Precip.Code_X10...Heavy.Snow, Area_Lochaber, Area_Torridon, Day_of_Year, Insolation, Wind.Speed, Summit.Wind.Speed ...\n\nX_selected &lt;- X_matrix[, selected_features, drop = FALSE]\n\nThis chunk implements a rigorous data splitting strategy that follows machine learning best practices to prevent data leakage and ensure robust model evaluation. The data is divided using a 70/15/15 split into training, validation, and test sets respectively, with stratified sampling to maintain proportional representation of each avalanche hazard level across all splits.\nThe training set (70%) is used for model learning and parameter optimization, the validation set (15%) guides hyperparameter tuning and prevents overfitting during model selection, and the test set (15%) provides an unbiased final evaluation of model performance.\nThe ROSE (Random Over-Sampling Examples) technique is applied exclusively to the training data after splitting to address the significant class imbalance identified in the EDA phase. Since ROSE only handles binary classification, the chunk implements an innovative one-versus-rest strategy that iteratively balances each minority class against all others, generating synthetic samples to achieve more balanced class distributions. The target is set to 95% of the majority class size to create substantial balance improvement while avoiding perfect balance that might introduce artificial patterns. This approach increases training data from the original imbalanced distribution to a more balanced dataset while preserving the natural class distributions in validation and test sets, ensuring that model evaluation reflects real-world performance. The imbalance ratio is reduced from the original severe imbalance to approximately 1.2:1, providing the neural network with sufficient examples of each avalanche hazard level to learn effective classification boundaries while maintaining evaluation integrity.\n\n# DATA SPLITTING (Before any sampling to prevent data leakage)\n\ncat(\"\\n=== DATA SPLITTING (70/15/15) ===\\n\")\n\n\n=== DATA SPLITTING (70/15/15) ===\n\n# Prepare data for splitting (without any balancing yet)\nfinal_data &lt;- as.data.frame(cbind(X_selected, y = y_vector))\n\n# Clean column names to avoid formula parsing issues\ncolnames(final_data) &lt;- make.names(colnames(final_data))\n\n# Split into train (70%), validation (15%), and test (15%)\nset.seed(42)\ntrain_indices &lt;- createDataPartition(final_data$y, p = 0.7, list = FALSE)\nremaining_data &lt;- final_data[-train_indices, ]\n\n# Split remaining 30% into validation (15%) and test (15%)\nval_test_indices &lt;- createDataPartition(remaining_data$y, p = 0.5, list = FALSE)\n\ntrain_data &lt;- final_data[train_indices, ]\nval_data &lt;- remaining_data[val_test_indices, ]\ntest_data &lt;- remaining_data[-val_test_indices, ]\n\ncat(\"Original data size:\", nrow(final_data), \"\\n\")\n\nOriginal data size: 7530 \n\ncat(\"Train set size:\", nrow(train_data), \"\\n\")\n\nTrain set size: 5273 \n\ncat(\"Validation set size:\", nrow(val_data), \"\\n\")\n\nValidation set size: 1129 \n\ncat(\"Test set size:\", nrow(test_data), \"\\n\")\n\nTest set size: 1128 \n\n# Check class distribution before balancing\ncat(\"\\nClass distribution in training set (before ROSE):\\n\")\n\n\nClass distribution in training set (before ROSE):\n\nprint(table(train_data$y))\n\n\n   0    1    2    3    4 \n1792 1743 1192  401  145 \n\ncat(\"\\nClass distribution in validation set:\\n\")\n\n\nClass distribution in validation set:\n\nprint(table(val_data$y))\n\n\n  0   1   2   3   4 \n382 375 255  89  28 \n\ncat(\"\\nClass distribution in test set:\\n\")\n\n\nClass distribution in test set:\n\nprint(table(test_data$y))\n\n\n  0   1   2   3   4 \n377 380 255  83  33 \n\n# ROSE SAMPLING (Applied only to training data using one-vs-rest strategy)\n\ncat(\"\\n=== ROSE SAMPLING ON TRAINING DATA ===\\n\")\n\n\n=== ROSE SAMPLING ON TRAINING DATA ===\n\n# Since ROSE only works with binary classification, we'll apply it iteratively\n# to balance each minority class against all others\n\n# Calculate class frequencies\nclass_counts &lt;- table(train_data$y)\ncat(\"Original class distribution:\\n\")\n\nOriginal class distribution:\n\nprint(class_counts)\n\n\n   0    1    2    3    4 \n1792 1743 1192  401  145 \n\nmax_count &lt;- max(class_counts)\ntarget_count &lt;- round(max_count * 0.95)  # Target 80% of majority class size\n\n# Initialize with original training data\nbalanced_train_data &lt;- train_data\n\nset.seed(42)\n\n# Apply ROSE to each minority class\nfor(class_val in names(class_counts)) {\n  current_count &lt;- class_counts[class_val]\n  \n  if(current_count &lt; target_count) {\n    cat(sprintf(\"Applying ROSE to class %s (%s): %d -&gt; %d samples\\n\", \n                class_val, fah_levels[as.numeric(class_val) + 1], current_count, target_count))\n    \n    # Create binary problem: current class vs all others\n    binary_data &lt;- train_data\n    binary_data$binary_target &lt;- ifelse(train_data$y == as.numeric(class_val), \"target_class\", \"other_class\")\n    binary_data$binary_target &lt;- factor(binary_data$binary_target)\n    \n    # Remove original target variable\n    binary_data_for_rose &lt;- binary_data[, !names(binary_data) %in% \"y\"]\n    \n    # Apply ROSE to create more samples of the minority class\n    rose_result &lt;- ROSE(binary_target ~ ., data = binary_data_for_rose, \n                       seed = 42 + as.numeric(class_val), p = 0.5)$data\n    \n    # Extract only the newly generated samples of the target class\n    new_samples &lt;- rose_result[rose_result$binary_target == \"target_class\", ]\n    \n    # Remove the binary target column and add back the original target\n    new_samples$binary_target &lt;- NULL\n    new_samples$y &lt;- as.numeric(class_val)\n    \n    # Calculate how many new samples we need\n    n_new_samples &lt;- target_count - current_count\n    \n    # Randomly select the required number of new samples\n    if(nrow(new_samples) &gt; n_new_samples) {\n      sample_indices &lt;- sample(nrow(new_samples), n_new_samples, replace = FALSE)\n      new_samples &lt;- new_samples[sample_indices, ]\n    }\n    \n    # Add new samples to balanced dataset\n    balanced_train_data &lt;- rbind(balanced_train_data, new_samples)\n    \n    cat(sprintf(\"Added %d new samples for class %s\\n\", nrow(new_samples), class_val))\n  }\n}\n\nApplying ROSE to class 2 (Considerable -): 1192 -&gt; 1702 samples\nAdded 510 new samples for class 2\nApplying ROSE to class 3 (Considerable +): 401 -&gt; 1702 samples\nAdded 1301 new samples for class 3\nApplying ROSE to class 4 (High): 145 -&gt; 1702 samples\nAdded 1557 new samples for class 4\n\n# Remove any potential duplicates and shuffle\nbalanced_train_data &lt;- balanced_train_data[!duplicated(balanced_train_data), ]\nbalanced_train_data &lt;- balanced_train_data[sample(nrow(balanced_train_data)), ]\n\ncat(\"Training set size after ROSE:\", nrow(balanced_train_data), \"\\n\")\n\nTraining set size after ROSE: 8641 \n\ncat(\"Class distribution after ROSE sampling:\\n\")\n\nClass distribution after ROSE sampling:\n\nprint(table(balanced_train_data$y))\n\n\n   0    1    2    3    4 \n1792 1743 1702 1702 1702 \n\n# Calculate balancing improvement\noriginal_imbalance &lt;- max(table(train_data$y)) / min(table(train_data$y))\nrose_imbalance &lt;- max(table(balanced_train_data$y)) / min(table(balanced_train_data$y))\ncat(sprintf(\"Original imbalance ratio: %.2f:1\\n\", original_imbalance))\n\nOriginal imbalance ratio: 12.36:1\n\ncat(sprintf(\"ROSE imbalance ratio: %.2f:1\\n\", rose_imbalance))\n\nROSE imbalance ratio: 1.05:1\n\ncat(sprintf(\"Imbalance reduction: %.1f%%\\n\", (1 - rose_imbalance/original_imbalance) * 100))\n\nImbalance reduction: 91.5%\n\n\nThis chunk implements the neural network architecture. The model features a three-layer deep neural network with batch normalization, dropout regularization, and Gaussian noise injection to prevent overfitting and improve generalization. The architecture includes progressively smaller hidden layers (256→128→64 neurons in the best configuration) that allow the network to learn hierarchical representations of the avalanche risk patterns. Batch normalization stabilizes training by normalizing inputs to each layer, while dropout randomly deactivates neurons during training to reduce overfitting. The chunk implements systematic hyperparameter tuning by testing three carefully designed configurations that vary in network depth, regularization strength, and learning parameters. Each configuration is evaluated using early stopping and learning rate reduction callbacks to ensure optimal training convergence. The best performing configuration is selected based on validation accuracy, providing a robust foundation for the final model training phase.\n\n# NEURAL NETWORK ARCHITECTURE\n\ncat(\"\\n=== NEURAL NETWORK ARCHITECTURE ===\\n\")\n\n\n=== NEURAL NETWORK ARCHITECTURE ===\n\n# Prepare data for Keras\nX_train &lt;- as.matrix(balanced_train_data[, -ncol(balanced_train_data)])\ny_train &lt;- balanced_train_data$y  # Already 0-based (0,1,2,3,4)\nX_val &lt;- as.matrix(val_data[, -ncol(val_data)])\ny_val &lt;- val_data$y\nX_test &lt;- as.matrix(test_data[, -ncol(test_data)])\ny_test &lt;- test_data$y\n\n# Set correct number of classes\nnum_classes &lt;- length(fah_levels)  # Should be 5\ncat(\"Number of classes:\", num_classes, \"\\n\")\n\nNumber of classes: 5 \n\ncat(\"Class range in train:\", min(y_train), \"to\", max(y_train), \"\\n\")\n\nClass range in train: 0 to 4 \n\ncat(\"Class range in val:\", min(y_val), \"to\", max(y_val), \"\\n\")\n\nClass range in val: 0 to 4 \n\ncat(\"Class range in test:\", min(y_test), \"to\", max(y_test), \"\\n\")\n\nClass range in test: 0 to 4 \n\n# Verify all classes are within expected range\nif(max(c(y_train, y_val, y_test)) &gt;= num_classes) {\n  cat(\"ERROR: Class values exceed expected range!\\n\")\n  stop(\"Fix class encoding\")\n}\n\ny_train_cat &lt;- to_categorical(y_train, num_classes = num_classes)\ny_val_cat &lt;- to_categorical(y_val, num_classes = num_classes)\ny_test_cat &lt;- to_categorical(y_test, num_classes = num_classes)\n\n# Advanced model architecture with regularization\ncreate_advanced_model &lt;- function(input_dim, num_classes, config) {\n  # Input layer with noise for regularization\n  input_layer &lt;- layer_input(shape = input_dim)\n  \n  # Add gaussian noise for regularization\n  x &lt;- input_layer %&gt;% layer_gaussian_noise(stddev = 0.01)\n  \n  # First hidden layer\n  x &lt;- x %&gt;%\n    layer_dense(units = config$hidden_units_1, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = config$dropout_rate_1)\n  \n  # Second hidden layer\n  x &lt;- x %&gt;%\n    layer_dense(units = config$hidden_units_2, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = config$dropout_rate_2)\n  \n  # Third hidden layer (smaller)\n  x &lt;- x %&gt;%\n    layer_dense(units = config$hidden_units_3, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = config$dropout_rate_3)\n  \n  # Output layer\n  predictions &lt;- x %&gt;%\n    layer_dense(units = num_classes, activation = 'softmax')\n  \n  model &lt;- keras_model(inputs = input_layer, outputs = predictions)\n  \n  # Compile with advanced optimizer\n  model %&gt;% compile(\n    optimizer = optimizer_adam(\n      learning_rate = config$learning_rate,\n      beta_1 = 0.9,\n      beta_2 = 0.999,\n      epsilon = 1e-07\n    ),\n    loss = 'categorical_crossentropy',\n    metrics = c('accuracy', 'categorical_crossentropy')\n  )\n  \n  return(model)\n}\n\n# Enhanced hyperparameter tuning\nbest_configs &lt;- list(\n  list(hidden_units_1 = 256, hidden_units_2 = 128, hidden_units_3 = 64,\n       dropout_rate_1 = 0.3, dropout_rate_2 = 0.4, dropout_rate_3 = 0.5,\n       learning_rate = 0.001, batch_size = 64),\n  list(hidden_units_1 = 512, hidden_units_2 = 256, hidden_units_3 = 128,\n       dropout_rate_1 = 0.2, dropout_rate_2 = 0.3, dropout_rate_3 = 0.4,\n       learning_rate = 0.0005, batch_size = 32),\n  list(hidden_units_1 = 384, hidden_units_2 = 192, hidden_units_3 = 96,\n       dropout_rate_1 = 0.25, dropout_rate_2 = 0.35, dropout_rate_3 = 0.45,\n       learning_rate = 0.0008, batch_size = 48)\n)\n\nbest_score &lt;- 0\nbest_train_score &lt;- 0\nbest_config &lt;- NULL\n\nfor(i in 1:length(best_configs)) {\n  config &lt;- best_configs[[i]]\n  cat(sprintf(\"Testing configuration %d/%d\\n\", i, length(best_configs)))\n  \n  set.seed(42)\n  model &lt;- create_advanced_model(ncol(X_train), num_classes, config)\n  \n  # Train with callbacks\n  history &lt;- model %&gt;% fit(\n    X_train, y_train_cat,\n    epochs = 100,\n    batch_size = config$batch_size,\n    validation_data = list(X_val, y_val_cat),\n    verbose = 0,\n    callbacks = list(\n      callback_early_stopping(patience = 15, restore_best_weights = TRUE),\n      callback_reduce_lr_on_plateau(patience = 8, factor = 0.5, min_lr = 1e-6)\n    )\n  )\n  \n  val_score &lt;- max(history$metrics$val_accuracy)\n  cat(sprintf(\"Validation accuracy: %.4f\\n\", val_score))\n  \n  train_score &lt;- max(history$metrics$accuracy)\n  cat(sprintf(\"Training accuracy: %.4f\\n\", train_score))\n  \n  if(val_score &gt; best_score) {\n    best_score &lt;- val_score\n    best_train_score &lt;- train_score\n    best_config &lt;- config\n  }\n}\n\nTesting configuration 1/3\nValidation accuracy: 0.6014\nTraining accuracy: 0.6711\nTesting configuration 2/3\nValidation accuracy: 0.6129\nTraining accuracy: 0.7027\nTesting configuration 3/3\nValidation accuracy: 0.6076\nTraining accuracy: 0.7067\n\ncat(\"\\nBest configuration found:\\n\")\n\n\nBest configuration found:\n\nprint(best_config)\n\n$hidden_units_1\n[1] 512\n\n$hidden_units_2\n[1] 256\n\n$hidden_units_3\n[1] 128\n\n$dropout_rate_1\n[1] 0.2\n\n$dropout_rate_2\n[1] 0.3\n\n$dropout_rate_3\n[1] 0.4\n\n$learning_rate\n[1] 5e-04\n\n$batch_size\n[1] 32\n\ncat(\"Best training accuracy:\", best_train_score, \"\\n\")\n\nBest training accuracy: 0.7026964 \n\ncat(\"Best validation accuracy:\", best_score, \"\\n\")\n\nBest validation accuracy: 0.6129318 \n\n\nThis chunk trains the final neural network model using the optimal hyperparameters identified in the previous tuning phase. The training process employs optimization strategies including extended training epochs (150) with patience-based early stopping to prevent overfitting while allowing sufficient time for convergence. The implementation uses advanced callbacks including learning rate reduction on plateau, which automatically decreases the learning rate when validation performance stagnates, enabling fine-tuned optimization in later training stages. Since the training data was already balanced using ROSE sampling, no additional class weighting is required, allowing the model to learn from the artificially balanced dataset. The training process monitors both training and validation metrics to track model performance and detect potential overfitting, with the best model weights automatically restored if performance degrades. This comprehensive training approach ensures the model achieves optimal performance on the avalanche hazard prediction task.\n\n# FINAL MODEL TRAINING\n\ncat(\"\\n=== FINAL MODEL TRAINING ===\\n\")\n\n\n=== FINAL MODEL TRAINING ===\n\n# Train final model with best configuration\nset.seed(42)\nfinal_model &lt;- create_advanced_model(ncol(X_train), num_classes, best_config)\n\n# Train final model (no class weights needed since we used ROSE)\nfinal_history &lt;- final_model %&gt;% fit(\n  X_train, y_train_cat,\n  epochs = 150,\n  batch_size = best_config$batch_size,\n  validation_data = list(X_val, y_val_cat),\n  callbacks = list(\n    callback_early_stopping(patience = 30, restore_best_weights = TRUE),\n    callback_reduce_lr_on_plateau(patience = 15, factor = 0.3, min_lr = 1e-7)\n  )\n)\n\nEpoch 1/150\n271/271 - 5s - 19ms/step - accuracy: 0.4074 - categorical_crossentropy: 1.6141 - loss: 1.6141 - val_accuracy: 0.5385 - val_categorical_crossentropy: 1.0981 - val_loss: 1.0981 - learning_rate: 5.0000e-04\nEpoch 2/150\n271/271 - 2s - 6ms/step - accuracy: 0.4679 - categorical_crossentropy: 1.2917 - loss: 1.2917 - val_accuracy: 0.5580 - val_categorical_crossentropy: 1.0490 - val_loss: 1.0490 - learning_rate: 5.0000e-04\nEpoch 3/150\n271/271 - 1s - 6ms/step - accuracy: 0.4932 - categorical_crossentropy: 1.1945 - loss: 1.1945 - val_accuracy: 0.5669 - val_categorical_crossentropy: 1.0102 - val_loss: 1.0102 - learning_rate: 5.0000e-04\nEpoch 4/150\n271/271 - 2s - 6ms/step - accuracy: 0.5217 - categorical_crossentropy: 1.1049 - loss: 1.1049 - val_accuracy: 0.5775 - val_categorical_crossentropy: 0.9940 - val_loss: 0.9940 - learning_rate: 5.0000e-04\nEpoch 5/150\n271/271 - 3s - 10ms/step - accuracy: 0.5494 - categorical_crossentropy: 1.0540 - loss: 1.0540 - val_accuracy: 0.5740 - val_categorical_crossentropy: 0.9761 - val_loss: 0.9761 - learning_rate: 5.0000e-04\nEpoch 6/150\n271/271 - 2s - 6ms/step - accuracy: 0.5598 - categorical_crossentropy: 1.0237 - loss: 1.0237 - val_accuracy: 0.5766 - val_categorical_crossentropy: 0.9774 - val_loss: 0.9774 - learning_rate: 5.0000e-04\nEpoch 7/150\n271/271 - 1s - 5ms/step - accuracy: 0.5801 - categorical_crossentropy: 0.9927 - loss: 0.9927 - val_accuracy: 0.5810 - val_categorical_crossentropy: 0.9781 - val_loss: 0.9781 - learning_rate: 5.0000e-04\nEpoch 8/150\n271/271 - 1s - 5ms/step - accuracy: 0.5791 - categorical_crossentropy: 0.9856 - loss: 0.9856 - val_accuracy: 0.5872 - val_categorical_crossentropy: 0.9697 - val_loss: 0.9697 - learning_rate: 5.0000e-04\nEpoch 9/150\n271/271 - 2s - 6ms/step - accuracy: 0.5901 - categorical_crossentropy: 0.9529 - loss: 0.9529 - val_accuracy: 0.5695 - val_categorical_crossentropy: 0.9828 - val_loss: 0.9828 - learning_rate: 5.0000e-04\nEpoch 10/150\n271/271 - 2s - 6ms/step - accuracy: 0.6039 - categorical_crossentropy: 0.9261 - loss: 0.9261 - val_accuracy: 0.5775 - val_categorical_crossentropy: 0.9720 - val_loss: 0.9720 - learning_rate: 5.0000e-04\nEpoch 11/150\n271/271 - 2s - 6ms/step - accuracy: 0.6100 - categorical_crossentropy: 0.9126 - loss: 0.9126 - val_accuracy: 0.5828 - val_categorical_crossentropy: 0.9525 - val_loss: 0.9525 - learning_rate: 5.0000e-04\nEpoch 12/150\n271/271 - 1s - 5ms/step - accuracy: 0.6085 - categorical_crossentropy: 0.9089 - loss: 0.9089 - val_accuracy: 0.5934 - val_categorical_crossentropy: 0.9658 - val_loss: 0.9658 - learning_rate: 5.0000e-04\nEpoch 13/150\n271/271 - 2s - 6ms/step - accuracy: 0.6269 - categorical_crossentropy: 0.8766 - loss: 0.8766 - val_accuracy: 0.5979 - val_categorical_crossentropy: 0.9596 - val_loss: 0.9596 - learning_rate: 5.0000e-04\nEpoch 14/150\n271/271 - 2s - 6ms/step - accuracy: 0.6268 - categorical_crossentropy: 0.8810 - loss: 0.8810 - val_accuracy: 0.5784 - val_categorical_crossentropy: 0.9579 - val_loss: 0.9579 - learning_rate: 5.0000e-04\nEpoch 15/150\n271/271 - 2s - 6ms/step - accuracy: 0.6316 - categorical_crossentropy: 0.8630 - loss: 0.8630 - val_accuracy: 0.5846 - val_categorical_crossentropy: 0.9571 - val_loss: 0.9571 - learning_rate: 5.0000e-04\nEpoch 16/150\n271/271 - 2s - 6ms/step - accuracy: 0.6353 - categorical_crossentropy: 0.8558 - loss: 0.8558 - val_accuracy: 0.5740 - val_categorical_crossentropy: 0.9652 - val_loss: 0.9652 - learning_rate: 5.0000e-04\nEpoch 17/150\n271/271 - 1s - 5ms/step - accuracy: 0.6350 - categorical_crossentropy: 0.8578 - loss: 0.8578 - val_accuracy: 0.5864 - val_categorical_crossentropy: 0.9685 - val_loss: 0.9685 - learning_rate: 5.0000e-04\nEpoch 18/150\n271/271 - 2s - 6ms/step - accuracy: 0.6490 - categorical_crossentropy: 0.8385 - loss: 0.8385 - val_accuracy: 0.5908 - val_categorical_crossentropy: 0.9634 - val_loss: 0.9634 - learning_rate: 5.0000e-04\nEpoch 19/150\n271/271 - 1s - 5ms/step - accuracy: 0.6525 - categorical_crossentropy: 0.8309 - loss: 0.8309 - val_accuracy: 0.5855 - val_categorical_crossentropy: 0.9685 - val_loss: 0.9685 - learning_rate: 5.0000e-04\nEpoch 20/150\n271/271 - 2s - 6ms/step - accuracy: 0.6617 - categorical_crossentropy: 0.8011 - loss: 0.8011 - val_accuracy: 0.5810 - val_categorical_crossentropy: 0.9756 - val_loss: 0.9756 - learning_rate: 5.0000e-04\nEpoch 21/150\n271/271 - 1s - 5ms/step - accuracy: 0.6561 - categorical_crossentropy: 0.8124 - loss: 0.8124 - val_accuracy: 0.5988 - val_categorical_crossentropy: 0.9537 - val_loss: 0.9537 - learning_rate: 5.0000e-04\nEpoch 22/150\n271/271 - 2s - 6ms/step - accuracy: 0.6600 - categorical_crossentropy: 0.7979 - loss: 0.7979 - val_accuracy: 0.5740 - val_categorical_crossentropy: 0.9638 - val_loss: 0.9638 - learning_rate: 5.0000e-04\nEpoch 23/150\n271/271 - 1s - 5ms/step - accuracy: 0.6660 - categorical_crossentropy: 0.7901 - loss: 0.7901 - val_accuracy: 0.5917 - val_categorical_crossentropy: 0.9576 - val_loss: 0.9576 - learning_rate: 5.0000e-04\nEpoch 24/150\n271/271 - 2s - 6ms/step - accuracy: 0.6757 - categorical_crossentropy: 0.7707 - loss: 0.7707 - val_accuracy: 0.5810 - val_categorical_crossentropy: 0.9669 - val_loss: 0.9669 - learning_rate: 5.0000e-04\nEpoch 25/150\n271/271 - 1s - 5ms/step - accuracy: 0.6757 - categorical_crossentropy: 0.7724 - loss: 0.7724 - val_accuracy: 0.5961 - val_categorical_crossentropy: 0.9606 - val_loss: 0.9606 - learning_rate: 5.0000e-04\nEpoch 26/150\n271/271 - 2s - 6ms/step - accuracy: 0.6812 - categorical_crossentropy: 0.7540 - loss: 0.7540 - val_accuracy: 0.5979 - val_categorical_crossentropy: 0.9651 - val_loss: 0.9651 - learning_rate: 5.0000e-04\nEpoch 27/150\n271/271 - 1s - 5ms/step - accuracy: 0.6992 - categorical_crossentropy: 0.7186 - loss: 0.7186 - val_accuracy: 0.5934 - val_categorical_crossentropy: 0.9606 - val_loss: 0.9606 - learning_rate: 1.5000e-04\nEpoch 28/150\n271/271 - 2s - 6ms/step - accuracy: 0.6982 - categorical_crossentropy: 0.7122 - loss: 0.7122 - val_accuracy: 0.5917 - val_categorical_crossentropy: 0.9620 - val_loss: 0.9620 - learning_rate: 1.5000e-04\nEpoch 29/150\n271/271 - 1s - 6ms/step - accuracy: 0.6993 - categorical_crossentropy: 0.7133 - loss: 0.7133 - val_accuracy: 0.5890 - val_categorical_crossentropy: 0.9652 - val_loss: 0.9652 - learning_rate: 1.5000e-04\nEpoch 30/150\n271/271 - 2s - 6ms/step - accuracy: 0.7083 - categorical_crossentropy: 0.6936 - loss: 0.6936 - val_accuracy: 0.5819 - val_categorical_crossentropy: 0.9673 - val_loss: 0.9673 - learning_rate: 1.5000e-04\nEpoch 31/150\n271/271 - 1s - 5ms/step - accuracy: 0.7058 - categorical_crossentropy: 0.7013 - loss: 0.7013 - val_accuracy: 0.5934 - val_categorical_crossentropy: 0.9598 - val_loss: 0.9598 - learning_rate: 1.5000e-04\nEpoch 32/150\n271/271 - 2s - 6ms/step - accuracy: 0.7047 - categorical_crossentropy: 0.6959 - loss: 0.6959 - val_accuracy: 0.5908 - val_categorical_crossentropy: 0.9745 - val_loss: 0.9745 - learning_rate: 1.5000e-04\nEpoch 33/150\n271/271 - 2s - 6ms/step - accuracy: 0.7175 - categorical_crossentropy: 0.6857 - loss: 0.6857 - val_accuracy: 0.5872 - val_categorical_crossentropy: 0.9774 - val_loss: 0.9774 - learning_rate: 1.5000e-04\nEpoch 34/150\n271/271 - 2s - 6ms/step - accuracy: 0.7175 - categorical_crossentropy: 0.6791 - loss: 0.6791 - val_accuracy: 0.6014 - val_categorical_crossentropy: 0.9770 - val_loss: 0.9770 - learning_rate: 1.5000e-04\nEpoch 35/150\n271/271 - 1s - 5ms/step - accuracy: 0.7123 - categorical_crossentropy: 0.6873 - loss: 0.6873 - val_accuracy: 0.6023 - val_categorical_crossentropy: 0.9727 - val_loss: 0.9727 - learning_rate: 1.5000e-04\nEpoch 36/150\n271/271 - 2s - 6ms/step - accuracy: 0.7254 - categorical_crossentropy: 0.6620 - loss: 0.6620 - val_accuracy: 0.6050 - val_categorical_crossentropy: 0.9760 - val_loss: 0.9760 - learning_rate: 1.5000e-04\nEpoch 37/150\n271/271 - 1s - 5ms/step - accuracy: 0.7209 - categorical_crossentropy: 0.6686 - loss: 0.6686 - val_accuracy: 0.6032 - val_categorical_crossentropy: 0.9799 - val_loss: 0.9799 - learning_rate: 1.5000e-04\nEpoch 38/150\n271/271 - 1s - 5ms/step - accuracy: 0.7182 - categorical_crossentropy: 0.6713 - loss: 0.6713 - val_accuracy: 0.5837 - val_categorical_crossentropy: 0.9792 - val_loss: 0.9792 - learning_rate: 1.5000e-04\nEpoch 39/150\n271/271 - 1s - 5ms/step - accuracy: 0.7212 - categorical_crossentropy: 0.6630 - loss: 0.6630 - val_accuracy: 0.5908 - val_categorical_crossentropy: 0.9821 - val_loss: 0.9821 - learning_rate: 1.5000e-04\nEpoch 40/150\n271/271 - 1s - 5ms/step - accuracy: 0.7253 - categorical_crossentropy: 0.6603 - loss: 0.6603 - val_accuracy: 0.5837 - val_categorical_crossentropy: 0.9857 - val_loss: 0.9857 - learning_rate: 1.5000e-04\nEpoch 41/150\n271/271 - 1s - 5ms/step - accuracy: 0.7272 - categorical_crossentropy: 0.6556 - loss: 0.6556 - val_accuracy: 0.5996 - val_categorical_crossentropy: 0.9806 - val_loss: 0.9806 - learning_rate: 1.5000e-04\n\n\nThis chunk conducts an extensive evaluation of the trained neural network model using multiple metrics appropriate for multiclass classification problems. The evaluation begins with standard test set performance metrics including accuracy and loss, then expands to detailed prediction analysis including confidence distribution assessment.\nThe chunk calculates prediction confidence levels to understand when the model is most and least certain about its predictions, which is crucial for operational avalanche forecasting where confidence levels inform decision-making. A comprehensive confusion matrix analysis reveals specific patterns in model errors, identifying which avalanche hazard levels are most commonly confused with each other.\nThe evaluation includes per-class metrics (precision, recall, F1-score) to understand model performance across all five avalanche risk levels, macro and weighted averages to account for class imbalance effects, and Cohen’s Kappa to measure agreement beyond chance. Advanced visualizations display training history, feature importance rankings, prediction confidence distributions, and class distribution comparisons to provide comprehensive insights into model behavior and performance characteristics.\nThis final chunk also implements a comprehensive evaluation framework specifically designed for ordinal classification problems in avalanche risk prediction. Unlike standard multiclass classification, avalanche hazard levels have a natural ordering from Low to High, making ordinal-specific metrics essential for proper model assessment. The evaluation calculates Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) treating predictions as ordinal values, adjacent accuracy metrics that measure predictions within one risk level of the true value (crucial for operational safety), and distance-weighted accuracy measures that penalize larger prediction errors more severely. The chunk implements avalanche-specific safety metrics including critical miss rate (predicting low risk when actual risk is high), conservative bias assessment, and safety margin effectiveness. Advanced correlation measures like Kendall’s Tau and Spearman’s rank correlation evaluate the model’s ability to maintain proper risk ordering. The evaluation concludes with comprehensive risk assessment metrics including high-risk detection sensitivity, confidence analysis by risk level, and directional accuracy measures. This specialized evaluation approach ensures the model is assessed not just for classification accuracy, but for its practical utility and safety implications in real-world avalanche forecasting scenarios.\n\n# EVALUATION METRICS\n\ncat(\"\\n=== COMPREHENSIVE EVALUATION ===\\n\")\n\n\n=== COMPREHENSIVE EVALUATION ===\n\n# Test set evaluation\ntest_scores &lt;- final_model %&gt;% evaluate(X_test, y_test_cat, verbose = 0)\ncat(\"Test accuracy:\", test_scores$accuracy, \"\\n\")\n\nTest accuracy: 0.5851064 \n\ncat(\"Test loss:\", test_scores$loss, \"\\n\")\n\nTest loss: 0.994503 \n\n# Detailed predictions and analysis\npredictions &lt;- final_model %&gt;% predict(X_test, verbose = 0)\npredicted_classes &lt;- apply(predictions, 1, which.max) - 1  # Convert back to 0-based\npredicted_probs &lt;- apply(predictions, 1, max)\n\n# Confidence analysis\nhigh_confidence &lt;- predicted_probs &gt; 0.8\nmedium_confidence &lt;- predicted_probs &gt; 0.6 & predicted_probs &lt;= 0.8\nlow_confidence &lt;- predicted_probs &lt;= 0.6\n\ncat(\"\\nPrediction confidence distribution:\\n\")\n\n\nPrediction confidence distribution:\n\ncat(\"High confidence (&gt;0.8):\", sum(high_confidence), \"predictions\\n\")\n\nHigh confidence (&gt;0.8): 147 predictions\n\ncat(\"Medium confidence (0.6-0.8):\", sum(medium_confidence), \"predictions\\n\") \n\nMedium confidence (0.6-0.8): 275 predictions\n\ncat(\"Low confidence (&lt;=0.6):\", sum(low_confidence), \"predictions\\n\")\n\nLow confidence (&lt;=0.6): 706 predictions\n\n# Accuracy by confidence level\nif(sum(high_confidence) &gt; 0) {\n  cat(\"\\nAccuracy by confidence level:\\n\")\n  cat(\"High confidence accuracy:\", mean(predicted_classes[high_confidence] == y_test[high_confidence]), \"\\n\")\n}\n\n\nAccuracy by confidence level:\nHigh confidence accuracy: 0.8911565 \n\nif(sum(medium_confidence) &gt; 0) {\n  cat(\"Medium confidence accuracy:\", mean(predicted_classes[medium_confidence] == y_test[medium_confidence]), \"\\n\")\n}\n\nMedium confidence accuracy: 0.6581818 \n\nif(sum(low_confidence) &gt; 0) {\n  cat(\"Low confidence accuracy:\", mean(predicted_classes[low_confidence] == y_test[low_confidence]), \"\\n\")\n}\n\nLow confidence accuracy: 0.4929178 \n\n# Confusion matrix with proper class labels\nconfusion_matrix &lt;- table(Predicted = predicted_classes, Actual = y_test)\ncat(\"\\nConfusion Matrix:\\n\")\n\n\nConfusion Matrix:\n\nprint(confusion_matrix)\n\n         Actual\nPredicted   0   1   2   3   4\n        0 295  99  11   2   0\n        1  68 201  72  17   2\n        2  12  69 135  38  12\n        3   2  11  33  24  14\n        4   0   0   4   2   5\n\n# Classification report\ncm &lt;- confusionMatrix(as.factor(predicted_classes), as.factor(y_test))\nprint(cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1   2   3   4\n         0 295  99  11   2   0\n         1  68 201  72  17   2\n         2  12  69 135  38  12\n         3   2  11  33  24  14\n         4   0   0   4   2   5\n\nOverall Statistics\n                                         \n               Accuracy : 0.5851         \n                 95% CI : (0.5557, 0.614)\n    No Information Rate : 0.3369         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.418          \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: 0 Class: 1 Class: 2 Class: 3 Class: 4\nSensitivity            0.7825   0.5289   0.5294  0.28916 0.151515\nSpecificity            0.8509   0.7874   0.8499  0.94258 0.994521\nPos Pred Value         0.7248   0.5583   0.5075  0.28571 0.454545\nNeg Pred Value         0.8863   0.7669   0.8608  0.94349 0.974933\nPrevalence             0.3342   0.3369   0.2261  0.07358 0.029255\nDetection Rate         0.2615   0.1782   0.1197  0.02128 0.004433\nDetection Prevalence   0.3608   0.3191   0.2358  0.07447 0.009752\nBalanced Accuracy      0.8167   0.6582   0.6897  0.61587 0.573018\n\n# VISUALIZATION AND FINAL INSIGHTS\n\ncat(\"\\n=== CREATING VISUALIZATIONS ===\\n\")\n\n\n=== CREATING VISUALIZATIONS ===\n\n# Add explicit green for validation\nvalidation_green &lt;- \"#008000\"  # Explicit green for validation elements\n\n# Training history - Accuracy\nif(length(final_history$metrics$accuracy) &gt; 0) {\n  plot(final_history$metrics$accuracy, type = 'l', col = sais_palette[2], \n       main = 'Model Accuracy', ylab = 'Accuracy', xlab = 'Epoch', lwd = 2)\n  if(length(final_history$metrics$val_accuracy) &gt; 0) {\n    lines(final_history$metrics$val_accuracy, col = validation_green, lwd = 2)\n    legend('bottomright', c('Training', 'Validation'), col = c(sais_palette[2], validation_green), lty = 1, lwd = 2)\n  }\n  grid()\n}\n\n\n\n\n\n\n\n# Training history - Loss (separate plot)\nif(length(final_history$metrics$loss) &gt; 0) {\n  plot(final_history$metrics$loss, type = 'l', col = sais_palette[2], \n       main = 'Model Loss', ylab = 'Loss', xlab = 'Epoch', lwd = 2)\n  if(length(final_history$metrics$val_loss) &gt; 0) {\n    lines(final_history$metrics$val_loss, col = validation_green, lwd = 2)\n    legend('topright', c('Training', 'Validation'), col = c(sais_palette[2], validation_green), lty = 1, lwd = 2)\n  }\n  grid()\n}\n\n\n\n\n\n\n\n# Feature importance (if Random Forest was successful)\nif(exists(\"rf_importance\") && !is.null(rf_importance)) {\n  top_features &lt;- head(selected_features, 10)\n   par(mar = c(8, 4, 4, 2))  # Larger bottom margin to prevent chopping\n   \n  if(length(top_features) &gt; 0 && all(top_features %in% rownames(rf_importance))) {\n    barplot(rf_importance[top_features, 1], \n            names.arg = substr(top_features, 1, 10),\n            las = 2, main = \"Top 10 Feature Importance\",\n            ylab = \"Importance\", cex.names = 0.8, col = sais_palette[2])\n  }\n}\n\n\n\n\n\n\n\n# Prediction confidence histogram (separate plot)\nhist(predicted_probs, main = \"Prediction Confidence Distribution\", \n     xlab = \"Confidence\", ylab = \"Frequency\", col = sais_palette[2], \n     border = sais_palette[1], breaks = 20)\ngrid()\n\n\n\n\n\n\n\n# ROSE Balanced Training Data distribution (separate plot)\n# Increase bottom margin for vertical labels\npar(mar = c(8, 4, 4, 2))  # Larger bottom margin\n\nbarplot(table(balanced_train_data$y), main = \"ROSE Balanced Training Data\", \n        names.arg = fah_levels, ylab = \"Count\", col = sais_palette[2],  # Use \"#0065bd\" for bars\n        border = sais_palette[1], las = 2, cex.names = 0.8)  # Vertical labels, size 0.8\ngrid()\n\n\n\n\n\n\n\n# Reset margins\npar(mar = c(5, 4, 4, 2))\n\n# Test Data Distribution (separate plot)\n# Increase bottom margin for vertical labels\npar(mar = c(8, 4, 4, 2))  # Larger bottom margin\n\nbarplot(table(y_test), main = \"Test Data Distribution\", \n        names.arg = fah_levels, ylab = \"Count\", col = sais_palette[2],  # Use \"#0065bd\" for bars\n        border = sais_palette[1], las = 2, cex.names = 0.8)  # Vertical labels, size 0.8\ngrid()\n\n\n\n\n\n\n\n# Reset margins\npar(mar = c(5, 4, 4, 2))\n\n# FINAL INSIGHTS AND RECOMMENDATIONS\n\ncat(\"\\n=== FINAL INSIGHTS AND RECOMMENDATIONS ===\\n\")\n\n\n=== FINAL INSIGHTS AND RECOMMENDATIONS ===\n\ncat(\"Original dataset size:\", nrow(final_data), \"\\n\")\n\nOriginal dataset size: 7530 \n\ncat(\"Training set size after ROSE:\", nrow(balanced_train_data), \"\\n\")\n\nTraining set size after ROSE: 8641 \n\ncat(\"Number of features selected:\", length(selected_features), \"\\n\")\n\nNumber of features selected: 25 \n\ncat(\"Best validation accuracy:\", sprintf(\"%.4f\", best_score), \"\\n\")\n\nBest validation accuracy: 0.6129 \n\ncat(\"Final test accuracy:\", sprintf(\"%.4f\", test_scores$accuracy), \"\\n\")\n\nFinal test accuracy: 0.5851 \n\ncat(\"Number of classes:\", num_classes, \"\\n\")\n\nNumber of classes: 5 \n\n# Performance by class analysis\ncat(\"\\n=== PERFORMANCE BY AVALANCHE RISK CLASS ===\\n\")\n\n\n=== PERFORMANCE BY AVALANCHE RISK CLASS ===\n\nfor(i in 0:(num_classes-1)) {\n  class_mask &lt;- y_test == i\n  if(sum(class_mask) &gt; 0) {\n    class_accuracy &lt;- mean(predicted_classes[class_mask] == y_test[class_mask])\n    class_count &lt;- sum(class_mask)\n    avg_confidence &lt;- mean(predicted_probs[class_mask])\n    cat(sprintf(\"%-15s: Accuracy=%.3f, Count=%3d, Avg Confidence=%.3f\\n\", \n                fah_levels[i+1], class_accuracy, class_count, avg_confidence))\n  }\n}\n\nLow            : Accuracy=0.782, Count=377, Avg Confidence=0.692\nModerate       : Accuracy=0.529, Count=380, Avg Confidence=0.547\nConsiderable - : Accuracy=0.529, Count=255, Avg Confidence=0.502\nConsiderable + : Accuracy=0.289, Count= 83, Avg Confidence=0.508\nHigh           : Accuracy=0.152, Count= 33, Avg Confidence=0.580\n\n# Error analysis\nmisclassified &lt;- predicted_classes != y_test\nif(sum(misclassified) &gt; 0) {\n  cat(\"\\n=== ERROR ANALYSIS ===\\n\")\n  cat(\"Total misclassified predictions:\", sum(misclassified), \"\\n\")\n  \n  # Low confidence errors\n  low_conf_errors &lt;- misclassified & low_confidence\n  cat(\"Low confidence errors:\", sum(low_conf_errors), \"out of\", sum(misclassified), \"total errors\\n\")\n  \n  # Most problematic class pairs\n  error_df &lt;- data.frame(\n    Actual = fah_levels[y_test[misclassified] + 1],\n    Predicted = fah_levels[predicted_classes[misclassified] + 1]\n  )\n  cat(\"\\nMost common misclassification patterns:\\n\")\n  error_patterns &lt;- table(error_df$Actual, error_df$Predicted)\n  print(error_patterns)\n}\n\n\n=== ERROR ANALYSIS ===\nTotal misclassified predictions: 468 \nLow confidence errors: 358 out of 468 total errors\n\nMost common misclassification patterns:\n                \n                 Considerable - Considerable + High Low Moderate\n  Considerable -              0             33    4  11       72\n  Considerable +             38              0    2   2       17\n  High                       12             14    0   0        2\n  Low                        12              2    0   0       68\n  Moderate                   69             11    0  99        0\n\n# Performance comparison with baseline\nbaseline_accuracy &lt;- max(table(y_test)) / length(y_test)\nimprovement &lt;- (test_scores$accuracy - baseline_accuracy) / baseline_accuracy * 100\n\ncat(\"\\n=== PERFORMANCE COMPARISON ===\\n\")\n\n\n=== PERFORMANCE COMPARISON ===\n\ncat(\"Baseline accuracy (most frequent class):\", sprintf(\"%.1f%%\", baseline_accuracy * 100), \"\\n\")\n\nBaseline accuracy (most frequent class): 33.7% \n\ncat(\"Neural network accuracy:\", sprintf(\"%.1f%%\", test_scores$accuracy * 100), \"\\n\")\n\nNeural network accuracy: 58.5% \n\ncat(\"Relative improvement:\", sprintf(\"%.1f%%\", improvement), \"\\n\")\n\nRelative improvement: 73.7% \n\n# Feature importance summary\nif(exists(\"feature_scores\")) {\n  cat(\"\\n=== TOP 10 MOST IMPORTANT FEATURES ===\\n\")\n  top_10_summary &lt;- head(feature_scores, 10)\n  for(i in 1:nrow(top_10_summary)) {\n    cat(sprintf(\"%2d. %-25s (Total Score: %.1f)\\n\", \n                i, top_10_summary$Feature[i], top_10_summary$Total_Score[i]))\n  }\n}\n\n\n=== TOP 10 MOST IMPORTANT FEATURES ===\n 1. Foot.Pen                  (Total Score: 134.8)\n 2. Drift                     (Total Score: 114.6)\n 3. Summit.Air.Temp           (Total Score: 106.4)\n 4. Total.Snow.Depth          (Total Score: 92.0)\n 5. Crystals                  (Total Score: 80.6)\n 6. Air.Temp                  (Total Score: 78.6)\n 7. Wind_Chill                (Total Score: 65.5)\n 8. Precip.Code_X0...None     (Total Score: 65.2)\n 9. Precip.Code_X8...Snow     (Total Score: 64.1)\n10. Snow.Index                (Total Score: 63.6)\n\n# Model architecture summary\ncat(\"\\n=== FINAL MODEL ARCHITECTURE SUMMARY ===\\n\")\n\n\n=== FINAL MODEL ARCHITECTURE SUMMARY ===\n\nif(exists(\"best_config\")) {\n  cat(\"Architecture: Input -&gt;\", best_config$hidden_units_1, \"-&gt;\", \n      best_config$hidden_units_2, \"-&gt;\", best_config$hidden_units_3, \"-&gt; Output (\", num_classes, \"classes)\\n\")\n  cat(\"Regularization: Batch Normalization + Dropout (\", \n      best_config$dropout_rate_1, \",\", best_config$dropout_rate_2, \",\", \n      best_config$dropout_rate_3, \") + Gaussian Noise\\n\")\n  cat(\"Optimizer: Adam with learning rate\", best_config$learning_rate, \"\\n\")\n  cat(\"Batch size:\", best_config$batch_size, \"\\n\")\n}\n\nArchitecture: Input -&gt; 512 -&gt; 256 -&gt; 128 -&gt; Output ( 5 classes)\nRegularization: Batch Normalization + Dropout ( 0.2 , 0.3 , 0.4 ) + Gaussian Noise\nOptimizer: Adam with learning rate 5e-04 \nBatch size: 32 \n\ncat(\"Total parameters: ~\", format(final_model$count_params(), big.mark = \",\"), \"\\n\")\n\nTotal parameters: ~ 181,765 \n\ncat(\"Training epochs:\", length(final_history$metrics$accuracy), \"\\n\")\n\nTraining epochs: 41 \n\n# Enhanced TEST SET EVALUATION METRICS\n\ncat(\"\\n=== ENHANCED TEST SET EVALUATION METRICS ===\\n\")\n\n\n=== ENHANCED TEST SET EVALUATION METRICS ===\n\n# Calculate comprehensive metrics for multiclass classification\ncalculate_multiclass_metrics &lt;- function(y_true, y_pred, class_names) {\n  # Convert to factors for caret\n  y_true_factor &lt;- factor(y_true, levels = 0:(length(class_names)-1), labels = class_names)\n  y_pred_factor &lt;- factor(y_pred, levels = 0:(length(class_names)-1), labels = class_names)\n  \n  # Overall metrics\n  accuracy &lt;- mean(y_true == y_pred)\n  \n  # Per-class metrics\n  precision_per_class &lt;- c()\n  recall_per_class &lt;- c()\n  f1_per_class &lt;- c()\n  \n  for(i in 0:(length(class_names)-1)) {\n    # True positives, false positives, false negatives\n    tp &lt;- sum(y_true == i & y_pred == i)\n    fp &lt;- sum(y_true != i & y_pred == i)\n    fn &lt;- sum(y_true == i & y_pred != i)\n    \n    # Calculate metrics\n    precision &lt;- ifelse(tp + fp == 0, 0, tp / (tp + fp))\n    recall &lt;- ifelse(tp + fn == 0, 0, tp / (tp + fn))\n    f1 &lt;- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))\n    \n    precision_per_class &lt;- c(precision_per_class, precision)\n    recall_per_class &lt;- c(recall_per_class, recall)\n    f1_per_class &lt;- c(f1_per_class, f1)\n  }\n  \n  # Macro averages\n  macro_precision &lt;- mean(precision_per_class)\n  macro_recall &lt;- mean(recall_per_class)\n  macro_f1 &lt;- mean(f1_per_class)\n  \n  # Weighted averages (weighted by support)\n  class_support &lt;- table(y_true)\n  weights &lt;- as.numeric(class_support) / sum(class_support)\n  \n  weighted_precision &lt;- sum(precision_per_class * weights)\n  weighted_recall &lt;- sum(recall_per_class * weights)\n  weighted_f1 &lt;- sum(f1_per_class * weights)\n  \n  return(list(\n    accuracy = accuracy,\n    macro_precision = macro_precision,\n    macro_recall = macro_recall,\n    macro_f1 = macro_f1,\n    weighted_precision = weighted_precision,\n    weighted_recall = weighted_recall,\n    weighted_f1 = weighted_f1,\n    per_class_precision = precision_per_class,\n    per_class_recall = recall_per_class,\n    per_class_f1 = f1_per_class,\n    class_names = class_names\n  ))\n}\n\n# Calculate detailed metrics\ndetailed_metrics &lt;- calculate_multiclass_metrics(y_test, predicted_classes, fah_levels)\n\n# Display overall metrics\ncat(\"\\n=== OVERALL TEST SET METRICS ===\\n\")\n\n\n=== OVERALL TEST SET METRICS ===\n\ncat(sprintf(\"Accuracy:           %.4f\\n\", detailed_metrics$accuracy))\n\nAccuracy:           0.5851\n\ncat(sprintf(\"Macro Precision:    %.4f\\n\", detailed_metrics$macro_precision))\n\nMacro Precision:    0.5062\n\ncat(sprintf(\"Macro Recall:       %.4f\\n\", detailed_metrics$macro_recall))\n\nMacro Recall:       0.4563\n\ncat(sprintf(\"Macro F1-Score:     %.4f\\n\", detailed_metrics$macro_f1))\n\nMacro F1-Score:     0.4657\n\ncat(sprintf(\"Weighted Precision: %.4f\\n\", detailed_metrics$weighted_precision))\n\nWeighted Precision: 0.5794\n\ncat(sprintf(\"Weighted Recall:    %.4f\\n\", detailed_metrics$weighted_recall))\n\nWeighted Recall:    0.5851\n\ncat(sprintf(\"Weighted F1-Score:  %.4f\\n\", detailed_metrics$weighted_f1))\n\nWeighted F1-Score:  0.5795\n\n# Display per-class metrics\ncat(\"\\n=== PER-CLASS METRICS ===\\n\")\n\n\n=== PER-CLASS METRICS ===\n\ncat(sprintf(\"%-15s %9s %9s %9s %9s\\n\", \"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"))\n\nClass           Precision    Recall  F1-Score   Support\n\ncat(paste(rep(\"-\", 60), collapse = \"\"), \"\\n\")\n\n------------------------------------------------------------ \n\nclass_support &lt;- table(y_test)\nfor(i in 1:length(fah_levels)) {\n  class_name &lt;- fah_levels[i]\n  precision &lt;- detailed_metrics$per_class_precision[i]\n  recall &lt;- detailed_metrics$per_class_recall[i]\n  f1 &lt;- detailed_metrics$per_class_f1[i]\n  support &lt;- class_support[as.character(i-1)]\n  \n  cat(sprintf(\"%-15s %9.4f %9.4f %9.4f %9d\\n\", \n              class_name, precision, recall, f1, support))\n}\n\nLow                0.7248    0.7825    0.7526       377\nModerate           0.5583    0.5289    0.5432       380\nConsiderable -     0.5075    0.5294    0.5182       255\nConsiderable +     0.2857    0.2892    0.2874        83\nHigh               0.4545    0.1515    0.2273        33\n\n# Additional advanced metrics\ncat(\"\\n=== ADDITIONAL METRICS ===\\n\")\n\n\n=== ADDITIONAL METRICS ===\n\n# Cohen's Kappa (agreement accounting for chance)\ntryCatch({\n  kappa_result &lt;- kappa2(data.frame(y_test, predicted_classes))\n  kappa_score &lt;- kappa_result$value\n  cat(sprintf(\"Cohen's Kappa:      %.4f\\n\", kappa_score))\n}, error = function(e) {\n  # Manual Cohen's Kappa calculation if irr package fails\n  observed_agreement &lt;- mean(y_test == predicted_classes)\n  \n  # Calculate expected agreement by chance\n  marginal_actual &lt;- table(y_test) / length(y_test)\n  marginal_predicted &lt;- table(predicted_classes) / length(predicted_classes)\n  \n  expected_agreement &lt;- 0\n  for(i in 0:(length(fah_levels)-1)) {\n    if(as.character(i) %in% names(marginal_actual) && as.character(i) %in% names(marginal_predicted)) {\n      expected_agreement &lt;- expected_agreement + marginal_actual[as.character(i)] * marginal_predicted[as.character(i)]\n    }\n  }\n  \n  kappa_score &lt;- (observed_agreement - expected_agreement) / (1 - expected_agreement)\n  cat(sprintf(\"Cohen's Kappa:      %.4f (manual calculation)\\n\", kappa_score))\n})\n\nCohen's Kappa:      0.4180\n\n# Mean Absolute Error for ordinal nature\nmae_score &lt;- mean(abs(y_test - predicted_classes))\ncat(sprintf(\"Mean Absolute Error (ordinal): %.4f\\n\", mae_score))\n\nMean Absolute Error (ordinal): 0.4849\n\n# Classification error by distance\nerror_distances &lt;- abs(y_test - predicted_classes)[y_test != predicted_classes]\nif(length(error_distances) &gt; 0) {\n  cat(\"\\n=== ERROR DISTANCE ANALYSIS ===\\n\")\n  cat(\"Distribution of prediction errors by distance:\\n\")\n  error_dist_table &lt;- table(error_distances)\n  for(dist in names(error_dist_table)) {\n    cat(sprintf(\"Distance %s: %d errors (%.1f%% of total errors)\\n\", \n                dist, error_dist_table[dist], \n                error_dist_table[dist]/length(error_distances)*100))\n  }\n}\n\n\n=== ERROR DISTANCE ANALYSIS ===\nDistribution of prediction errors by distance:\nDistance 1: 395 errors (84.4% of total errors)\nDistance 2: 67 errors (14.3% of total errors)\nDistance 3: 6 errors (1.3% of total errors)\n\n# ORDINAL EVALUATION METRICS\n\ncat(\"\\n=== COMPREHENSIVE ORDINAL EVALUATION METRICS ===\\n\")\n\n\n=== COMPREHENSIVE ORDINAL EVALUATION METRICS ===\n\n# Load required library for ordinal metrics\nif(!require(\"Metrics\", quietly = TRUE)) {\n  install.packages(\"Metrics\")\n  library(Metrics)\n}\n\n# 1. Mean Absolute Error (MAE) - already calculated above but expanding\ncat(sprintf(\"Mean Absolute Error (MAE):        %.4f\\n\", mae_score))\n\nMean Absolute Error (MAE):        0.4849\n\n# 2. Root Mean Squared Error (RMSE) for ordinal data\nrmse_score &lt;- sqrt(mean((y_test - predicted_classes)^2))\ncat(sprintf(\"Root Mean Squared Error (RMSE):   %.4f\\n\", rmse_score))\n\nRoot Mean Squared Error (RMSE):   0.7973\n\n# 3. Mean Squared Error (MSE)\nmse_score &lt;- mean((y_test - predicted_classes)^2)\ncat(sprintf(\"Mean Squared Error (MSE):         %.4f\\n\", mse_score))\n\nMean Squared Error (MSE):         0.6356\n\n# 4. Ordinal classification accuracy (exact match)\nordinal_accuracy &lt;- mean(y_test == predicted_classes)\ncat(sprintf(\"Ordinal Accuracy (exact match):   %.4f\\n\", ordinal_accuracy))\n\nOrdinal Accuracy (exact match):   0.5851\n\n# 5. Adjacent accuracy (off by at most 1 level)\nadjacent_accuracy &lt;- mean(abs(y_test - predicted_classes) &lt;= 1)\ncat(sprintf(\"Adjacent Accuracy (≤1 level off): %.4f\\n\", adjacent_accuracy))\n\nAdjacent Accuracy (≤1 level off): 0.9353\n\n# 6. Within-2 accuracy (off by at most 2 levels)\nwithin2_accuracy &lt;- mean(abs(y_test - predicted_classes) &lt;= 2)\ncat(sprintf(\"Within-2 Accuracy (≤2 levels off): %.4f\\n\", within2_accuracy))\n\nWithin-2 Accuracy (≤2 levels off): 0.9947\n\n# 7. Ordinal loss (penalizes larger deviations more)\nordinal_loss &lt;- sum(abs(y_test - predicted_classes)^2) / length(y_test)\ncat(sprintf(\"Ordinal Loss (squared deviations): %.4f\\n\", ordinal_loss))\n\nOrdinal Loss (squared deviations): 0.6356\n\n# 8. Weighted accuracy by distance\ndistance_weights &lt;- 1 / (1 + abs(y_test - predicted_classes))\nweighted_accuracy &lt;- mean(distance_weights)\ncat(sprintf(\"Distance-Weighted Accuracy:       %.4f\\n\", weighted_accuracy))\n\nDistance-Weighted Accuracy:       0.7813\n\n# 9. Kendall's Tau (rank correlation)\nif(require(\"Kendall\", quietly = TRUE)) {\n  tryCatch({\n    kendall_result &lt;- Kendall(y_test, predicted_classes)\n    kendall_tau &lt;- kendall_result$tau\n    kendall_p &lt;- kendall_result$sl[1]\n    cat(sprintf(\"Kendall's Tau (rank correlation): %.4f (p-value: %.4f)\\n\", kendall_tau, kendall_p))\n  }, error = function(e) {\n    cat(\"Kendall's Tau calculation failed\\n\")\n  })\n} else {\n  # Manual calculation of Kendall's Tau\n  n &lt;- length(y_test)\n  concordant &lt;- 0\n  discordant &lt;- 0\n  \n  for(i in 1:(n-1)) {\n    for(j in (i+1):n) {\n      if((y_test[i] - y_test[j]) * (predicted_classes[i] - predicted_classes[j]) &gt; 0) {\n        concordant &lt;- concordant + 1\n      } else if((y_test[i] - y_test[j]) * (predicted_classes[i] - predicted_classes[j]) &lt; 0) {\n        discordant &lt;- discordant + 1\n      }\n    }\n  }\n  \n  kendall_tau_manual &lt;- (concordant - discordant) / (n * (n - 1) / 2)\n  cat(sprintf(\"Kendall's Tau (manual calc):      %.4f\\n\", kendall_tau_manual))\n}\n\nKendall's Tau (rank correlation): 0.6424 (p-value: 0.0000)\n\n# 10. Spearman's rank correlation\nspearman_corr &lt;- cor(y_test, predicted_classes, method = \"spearman\")\ncat(sprintf(\"Spearman's Rank Correlation:      %.4f\\n\", spearman_corr))\n\nSpearman's Rank Correlation:      0.7139\n\n# 11. Pearson correlation (treating as continuous)\npearson_corr &lt;- cor(y_test, predicted_classes, method = \"pearson\")\ncat(sprintf(\"Pearson Correlation:              %.4f\\n\", pearson_corr))\n\nPearson Correlation:              0.6984\n\n# 12. Cumulative accuracy metrics\ncumulative_accuracies &lt;- c()\nfor(tolerance in 0:4) {\n  cum_acc &lt;- mean(abs(y_test - predicted_classes) &lt;= tolerance)\n  cumulative_accuracies &lt;- c(cumulative_accuracies, cum_acc)\n  cat(sprintf(\"Cumulative Accuracy (≤%d levels): %.4f\\n\", tolerance, cum_acc))\n}\n\nCumulative Accuracy (≤0 levels): 0.5851\nCumulative Accuracy (≤1 levels): 0.9353\nCumulative Accuracy (≤2 levels): 0.9947\nCumulative Accuracy (≤3 levels): 1.0000\nCumulative Accuracy (≤4 levels): 1.0000\n\n# 13. Ordinal-specific confusion matrix analysis\ncat(\"\\n=== ORDINAL CONFUSION MATRIX ANALYSIS ===\\n\")\n\n\n=== ORDINAL CONFUSION MATRIX ANALYSIS ===\n\n# Distance-based confusion analysis\ndistance_matrix &lt;- outer(0:4, 0:4, function(x, y) abs(x - y))\nrownames(distance_matrix) &lt;- fah_levels\ncolnames(distance_matrix) &lt;- fah_levels\n\ncat(\"Distance matrix between classes:\\n\")\n\nDistance matrix between classes:\n\nprint(distance_matrix)\n\n               Low Moderate Considerable - Considerable + High\nLow              0        1              2              3    4\nModerate         1        0              1              2    3\nConsiderable -   2        1              0              1    2\nConsiderable +   3        2              1              0    1\nHigh             4        3              2              1    0\n\n# Weighted confusion matrix (penalizing by distance)\nweighted_confusion &lt;- confusion_matrix * distance_matrix[1:nrow(confusion_matrix), 1:ncol(confusion_matrix)]\ncat(\"\\nWeighted confusion matrix (errors penalized by distance):\\n\")\n\n\nWeighted confusion matrix (errors penalized by distance):\n\nprint(weighted_confusion)\n\n         Actual\nPredicted  0  1  2  3  4\n        0  0 99 22  6  0\n        1 68  0 72 34  6\n        2 24 69  0 38 24\n        3  6 22 33  0 14\n        4  0  0  8  2  0\n\n# Total weighted error\ntotal_weighted_error &lt;- sum(weighted_confusion)\ncat(sprintf(\"Total Weighted Error Score: %.2f\\n\", total_weighted_error))\n\nTotal Weighted Error Score: 547.00\n\n# 14. Severity-weighted metrics (higher penalties for severe misclassifications)\nseverity_weights &lt;- c(1, 2, 4, 8, 16)  # Exponential penalty for higher risk misses\nseverity_penalty &lt;- 0\n\nfor(i in 1:length(y_test)) {\n  actual_severity &lt;- severity_weights[y_test[i] + 1]\n  predicted_severity &lt;- severity_weights[predicted_classes[i] + 1]\n  \n  # Penalty for underestimating risk (more dangerous)\n  if(predicted_classes[i] &lt; y_test[i]) {\n    severity_penalty &lt;- severity_penalty + (actual_severity - predicted_severity) * 2\n  } \n  # Penalty for overestimating risk (less dangerous but still costly)\n  else if(predicted_classes[i] &gt; y_test[i]) {\n    severity_penalty &lt;- severity_penalty + (predicted_severity - actual_severity) * 1\n  }\n}\n\navg_severity_penalty &lt;- severity_penalty / length(y_test)\ncat(sprintf(\"Average Severity Penalty:         %.4f\\n\", avg_severity_penalty))\n\nAverage Severity Penalty:         1.9273\n\n# 15. Directional accuracy (proportion of correct trends)\n# For ordinal data, we can assess if the model correctly identifies the direction of risk\ncorrect_direction &lt;- 0\ntotal_comparisons &lt;- 0\n\nfor(i in 1:(length(y_test)-1)) {\n  for(j in (i+1):length(y_test)) {\n    actual_diff &lt;- y_test[i] - y_test[j]\n    predicted_diff &lt;- predicted_classes[i] - predicted_classes[j]\n    \n    if(actual_diff != 0) {  # Only consider cases where there's an actual difference\n      if(sign(actual_diff) == sign(predicted_diff)) {\n        correct_direction &lt;- correct_direction + 1\n      }\n      total_comparisons &lt;- total_comparisons + 1\n    }\n  }\n}\n\ndirectional_accuracy &lt;- ifelse(total_comparisons &gt; 0, correct_direction / total_comparisons, 0)\ncat(sprintf(\"Directional Accuracy:             %.4f\\n\", directional_accuracy))\n\nDirectional Accuracy:             0.7098\n\n# 16. Proportional reduction in error (compared to baseline ordinal prediction)\nbaseline_mae &lt;- mean(abs(y_test - median(y_test)))\nproportional_reduction &lt;- (baseline_mae - mae_score) / baseline_mae\ncat(sprintf(\"Proportional Reduction in MAE:    %.4f\\n\", proportional_reduction))\n\nProportional Reduction in MAE:    0.3902\n\n# 17. Ordinal classification summary table\ncat(\"\\n=== ORDINAL CLASSIFICATION SUMMARY TABLE ===\\n\")\n\n\n=== ORDINAL CLASSIFICATION SUMMARY TABLE ===\n\nordinal_summary &lt;- data.frame(\n  Metric = c(\"Exact Accuracy\", \"Adjacent Accuracy (±1)\", \"Within-2 Accuracy (±2)\", \n             \"MAE\", \"RMSE\", \"Kendall's Tau\", \"Spearman's ρ\", \"Directional Accuracy\"),\n  Value = c(ordinal_accuracy, adjacent_accuracy, within2_accuracy, \n            mae_score, rmse_score, \n            ifelse(exists(\"kendall_tau\"), kendall_tau, \n                   ifelse(exists(\"kendall_tau_manual\"), kendall_tau_manual, NA)),\n            spearman_corr, directional_accuracy),\n  Interpretation = c(\"Perfect predictions\", \"Off by ≤1 level\", \"Off by ≤2 levels\",\n                    \"Avg distance from true\", \"Penalizes large errors\", \n                    \"Rank correlation\", \"Rank correlation\", \"Correct ordering\"),\n  stringsAsFactors = FALSE\n)\n\nkable(ordinal_summary, digits = 4, \n           caption = \"Comprehensive Ordinal Classification Metrics\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nComprehensive Ordinal Classification Metrics\n\n\nMetric\nValue\nInterpretation\n\n\n\n\nExact Accuracy\n0.5851\nPerfect predictions\n\n\nAdjacent Accuracy (±1)\n0.9353\nOff by ≤1 level\n\n\nWithin-2 Accuracy (±2)\n0.9947\nOff by ≤2 levels\n\n\nMAE\n0.4849\nAvg distance from true\n\n\nRMSE\n0.7973\nPenalizes large errors\n\n\nKendall's Tau\n0.6424\nRank correlation\n\n\nSpearman's ρ\n0.7139\nRank correlation\n\n\nDirectional Accuracy\n0.7098\nCorrect ordering\n\n\n\n\n\n\n# 18. Risk assessment specific metrics for avalanche prediction\ncat(\"\\n=== AVALANCHE-SPECIFIC RISK ASSESSMENT METRICS ===\\n\")\n\n\n=== AVALANCHE-SPECIFIC RISK ASSESSMENT METRICS ===\n\n# Critical miss rate (predicting lower risk when actual is high)\nhigh_risk_actual &lt;- y_test &gt;= 3  # Considerable+ and High\nlow_predicted_for_high_actual &lt;- (y_test &gt;= 3) & (predicted_classes &lt;= 1)  # Predicted Low/Moderate\ncritical_miss_rate &lt;- sum(low_predicted_for_high_actual) / sum(high_risk_actual)\n\ncat(sprintf(\"Critical Miss Rate (Low pred/High actual): %.4f\\n\", critical_miss_rate))\n\nCritical Miss Rate (Low pred/High actual): 0.1810\n\n# Conservative bias (tendency to over-predict risk)\nconservative_predictions &lt;- sum(predicted_classes &gt; y_test)\nliberal_predictions &lt;- sum(predicted_classes &lt; y_test)\nconservative_bias &lt;- (conservative_predictions - liberal_predictions) / length(y_test)\n\ncat(sprintf(\"Conservative Bias (&gt;0 = over-predict):    %.4f\\n\", conservative_bias))\n\nConservative Bias (&gt;0 = over-predict):    -0.0585\n\n# Safety margin effectiveness\nsafe_predictions &lt;- sum(predicted_classes &gt;= y_test)  # At or above actual risk\nsafety_effectiveness &lt;- safe_predictions / length(y_test)\n\ncat(sprintf(\"Safety Effectiveness (≥actual risk):     %.4f\\n\", safety_effectiveness))\n\nSafety Effectiveness (≥actual risk):     0.7633\n\n# High-risk detection sensitivity and specificity\nhigh_risk_threshold &lt;- 2  # Considerable- and above\ntrue_high_risk &lt;- y_test &gt;= high_risk_threshold\npredicted_high_risk &lt;- predicted_classes &gt;= high_risk_threshold\n\nhigh_risk_sensitivity &lt;- sum(true_high_risk & predicted_high_risk) / sum(true_high_risk)\nhigh_risk_specificity &lt;- sum(!true_high_risk & !predicted_high_risk) / sum(!true_high_risk)\n\ncat(sprintf(\"High-Risk Sensitivity (detect high):     %.4f\\n\", high_risk_sensitivity))\n\nHigh-Risk Sensitivity (detect high):     0.7197\n\ncat(sprintf(\"High-Risk Specificity (avoid false+):    %.4f\\n\", high_risk_specificity))\n\nHigh-Risk Specificity (avoid false+):    0.8758\n\n# Model confidence analysis for different risk levels\ncat(\"\\n=== CONFIDENCE ANALYSIS BY RISK LEVEL ===\\n\")\n\n\n=== CONFIDENCE ANALYSIS BY RISK LEVEL ===\n\nfor(risk_level in 0:4) {\n  if(sum(y_test == risk_level) &gt; 0) {\n    level_mask &lt;- y_test == risk_level\n    avg_confidence &lt;- mean(predicted_probs[level_mask])\n    correct_predictions &lt;- sum(predicted_classes[level_mask] == y_test[level_mask])\n    total_predictions &lt;- sum(level_mask)\n    accuracy_at_level &lt;- correct_predictions / total_predictions\n    \n    cat(sprintf(\"%-15s: Avg Confidence=%.3f, Accuracy=%.3f, Count=%d\\n\", \n                fah_levels[risk_level + 1], avg_confidence, accuracy_at_level, total_predictions))\n  }\n}\n\nLow            : Avg Confidence=0.692, Accuracy=0.782, Count=377\nModerate       : Avg Confidence=0.547, Accuracy=0.529, Count=380\nConsiderable - : Avg Confidence=0.502, Accuracy=0.529, Count=255\nConsiderable + : Avg Confidence=0.508, Accuracy=0.289, Count=83\nHigh           : Avg Confidence=0.580, Accuracy=0.152, Count=33\n\n# Final ordinal performance summary\ncat(\"\\n=== FINAL ORDINAL PERFORMANCE SUMMARY ===\\n\")\n\n\n=== FINAL ORDINAL PERFORMANCE SUMMARY ===\n\ncat(\"==========================================\\n\")\n\n==========================================\n\ncat(sprintf(\"Model Performance on Ordinal Avalanche Risk Prediction:\\n\"))\n\nModel Performance on Ordinal Avalanche Risk Prediction:\n\ncat(sprintf(\"- Exact Match Accuracy:     %.1f%%\\n\", ordinal_accuracy * 100))\n\n- Exact Match Accuracy:     58.5%\n\ncat(sprintf(\"- Adjacent Accuracy (±1):   %.1f%%\\n\", adjacent_accuracy * 100))\n\n- Adjacent Accuracy (±1):   93.5%\n\ncat(sprintf(\"- Safety Effectiveness:     %.1f%%\\n\", safety_effectiveness * 100))\n\n- Safety Effectiveness:     76.3%\n\ncat(sprintf(\"- Critical Miss Rate:       %.1f%%\\n\", critical_miss_rate * 100))\n\n- Critical Miss Rate:       18.1%\n\ncat(sprintf(\"- Mean Absolute Error:      %.2f levels\\n\", mae_score))\n\n- Mean Absolute Error:      0.48 levels\n\ncat(sprintf(\"- Rank Correlation:         %.3f\\n\", spearman_corr))\n\n- Rank Correlation:         0.714\n\ncat(sprintf(\"- Conservative Bias:        %.3f\\n\", conservative_bias))\n\n- Conservative Bias:        -0.059\n\nif(critical_miss_rate &lt; 0.05) {\n  cat(\"✓ Excellent safety performance (critical miss rate &lt; 5%)\\n\")\n} else if(critical_miss_rate &lt; 0.10) {\n  cat(\"⚠ Good safety performance (critical miss rate &lt; 10%)\\n\")\n} else {\n  cat(\"⚠ Concerning safety performance (critical miss rate ≥ 10%)\\n\")\n}\n\n⚠ Concerning safety performance (critical miss rate ≥ 10%)\n\nif(adjacent_accuracy &gt; 0.90) {\n  cat(\"✓ Excellent ordinal accuracy (&gt;90% within ±1 level)\\n\")\n} else if(adjacent_accuracy &gt; 0.80) {\n  cat(\"✓ Good ordinal accuracy (&gt;80% within ±1 level)\\n\")\n} else {\n  cat(\"⚠ Moderate ordinal accuracy (&lt;80% within ±1 level)\\n\")\n}\n\n✓ Excellent ordinal accuracy (&gt;90% within ±1 level)\n\ncat(\"==========================================\\n\")\n\n==========================================\n\n# Clean up\nrm(list = ls()[grepl(\"temp|tmp\", ls())])\ngc()\n\n          used  (Mb) gc trigger  (Mb) max used  (Mb)\nNcells 2998502 160.2    5153475 275.3  5153475 275.3\nVcells 9966801  76.1   24005158 183.2 24001389 183.2\n\ncat(\"\\n=== MODEL TRAINING AND EVALUATION COMPLETE ===\\n\")\n\n\n=== MODEL TRAINING AND EVALUATION COMPLETE ==="
  },
  {
    "objectID": "llm-reflection.html",
    "href": "llm-reflection.html",
    "title": "LLM Usage and Reflection",
    "section": "",
    "text": "Our group extensively used multiple artificial intelligence tools throughout this avalanche prediction neural network project, employing a systematic and transparent approach to AI integration. The tools included Claude (Sonnet and Opus versions) by Anthropic, ChatGPT-5 by OpenAI and Grok (Grok 4) by xAI. All interactions were documented and archived in a dedicated Claude project for complete traceability1."
  },
  {
    "objectID": "llm-reflection.html#ai-statement",
    "href": "llm-reflection.html#ai-statement",
    "title": "LLM Usage and Reflection",
    "section": "",
    "text": "Our group extensively used multiple artificial intelligence tools throughout this avalanche prediction neural network project, employing a systematic and transparent approach to AI integration. The tools included Claude (Sonnet and Opus versions) by Anthropic, ChatGPT-5 by OpenAI and Grok (Grok 4) by xAI. All interactions were documented and archived in a dedicated Claude project for complete traceability1."
  },
  {
    "objectID": "llm-reflection.html#phases-of-ai-utilisation",
    "href": "llm-reflection.html#phases-of-ai-utilisation",
    "title": "LLM Usage and Reflection",
    "section": "Phases of AI Utilisation",
    "text": "Phases of AI Utilisation\nAI tools supported key stages of our data science workflow:\n\nExploratory Data Analysis (EDA): Assisted in identifying trends and patterns in the Scotland avalanche forecasts dataset.\nModel Development and Preprocessing: Provided guidance on handling missing data imputation, outlier detection/treatment, and class imbalance. Suggested methods included median imputation by geographic area for numerical variables, mode imputation by area for categorical variables, and context-specific outlier thresholds (e.g., capping altitude at Scotland’s highest peak of 1,345m).\nCode Development: Generated comprehensive R code using keras3 and TensorFlow, based on our EDA findings and requirements. Key prompts covered outlier assessment, missing value strategies, model architecture (including Random Forest feature selection and hyperparameter tuning via grid search), and data splitting (e.g., adjusting from 80/10/10 to 70/15/10 ratios). Also aided debugging, such as transitioning from manual upsampling to ROSE techniques.\nReport Writing and Review: Helped draft analysis sections, interpret evaluation metrics, and perform proofreading, spell-checking, and grammar verification.\n\nWe maintained critical oversight over the AI deliverables. All AI-generated content was verified against avalanche forecasting literature and Scottish meteorological standards, with code tested for functionality in our dataset context."
  },
  {
    "objectID": "llm-reflection.html#benefits-and-verification",
    "href": "llm-reflection.html#benefits-and-verification",
    "title": "LLM Usage and Reflection",
    "section": "Benefits and Verification",
    "text": "Benefits and Verification\nAI integration offered our group substantial advantages, including:\n\nAccelerated timelines and exposure to advanced techniques.\nComprehensive code documentation and creative problem-solving.\n\nAccuracy was verified through:\n\nCross-referencing with peer-reviewed research.\nSystematic code testing and manual spot-checks.\nComparing outputs across tools for inconsistencies.\nDomain-specific evaluation of methodologies."
  },
  {
    "objectID": "llm-reflection.html#critical-reflection",
    "href": "llm-reflection.html#critical-reflection",
    "title": "LLM Usage and Reflection",
    "section": "Critical Reflection",
    "text": "Critical Reflection\nAI tools demonstrated significant strengths but also limitations:\n\nAdvantages: Excelled in generating syntactically correct code and providing creative solutions, enhancing efficiency and innovation.\nLimitations: Occasionally suggested technically sound but contextually unsuitable approaches, lacking nuance in domain-specific factors (e.g., Scottish mountain meteorology). Different tools sometimes gave conflicting advice, requiring synthesis and judgment.\n\nUltimately, AI serves as a powerful assistant but cannot replace human critical thinking, domain expertise, or methodological rigor. This project highlighted the importance of thoughtful, transparent AI use to amplify capabilities while maintaining control."
  },
  {
    "objectID": "llm-reflection.html#llm-prompt-links",
    "href": "llm-reflection.html#llm-prompt-links",
    "title": "LLM Usage and Reflection",
    "section": "LLM prompt links",
    "text": "LLM prompt links\n\nChats from Claude\nEDA\nOutlier Analysis\nNaN analsis\nFeature Selection\nModel Development\nModel Development\nModel Debugging\n\n\nChats from Grok\nEDA and initial Model Development\nModel Refinement\nModel Debugging\nMarkdown format & Webpage hosting\n\n\nChats from ChatGPT\nEDA and initial Model Development\nEDA and initial Model Development\nModel Refinement\nModel selection\nGithub Help"
  },
  {
    "objectID": "llm-reflection.html#footnotes",
    "href": "llm-reflection.html#footnotes",
    "title": "LLM Usage and Reflection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClaude projects on the Claude Pro plan cannot be made available to the public (projects can be shared publicly on the Enterprise plan). However, links to the individual chats can be shared publicly. A sample of chats are shared below in the LLM prompt links section.↩︎"
  }
]